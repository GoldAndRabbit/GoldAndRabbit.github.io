<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>G&amp;R Blog</title>
  
  <subtitle>G&amp;R Blog</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2025-04-28T10:36:03.624Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>G&amp;R</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>G&amp;R Blog Release Regulation</title>
    <link href="http://example.com/2099/01/30/G&amp;R%20Blog%20Release%20Regulation/"/>
    <id>http://example.com/2099/01/30/G&amp;R%20Blog%20Release%20Regulation/</id>
    <published>2099-01-30T12:00:00.000Z</published>
    <updated>2025-04-28T10:36:03.624Z</updated>
    
    <content type="html"><![CDATA[<div align="center"><img src="/imgs/G&R Blog Release Regulation/1.png" width="60%"/></div><h2 id="G-amp-R-Blog-Release-Regulation"><a href="#G-amp-R-Blog-Release-Regulation" class="headerlink" title="G&amp;R Blog Release Regulation"></a>G&amp;R Blog Release Regulation</h2><p><strong>逻辑组织规范</strong><br>1.文章表述尽量符合金字塔原理: 结论先行, 自顶向下<br>2.注重文章之间的脉络与互相引用, 形成知识体系, 而不是零碎的众多文章的堆砌  </p><p><strong>格式规范</strong><br>1.标题默认采用 markdown 二级标题, 子标题采用三级标题<br>2.不采用 markdown 标准格式下, 使用末尾双空格换行<br>3.一句话末尾不用任何标点符号或者使用英文.符号<br>4.单词前后保留空格, 公式前后保留空格, 数字前后可选保留空格<br>5.强调某些内容采用 [内容] 或者 <strong>内容</strong><br>6.行与行之间最多空一行, 保持文章紧凑感  </p><h2 id="测试-3-hexo-主题对-markdown-文件渲染效果"><a href="#测试-3-hexo-主题对-markdown-文件渲染效果" class="headerlink" title="测试 3-hexo 主题对 markdown 文件渲染效果"></a>测试 3-hexo 主题对 markdown 文件渲染效果</h2><h3 id="1-数学公式"><a href="#1-数学公式" class="headerlink" title="1.数学公式"></a>1.数学公式</h3><p>需要在文章里面增加配置</p><pre><code>date: 2022-03-20 22:00:00mathjax: truecategories: - Ads_RecSys---</code></pre><p>测试行内公式 $b \to a$, $c=\frac{a}{b}$</p><p>测试带有大花括号的行内公式: $N={1,2,..,n}$</p><p>测试带有粗体的行内公式: $\textbf y=kx+b$</p><p>测试花体公式的效果 $\mathbb N$</p><p>测试花体公式的效果 $\mathcal L$</p><p>测试带序号的多行公式</p><script type="math/tex; mode=display">\begin{align}a&=c \\b&=d \\e&=2.71828\end{align}</script><p>测试带有颜色的无序号多行公式</p><script type="math/tex; mode=display">\begin{aligned}\color{blue}{v_i: 2^M \to \mathbb R_{\ge 0}} \\\color{green}{v_i: 2^M \to \mathbb R_{\ge 0}}\end{aligned}</script><h3 id="2-测试读取图片效果"><a href="#2-测试读取图片效果" class="headerlink" title="2.测试读取图片效果"></a>2.测试读取图片效果</h3><p>上线显示的结果如下, 线上可以正确显示 <strong>在 vscode 中预览模式中不可正确显示</strong>  </p><div align="center"><img src="/imgs/G&R Blog Release Regulation/0.png" width="10%"/></div><p>本地预览的结果如下, 在 vscode 中 markdown 右侧预览模式中可以正确显示, <strong>线上环境无法正确显示</strong><br>本地调试按照相对路径来写, 上线之前将相对路径改成绝对路径:<br>例如 </p><pre><code>&quot;../imgs/G&amp;R Blog Release Regulation/0.png&quot;=&gt; &quot;/imgs/G&amp;R Blog Release Regulation/0.png&quot;</code></pre><div align="center"><img src="../imgs/G&R Blog Release Regulation/0.png" width="10%"/></div><h3 id="3-多级标题显示效果"><a href="#3-多级标题显示效果" class="headerlink" title="3.多级标题显示效果"></a>3.多级标题显示效果</h3><h1 id="我是一级标题"><a href="#我是一级标题" class="headerlink" title="我是一级标题"></a>我是一级标题</h1><h2 id="我是二级标题-有下划线效果"><a href="#我是二级标题-有下划线效果" class="headerlink" title="我是二级标题, 有下划线效果"></a>我是二级标题, 有下划线效果</h2><h3 id="我是三级标题"><a href="#我是三级标题" class="headerlink" title="我是三级标题"></a>我是三级标题</h3><h4 id="我是四级标题"><a href="#我是四级标题" class="headerlink" title="我是四级标题"></a>我是四级标题</h4><h5 id="我是五级标题"><a href="#我是五级标题" class="headerlink" title="我是五级标题"></a>我是五级标题</h5><ul><li>内容1</li><li>内容2</li><li>内容3</li></ul><ol><li>列表1</li><li>列表1</li><li>列表1</li></ol><blockquote><p>我是内容1</p><p>我是内容2</p></blockquote><h3 id="4-代码显示效果"><a href="#4-代码显示效果" class="headerlink" title="4.代码显示效果"></a>4.代码显示效果</h3><pre><code class="lang-cpp">#include &lt;iostream&gt;int main() &#123;  return 0;&#125;</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/G&amp;R Blog Release Regulation/1.png&quot; width=&quot;60%&quot;/&gt;
&lt;/div&gt;

&lt;h2 id=&quot;G-amp-R-Blog-Release-Regulation&quot;&gt;&lt;a hr</summary>
      
    
    
    
    <category term="Coding" scheme="http://example.com/categories/Coding/"/>
    
    
  </entry>
  
  <entry>
    <title>A Collection of High-Quality Blog</title>
    <link href="http://example.com/2099/01/01/A%20Collection%20of%20High-Quality%20Blog/"/>
    <id>http://example.com/2099/01/01/A%20Collection%20of%20High-Quality%20Blog/</id>
    <published>2099-01-01T12:28:00.000Z</published>
    <updated>2025-04-28T11:04:15.173Z</updated>
    
    <content type="html"><![CDATA[<div align="center"><img src="/imgs/A Collection of High-Quality Blog/0.png" width="80%"/></div><h2 id="G-amp-R-Blog-Rating-Metric"><a href="#G-amp-R-Blog-Rating-Metric" class="headerlink" title="G&amp;R Blog Rating Metric"></a>G&amp;R Blog Rating Metric</h2><p><strong>Top-Quality Blog 顶级质量</strong><br>(i). 包含大量一流的原创示意图, 高密度手打数学公式, 并附带关键公式推导<br>(ii). 注重建立 intuition 而不是知识的堆砌<br>(iii). 注重知识之间的 connection, 文章之间有高密度的相互引用<br>(iv). 领域覆盖广泛但有相当的深度, 覆盖较多属于领域公认的解释性的文章, 较高的引用次数<br>(v). 长期在领域内持续精进  </p><p><strong>Excellent Quality Blog 中等质量</strong><br>具备逻辑组织, 具备为 &lt;10 篇精品  </p><p><strong>Learnable Blog 可学习</strong><br>具备可学习的知识  </p><h2 id="Top-Quality-Blog-Collection"><a href="#Top-Quality-Blog-Collection" class="headerlink" title="Top-Quality Blog Collection"></a>Top-Quality Blog Collection</h2><p>Lil’Log      <a href="https://lilianweng.github.io/">https://lilianweng.github.io/</a><br>Brian Pulfer <a href="https://www.brianpulfer.ch/blog">https://www.brianpulfer.ch/blog</a> </p><h2 id="Excellent-Quality-Blog-Collection"><a href="#Excellent-Quality-Blog-Collection" class="headerlink" title="Excellent Quality Blog Collection"></a>Excellent Quality Blog Collection</h2><p>Jay Mody <a href="https://jaykmody.com/">https://jaykmody.com/</a>  代表作 GPT in 60 Lines of NumPy<br>G&amp;R <a href="https://goldandrabbit.github.io/">https://goldandrabbit.github.io/</a>  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/A Collection of High-Quality Blog/0.png&quot; width=&quot;80%&quot;/&gt;
&lt;/div&gt;


&lt;h2 id=&quot;G-amp-R-Blog-Rating-Metric&quot;&gt;&lt;a </summary>
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>Linux Access the Open Internet</title>
    <link href="http://example.com/2025/02/01/Linux%20Access%20the%20Open%20Internet/"/>
    <id>http://example.com/2025/02/01/Linux%20Access%20the%20Open%20Internet/</id>
    <published>2025-02-01T12:43:00.000Z</published>
    <updated>2025-04-28T09:20:54.407Z</updated>
    
    <content type="html"><![CDATA[<div align="center"><img src="/imgs/Linux Access the Open Internet/0.png" width="60%"/></div><h2 id="Linux-Access-the-Open-Internet"><a href="#Linux-Access-the-Open-Internet" class="headerlink" title="Linux Access the Open Internet"></a>Linux Access the Open Internet</h2><p>1.Hiddify-Linux<br>下载 Hiddify-Linux-x64.AppImage from <a href="https://d2.freessr2.xyz/Hiddify-Linux-x64.AppImage">https://d2.freessr2.xyz/Hiddify-Linux-x64.AppImage</a></p><pre><code class="lang-bash">chmod 777 Hiddify-Linux-x64.AppImage./Hiddify-Linux-x64.AppImage</code></pre><p>2.SS 节点配置<br>从这里获取 <a href="https://github.com/Alvin9999/new-pac/wiki/ss%E5%85%8D%E8%B4%B9%E8%B4%A6%E5%8F%B7">https://github.com/Alvin9999/new-pac/wiki/ss%E5%85%8D%E8%B4%B9%E8%B4%A6%E5%8F%B7</a></p><p>然后 ss 配置复制到剪贴板</p><pre><code class="lang-bash">ss://YWVzLTI1Ni1nY206ZG9uZ3RhaXdhbmcuY29t@46.17.42.24:23355#dongtaiwang.com%E8%8A%82%E7%82%B9ss</code></pre><p>Reference<br>[1]. <a href="https://github.com/Alvin9999/new-pac/wiki/Linux%E7%B3%BB%E7%BB%9F%E7%BF%BB%E5%A2%99%E6%96%B9%E6%B3%95">https://github.com/Alvin9999/new-pac/wiki/Linux%E7%B3%BB%E7%BB%9F%E7%BF%BB%E5%A2%99%E6%96%B9%E6%B3%95</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/Linux Access the Open Internet/0.png&quot; width=&quot;60%&quot;/&gt;
&lt;/div&gt;

&lt;h2 id=&quot;Linux-Access-the-Open-Internet&quot;&gt;&lt;a </summary>
      
    
    
    
    <category term="Coding" scheme="http://example.com/categories/Coding/"/>
    
    
  </entry>
  
  <entry>
    <title>2024 Annual Summary</title>
    <link href="http://example.com/2024/12/29/2024%20Annual%20Summary/"/>
    <id>http://example.com/2024/12/29/2024%20Annual%20Summary/</id>
    <published>2024-12-29T01:00:00.000Z</published>
    <updated>2025-04-28T09:20:54.393Z</updated>
    
    <content type="html"><![CDATA[<h2 id="年度关键词"><a href="#年度关键词" class="headerlink" title="年度关键词"></a>年度关键词</h2><center><font color="#1E90FF" size="5"><strong>Wabi-sabi</strong></font></center><h2 id="视频总结"><a href="#视频总结" class="headerlink" title="视频总结"></a>视频总结</h2><center><iframe src="//player.bilibili.com/player.html?isOutside=true&aid=113737193227374&bvid=BV1vj6eYhE6T&cid=27599308006&p=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height="300"></iframe></center><h2 id="关于-Annual-Summary-改进方案"><a href="#关于-Annual-Summary-改进方案" class="headerlink" title="关于 Annual Summary 改进方案"></a>关于 Annual Summary 改进方案</h2><p>年终总结至今已有 7 年, 重新思考下年度总结的定位, 近几年年终总结是有价值的, 但不够高效, 原因在于<br>1.之前年终总结内容中集中于 [工作内容回顾] [工作方法论总结] [价值观念和学习效率方法论上的转变阐述]，更偏向于 [阐述]，有点像是国外个人效率提升书籍的风格, 但构建年终总结的初衷还不是对外作为个人内容的宣发, 是对于自己效率提升的精神原则强化, 是对自己看的内容, 是训练大脑构建一种有监督的网络, 形成一种做事层面的个人习惯和肌肉记忆<br>2.人之所以走向更高效的产出和更专业的运作, 或者变得更优秀, 关键的词是 “做事” 或者说 “执行”, 而不是 “阐述”, 实践出真知, 坚持多做事, 几乎 (不总结) 是一种显然更高效的状态, 总结的方式应该只有一种: “趁热总结”, 总结的维度压缩到”上个月”, “上周” 这样的时间节点, 赶上的是 “趁热总结”, 有且仅有 “趁热” 的总结才是有意义的总结, 然后快速转向下一个方向去做 “执行” 或者 “试错”; 因此谈到 “总结” 方向应该集中在, 聚焦于 [做事过程中调整的速度] 和 [做事的量] 仅仅这两个方面<br>3.文字是一种总结方式, 今年开始保留文字, 增加视频内容总结<br>4.年中总结具体的做法改进方案如下:<br>(i). 年终总结保留, 减少 [经历和理念阐述] 的篇幅到 2000 字以内, 文字部分写作时间加上上线不得超过 2h<br>(ii). 拍摄视频内容总结  </p><h2 id="工作内容"><a href="#工作内容" class="headerlink" title="工作内容"></a>工作内容</h2><p>1.<strong>Tiktok 广告算法</strong> 国际化广告业务中的关键思考:<br>(i). <strong>重隐私保护下的技术选型</strong> 业务算法重度依赖数据, 而在隐私保护政策下, 如何合理地满足隐私规则的条件下, 去完成算法优化这个问题成为投入占比 40% 以上的问题; 一个好的海外业务算法工程师, 首先是一个隐私保护规则的专家 &amp; 隐私保护构建之上的技术专家<br>(ii). <strong>地理文化差异下的业务算法策略</strong> 大胆激进探索局部的策略, 好比山地作战我们打游击, 平原战场我们堆火炮, 不可以一视同仁. 东南亚的发展中国家的兄弟们不可能有美帝公民消费能力, 从小灌输品牌心智的欧洲人也很不会崇尚发展中国家口中的 “性价比”; 从来没有教科书上的正确的算法策略, 只有最大胆小心求证的算法策略<br>(iii). <strong>团队效率提升来自于高效迭代 pipeline 的构建</strong> 国内广告算法基础建设上构建了稳定有效的 pipeline, 但海外业务的 pipeline 在基础建设上经常出现模块间相互 block 的现象, 导致整体的效率难以做到最大化, 但在高压和急迫的业务目标下通常需要作出一定程度的妥协, 进而无法形成较强的飞轮效应; 其实不仅局限于广告业务, 任何增长很快的业务或者产品, 无一例外都是在紧张的业务迭代中构建了下限很高的 pipeline, 进而使得迭代能够以高效的方式去运行  </p><h2 id="工作方法论"><a href="#工作方法论" class="headerlink" title="工作方法论"></a>工作方法论</h2><p>1.<strong>高级对比工程师</strong><br>对于一个工程师来说, 一切工作皆为 “对比 (学习)”, 贯穿多年的工作形态是 “对比”, 例如<br>(i). 不同方法的 AB 对比<br>(ii). 业务现状的新旧对比, 反馈数据的新旧对比<br>(iii). 同一个技术问题不同解决方法的对比<br>当我们在 “对比” 的时候, 我们就是在执行 “工作”, 高效工作的常态就是更高效地 “对比”  </p><p>2.<strong>打磨是决定任务完成质量中最重要的事情</strong> 打磨实现的效果是, 通过 [实验] 或者 [持续训练] 得到一种解决方案或者习惯, 使得<br>(i). 避免运气的干扰<br>(ii). 避免意外状况的干扰<br>(iii). 避免环境机制内波动的干扰<br>(iv). 避免任何不扎实的因素影响到最终的结果  </p><p>3.<strong>一脚刹车, 一脚油门</strong> 在做事关键的节奏上, 主动慢一下. 一个高效执行的节奏应该是一个 “快慢刀”, 在快速的推进中的某些节点上是很慢的, 在很 “慢” 节奏的理解和推翻中, 马上又很快地去快马加鞭地求证. 在这种设定下, 可能我们会经常出现 “哎等等, 你再说一遍, 哎等等, 你再说一遍, 这个东西到底是怎样的” 类似的场景 </p><p>4.<strong>“理事” 和 “干事” 的比例优化</strong> 把做一件事做成可以分为 “理事” 和 “干事” 两类状态, 其中<br>(i). 理事: 梳理清楚业务的现状是什么, 要达成的目标是什么, 收益空间来自于是什么, 实现的方法是什么, 可能的风险是什么. 以及为了梳理清楚上面的几个 “xx 是什么” 而进行的调研和分析, 包括数据分析和意见咨询, 和源码阅读等. 所谓 “不可不察也”, 就是说明 理事的重要性<br>(ii). 干事: 实际的执行. 除了理事以外的剩余的事情<br>这里这两件事的投入时间比例决定了一个人能力的高低, 能力越强的人, 理事占比越高, 理事更加清晰有效, 能够从复杂的业务和复杂的系统中一眼看出来风险, 关键点, 本质, 运动规律和未来展望; 这里不是说理事的人干事不行, 优秀人才都默认能 100% 完成 “干事” 的事, 在我的视野下, 优秀的人 “理事” 的精力消耗占比不应该低于 50%, 维持在 60%-70%, 追求 80%-90%  </p><p>5.<strong>算法工程师的两类问题</strong> 作为算法工程师, 面对一个问题, 需要判断一个 (要解决的) 问题是 (更应该被定义为) 数学上的道理解决的问题还是一个系统问题 (系统链路, 系统模块之上的和所需要的信息量上的道理解决的问题)  </p><p>6.<strong>两类招聘小弟的选择原则</strong><br>(i). 能持续提供聪明的 idea 的<br>(ii). 不能持续提供聪明的 idea, 但是能持续超级勤奋, 不怕脏活累活  </p><h2 id="价值观升级"><a href="#价值观升级" class="headerlink" title="价值观升级"></a>价值观升级</h2><p>1.<strong>wabi-sabi</strong> 来自于日本的一种哲学理念, 中文没有任何靠谱的翻译, 广泛应用于艺术设计和公司管理和生活理念, 大意是 “默认认可万物的不完美性和持续变化性, 进而获得更持久的进步或者更和谐的状态” 首先认可万物都是处于一种不完美的状态, 为什么优秀的日本人是不断努力改进和追求目标呢? 因为当前这个状态是不完美的, 所以我每天早晨起来去改进这件事, 但是这个不完美不是我的一种 “错误”, 是一种普遍存在的哲学状态  </p><p>2.<strong>start your day in you own life and not in somebody else’s life</strong> 社交媒体泛滥的时代, 我们很容易被卷入 somebody else’s life, 比如 xx 获奖了, xx 房子降价了, 某个大佬又说什么名言金句了. 这都是 somebody else’s life, 我们每天都应 start day in own life, 时间放在自己的生活上  </p><p>3.<strong>耐心是最牛逼的品质</strong>, 生活中一切有挑战性的问题都是耐心练习, 每天都要做耐心练习, 并且把所有遇到的挑战当作是 “耐心练习的机会”, 啊上帝你真好, 赐给了我一个真么好的机会让我练习耐心 </p><p>4.<strong>趁热总结</strong> 世界上只有一种有效的总结, 叫做 “趁热总结”, 总结当且仅仅当和下一步执行紧紧结合在一起, 总结才有意义, 和吃鱼差不多, 不管什么鱼只有新鲜的才好吃; 不管是项目还是自我提升的过应该类似一个车轮一样, 有 2 个点, 想法 (总结) =&gt; 执行  =&gt; 想法 (总结) =&gt; 执行 的滚轮, 如果不滚起来, 单纯的 “总结” 没有任何意义   </p><p>5.<strong>Thinking like a farmer</strong> 把自己的一年的工作当作是春天种地, 夏天劳作, 秋天收获的过程, 每天和每天的之间的差异就在于一天内从早到晚的过程, 比如今天我种下了多少颗种子, 翻了多少土, 翻了多少土回家的时候, 晚上不去思考今年我会收获多少种子能赚多少钱, 或者今年可能收获很少可能赚不了多少钱  </p><p>6.<strong>宁静致远</strong>: 什么是宁静致远? 宁静这个词实在是抽象, 来自我们中国的概念总是有着很高的理解成本, 需要更多的阅历和更多的思考才能感受到前人的智慧. 但今年第一次有了一些体验. 好比一个由多个零件构成的快速运转的机器, 局部有很精确的处理, 达到全局的有效化  </p><p>7.<strong>have a wonderful rest of you day</strong> 这是一个来自斯坦福同事写道的一句话, 我发现理论出身水平越高的人, 更十分擅长提供情绪价值, 虽然是一句很简单的话, 但让我记住了很久  </p><h2 id="学习方法论"><a href="#学习方法论" class="headerlink" title="学习方法论"></a>学习方法论</h2><p>1.<strong>“学东西”其实是个伪命题</strong>, “学东西” 是个非常低级别的概念, 因为没有和目标结合的 “学东西” 毫无意义, 有且仅有目标的时候, 学东西这个概念才是生效的  </p><p>2.<strong>问, 是唯一能做的关键事情</strong> 任何公开的资料都能我们都可从互联网上获取, 但是能否主动的提问, 以及能和谁去提问, 拉开了我们的学习的效率差距. 我们能做的只有更主动的问. 就像苏格拉底一样, 我知道 “我不知道” 是最重要的事情, 因为我知道我不知道, 所以我可以和你讨论, 和你提问, 我能通过和你讨论或者提问有所收获;</p><p>3.<strong>无缝插入的提问</strong> 随机插入的任何时间地点任何时间任何形式的提问, 并交谈中强行插入询问, 比约会 &amp; 开会, 约饭要高效的多, 一方面不会考虑立场和其他的因素, 给到一个更直接更本能的描述; 同时也不需要消耗更多的时间  </p><p>4.<strong>学习一个东西, 本质是学习这个东西的边界</strong> 学习这件事情, 学习的知识的内容不是最重要的, 学习到的内容的能力的边界是最重要的, 当我们能完全理解清楚我们要学习的内容的能力边界, 学习这件事情就结束了; 注意这里不是说对于一个内容不了解它内部的结构和本质, 因为如果不了解本质, 也就无法知道完整的边界; 之所以强调这个概念, 是为了将 “学习” 和 “世界” 做连接本身比沉浸在细节中重要的多   </p><p>5.<strong>论文笔记要以面带点</strong>. paper notes 的方式不够高效, 能转化成方向性的梳理的 paper notes, 一定用方向性的 paper notes, 不单独写 paper notes  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;年度关键词&quot;&gt;&lt;a href=&quot;#年度关键词&quot; class=&quot;headerlink&quot; title=&quot;年度关键词&quot;&gt;&lt;/a&gt;年度关键词&lt;/h2&gt;&lt;center&gt;
&lt;font color=&quot;#1E90FF&quot; size=&quot;5&quot;&gt;
&lt;strong&gt;Wabi-sabi&lt;/s</summary>
      
    
    
    
    <category term="Rethinking" scheme="http://example.com/categories/Rethinking/"/>
    
    
  </entry>
  
  <entry>
    <title>Ubuntu 24.04 Upgrade Log</title>
    <link href="http://example.com/2024/10/30/Ubuntu%2024.04%20Upgrade%20Log/"/>
    <id>http://example.com/2024/10/30/Ubuntu%2024.04%20Upgrade%20Log/</id>
    <published>2024-10-30T12:00:00.000Z</published>
    <updated>2025-04-28T09:20:54.413Z</updated>
    
    <content type="html"><![CDATA[<div align="center"><img src="/imgs/Ubuntu 24.04 Upgrade Log/0.png" width="80%"/></div><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>1.解决中文输入法直接挂掉: 设置输入法<br>2.解决界面卡顿: nvidia-driver 重安装<br>3.解决鼠标右键不能用: 重装 gnome-tweaks  </p><h2 id="解决中文输入法直接挂掉-设置输入法"><a href="#解决中文输入法直接挂掉-设置输入法" class="headerlink" title="解决中文输入法直接挂掉: 设置输入法"></a>解决中文输入法直接挂掉: 设置输入法</h2><p>1.去 Region &amp; Language 里面修改引擎为 IBus, (印象中之前是设置为 Fcitx4 的)<br>2.输入法的 Preference 里面修改 mode 为 full pinyin (升级完成自动变成了 Double yinpin)  </p><h2 id="解决界面卡顿-nvidia-driver-重安装"><a href="#解决界面卡顿-nvidia-driver-重安装" class="headerlink" title="解决界面卡顿: nvidia-driver 重安装"></a>解决界面卡顿: nvidia-driver 重安装</h2><p>发现升级完成之后界面直接变卡了, 原因是 nvidia-driver 挂了, 锁定原因的现象是  </p><pre><code class="lang-bash">nvida-msi</code></pre><p>发现没有输出结果, 解决方式是</p><pre><code class="lang-bash">sudo ubuntu-drivers install</code></pre><p>然后重新启动</p><pre><code class="lang-bash">reboot</code></pre><p>发现界面变得正常丝滑起来了</p><h2 id="解决鼠标右键不能用-重装-gnome-tweaks"><a href="#解决鼠标右键不能用-重装-gnome-tweaks" class="headerlink" title="解决鼠标右键不能用: 重装 gnome-tweaks"></a>解决鼠标右键不能用: 重装 gnome-tweaks</h2><p>发现鼠标右键直接不能用了</p><pre><code class="lang-bash">sudo apt install gnome-tweaks</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. <a href="https://ubuntu.com/server/docs/nvidia-drivers-installation">https://ubuntu.com/server/docs/nvidia-drivers-installation</a>  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/Ubuntu 24.04 Upgrade Log/0.png&quot; width=&quot;80%&quot;/&gt;
&lt;/div&gt;

&lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;head</summary>
      
    
    
    
    <category term="Coding" scheme="http://example.com/categories/Coding/"/>
    
    
  </entry>
  
  <entry>
    <title>LLaMA</title>
    <link href="http://example.com/2024/09/05/LLaMA/"/>
    <id>http://example.com/2024/09/05/LLaMA/</id>
    <published>2024-09-05T12:00:00.000Z</published>
    <updated>2025-04-28T11:04:15.181Z</updated>
    
    <content type="html"><![CDATA[<div align="center"><img src="/imgs/LLaMA/llama.png" width="80%"/></div><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>1.LLaMA<br>2.LLaMA2 和 LLaMA-2-Chat<br>3.LLaMA3  </p><h2 id="LLaMA"><a href="#LLaMA" class="headerlink" title="LLaMA"></a>LLaMA</h2><p>参数量: 7B/13B/33B/65B<br>上下文长度: 2K<br>数据集大小: 1.4 Trillion token  </p><h3 id="LLaMA-预训练数据"><a href="#LLaMA-预训练数据" class="headerlink" title="LLaMA 预训练数据"></a>LLaMA 预训练数据</h3><div align="center"><img src="/imgs/LLaMA/0.png" width="60%"/></div><p>exhuasively,<br>1.English CommonCrawl: 英文爬虫数据, 占比高达 67%, 绝对的大头数据, 训练了 1.10 个 epoch<br>2.C4: 探索实验的时候, 发现使用不同预处理的方法后的 CommonCrawl 可以提高性能.<br>3.Github: 基于 Google BigQuery 得到的 Github 数据, 只用了 Apache/BSD/MIT licenses 的数据. 采用两种规则过滤: 利用每行的长度和数字字母的比例干掉有杂质的数据, 用正则表达式过滤掉 header 之类的信息. 最后再做文件级别的去重操作.<br>4.Wikipedia: 维基百科, 高质量的数据, 占比 4.5%, 训练了 2.45 个 epoch. 覆盖 20 种语言. 删掉了各种链接, 评论和其他的 boilerplate<br>5.Books: 包括两个数据集: Gutenberg 和 Book3<br>6.ArXiv: 学术论文, 高质量数据, 占比 4.5%, 训练 1 个 epoch<br>7.StackExchange: 高质量的问答数据  </p><h3 id="LLaMA-Tokenizer"><a href="#LLaMA-Tokenizer" class="headerlink" title="LLaMA Tokenizer"></a>LLaMA Tokenizer</h3><p>1.Tokenizer 采用 byte-pair encoding algorithm (BPE) 采用 Sentence-Piece 实现<br>2.所有数字 split 成 individual digits<br>3.未知的 UTF-8 字符用 byte 表示<br>4.词表大小 32K</p><h3 id="LLaMA-Arichitecture"><a href="#LLaMA-Arichitecture" class="headerlink" title="LLaMA Arichitecture"></a>LLaMA Arichitecture</h3><p>基于 transformer decoder-only 主架构上, 上做了如下的改进</p><h3 id="Pre-normalization-RMSNorm-normalizing"><a href="#Pre-normalization-RMSNorm-normalizing" class="headerlink" title="Pre-normalization: RMSNorm normalizing"></a>Pre-normalization: RMSNorm normalizing</h3><p>1.相比原始的 transformer 架构, 采用了 pre-normalization 的方式: 也就是说在 input 之后先过 layer-norm 再 attention, 而不是传统方法上的先 attention 再 layer-norm  </p><p>2.将 pre-normalization 的方式从原始的 layer-normalization 转化成了 RMSNorm</p><div align="center"><img src="/imgs/LLaMA/1.png" width="60%"/></div><p>RMSNorm 是一种简化版本的的 LayerNorm, 相比原始的 LayerNorm, RMS 稳定性和泛化性都更好, 效率提升了 10-50%  (有这么强吗?) </p><p>RMSNorm 是 Root Mean Square Normalization Layer 均方根正则化层, 计算公式如下</p><script type="math/tex; mode=display">\overline a_i=\frac{a_i}{RMS}=\frac{a_i}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}a_i^2}}</script><h3 id="SwiGLU-activation-function"><a href="#SwiGLU-activation-function" class="headerlink" title="SwiGLU activation function"></a>SwiGLU activation function</h3><p>SwiGLU 是一个激活函数, 用来取代标准的 ReLU</p><h3 id="RoPE-旋转位置编码"><a href="#RoPE-旋转位置编码" class="headerlink" title="RoPE 旋转位置编码"></a>RoPE 旋转位置编码</h3><h3 id="训练参数"><a href="#训练参数" class="headerlink" title="训练参数"></a>训练参数</h3><p>Opitimizer<br>AadamW<br>beta_1 = 0.9<br>beta_2 = 0.95<br>weight decay = 0.1<br>gradient clip = 1.0<br>warmup_step = 2000</p><h2 id="LLaMA2"><a href="#LLaMA2" class="headerlink" title="LLaMA2"></a>LLaMA2</h2><p>LLaMA2 是 LLaMA 升级版本, 同时 LLaMA2 基础上 finetune 了一个 LLaMA2-Chat 用于对话场景.<br>参数量: 7B/13B/70B, 还有个 34B 版本未公布<br>上下文长度: 4K (+100%)<br>数据集大小: 2.0 Trillion token (+40%)</p><p>LLaMA1 v.s. LLaMA2 宏观对比</p><div align="center"><img src="/imgs/LLaMA/2.png" width="60%"/></div><h3 id="LLaMA2-预训练数据"><a href="#LLaMA2-预训练数据" class="headerlink" title="LLaMA2 预训练数据"></a>LLaMA2 预训练数据</h3><blockquote><p>LLaMA-2 adopts a new mixture of pre-training data (i.e., sources that are known to be high-quality and factual are sampled more heavily) and increases the size of the pre-training dataset by 40%.</p></blockquote><p>intuitively,<br>1.采用了新的混合数据, 数据质量是显著很高的, 且事实类的数据被更大量的采样, 相比 LLaMA1 数据增加了 40%, 不得不说 效果都靠堆砌数据啊   </p><h3 id="LLaMA2-Tokenizer"><a href="#LLaMA2-Tokenizer" class="headerlink" title="LLaMA2 Tokenizer"></a>LLaMA2 Tokenizer</h3><p>1.Tokenizer 采用 byte-pair encoding algorithm (BPE) 采用 Sentence-Piece 实现<br>2.所有数字 split 成 individual digits<br>3.未知的 UTF-8 字符用 byte 表示<br>4.词表大小 32K</p><h3 id="LLaMA2-Architecture"><a href="#LLaMA2-Architecture" class="headerlink" title="LLaMA2 Architecture"></a>LLaMA2 Architecture</h3><p>LLaMA2 核心改动是采用了 Grouped-Query Attention (GQA)</p><h3 id="Grouped-Query-Attention-GQA"><a href="#Grouped-Query-Attention-GQA" class="headerlink" title="Grouped-Query Attention (GQA)"></a>Grouped-Query Attention (GQA)</h3><p>其实是原始的 multi-head self attention 和 multi-query attention 的一个折中选择   </p><div align="center"><img src="/imgs/LLaMA/3.png" width="60%"/></div><p>将总共 N 个 self-attention head 分成了几个组, 每个组里面用一个</p><p>思考: 这个 Group_num 怎么调优呢?  </p><h2 id="LLaMA-2-Chat"><a href="#LLaMA-2-Chat" class="headerlink" title="LLaMA-2-Chat"></a>LLaMA-2-Chat</h2><p>有了 LLaMA2 之后, SFT + RLHF 搞出来 LLaMA-2-Chat</p><h2 id="LLaMA3"><a href="#LLaMA3" class="headerlink" title="LLaMA3"></a>LLaMA3</h2><p>参数量: 8B/70B/405B<br>上下文长度: 128K (显著增大)<br>数据集大小: 15 Trillion tokens (显著增大)</p><h3 id="LLaMA3-预训练数据"><a href="#LLaMA3-预训练数据" class="headerlink" title="LLaMA3 预训练数据"></a>LLaMA3 预训练数据</h3><p>Meta 自己构建一个数据集, 并做了大量的洗数据操作:  </p><h4 id="PII-and-safety-filtering-personally-identifiable-information"><a href="#PII-and-safety-filtering-personally-identifiable-information" class="headerlink" title="PII and safety filtering personally identifiable information"></a>PII and safety filtering personally identifiable information</h4><p>过滤掉个人身份信息</p><h4 id="Text-extraction-and-cleaning"><a href="#Text-extraction-and-cleaning" class="headerlink" title="Text extraction and cleaning"></a>Text extraction and cleaning</h4><p>原生 HTML 数据清理</p><h4 id="De-duplication-去重操作的-3-个层级"><a href="#De-duplication-去重操作的-3-个层级" class="headerlink" title="De-duplication 去重操作的 3 个层级"></a>De-duplication 去重操作的 3 个层级</h4><p>1.URL-level de-duplication<br>2.Document-level de-duplication: 采用的是 MinHash算法<br>3.Line-level de-duplication: 行级别去重, 采用 ccNet, 在每 3000 万 doc 中移除出现超过 6 次的行 </p><h4 id="Heuristic-filtering"><a href="#Heuristic-filtering" class="headerlink" title="Heuristic filtering"></a>Heuristic filtering</h4><p>启发式过滤低质量文档或者重复太多的文档   </p><p>1.计算重复 n-gram 覆盖率比例, 来干掉日志记录或者错误消息组成的行, 这种就是很长很独特但没有意义的东西<br>2.使用 “dirty word” 计数这种启发式规则过滤成人网站 </p><h4 id="基于模型的质量过滤"><a href="#基于模型的质量过滤" class="headerlink" title="基于模型的质量过滤"></a>基于模型的质量过滤</h4><p>1.用 fasttext 训练一个是否被维基百科引用的分类器<br>2.基于 LLaMa2 预训练计算一个 Roberta 分类器</p><h4 id="多语言数据"><a href="#多语言数据" class="headerlink" title="多语言数据"></a>多语言数据</h4><p>1.使用 fasttext 将文档分成 176 中语言</p><h4 id="确定数据比例"><a href="#确定数据比例" class="headerlink" title="确定数据比例"></a>确定数据比例</h4><p>为了构建高质量的数据集, 必须确定预训练数据中不同数据源的比例<br>1.首先训练一个知识类别分类器, 对 web 上出现太多的类别进行采样减少<br>2.怎么确定最优的比例 ? 在一个数据比例上训练几个小模型, 并使用这些数据比例来预测大模型的性能. 然后搞几个不同的比例, 对比不同数据比例上的效果, 然后重复这个过程, 确定比例候选, 然后再这个比例上选一个更大的模型, 评估该模型在几个关键基准上的表现<br>3.数据比例总结: 50% 常识 token, 25% 数学和推理 token, 17% 的代码 token, 8% 的多语言 token</p><h3 id="LLaMa3-Architecture"><a href="#LLaMa3-Architecture" class="headerlink" title="LLaMa3 Architecture"></a>LLaMa3 Architecture</h3><p>LLaMa3 在架构上有 4 个核心优化<br>1.Group query attention: 延续 GQA, 采用 8 个 k-v heads 来提升 inference 效率<br>2.Attention Mask: 采用了一种 attention mask 方式, 目的是为了避免出现这样一种情况: 对同一个序列, 在不同 doc 之间的 self-attention 是不同的;  发现在标准预训练的过程中, 这个操作没什么影响, 但是在对超长序列预训练的时候, 这个很重要<br>3.采用了个 128K token 的词表, 其中有 100K 来自于 tiktoken tokenizer, 28K 来自于非英语语种<br>4.调整了 RoPE 的一个超参数: base frequency hyperparameter 到 50K, 目的是能支持更长的上下文, 有文献曾经说明最长支持 32768 的长度</p><h2 id="Now-My-Perspective"><a href="#Now-My-Perspective" class="headerlink" title="Now My Perspective"></a>Now My Perspective</h2><p>1.相比 OpenAI 那种 Closed AI, 这是真 OpenAI</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. LLaMA: Open and Efficient Foundation Language Models.<br>[2]. LLaMA-1 技术详解. <a href="https://zhuanlan.zhihu.com/p/648774481">https://zhuanlan.zhihu.com/p/648774481</a><br>[3]. LLaMA 2: Open Foundation and Fine-Tuned Chat Models.<br>[4]. LLaMA 超详细解读（paper &amp; code）. <a href="https://zhuanlan.zhihu.com/p/632102048">https://zhuanlan.zhihu.com/p/632102048</a>.<br>[5]. LLaMA-2 from the Ground Up. <a href="https://cameronrwolfe.substack.com/p/LLaMA-2-from-the-ground-up">https://cameronrwolfe.substack.com/p/LLaMA-2-from-the-ground-up</a><br>[6]. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.<br>[7]. Root Mean Square Layer Normalization.<br>[8]. The Llama 3 Herd of Models. <a href="https://arxiv.org/pdf/2407.21783">https://arxiv.org/pdf/2407.21783</a><br>[9]. Llama 3.1技术报告（精华版）. <a href="https://zhuanlan.zhihu.com/p/712251536">https://zhuanlan.zhihu.com/p/712251536</a>  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/LLaMA/llama.png&quot; width=&quot;80%&quot;/&gt;
&lt;/div&gt;

&lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;</summary>
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>Large Language Model</title>
    <link href="http://example.com/2024/06/30/Large%20Language%20Model/"/>
    <id>http://example.com/2024/06/30/Large%20Language%20Model/</id>
    <published>2024-06-30T12:00:00.000Z</published>
    <updated>2025-04-28T11:04:15.168Z</updated>
    
    <content type="html"><![CDATA[<div align="center"><img src="/imgs/Large Language Model/llm.png" width="80%"/></div><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>1.Large Language Model == Next Token Prediction 大语言模型就是单字接龙模型<br>2.LLM 怎么 train?<br>3.LLM 怎么 inference?<br>4.Emergent abilities 涌现能力<br>5.Why LLM work in Everywhere? 为啥 LLM 是真的能 work?<br>6.LLM: A Way to AGI LLM 是走向通用人工智能一条路径</p><h2 id="Large-Language-Model-Next-Token-Prediction-大语言模型就是单字接龙模型"><a href="#Large-Language-Model-Next-Token-Prediction-大语言模型就是单字接龙模型" class="headerlink" title="Large Language Model == Next Token Prediction 大语言模型就是单字接龙模型"></a>Large Language Model == Next Token Prediction 大语言模型就是单字接龙模型</h2><p>涉及到自然语言的任务, (几乎) 所有的任务都可以被转化成 Next Token Prediction 问题; 所以 2024.11.01 目前来看 LLM 其实就是在做 Next Token Prediction, 还是遵循的 GPT 原始的思想   </p><p>比如 QA Task:<br>Where is Tsinghua hua University ? Answer:<br>=&gt; Beijing</p><p>比如机器翻译<br>Machine Translation:<br>Tom chases Jerry<br>汤姆追杰瑞</p><p>仍然可以理解为 Next Sentence Prediction<br>Tom chases Jerry =&gt; 汤<br>Tom chases Jerry 汤 =&gt; 姆<br>Tom chases Jerry 汤姆 =&gt; 追<br>Tom chases Jerry 汤姆追 =&gt; 杰<br>Tom chases Jerry 汤姆追杰 =&gt; 瑞</p><h2 id="LLM-怎么-train"><a href="#LLM-怎么-train" class="headerlink" title="LLM 怎么 train ?"></a>LLM 怎么 train ?</h2><p>前置条件: 需要收集海量的高质量的数据, 并进行有效的预处理; 虽然我们谈到模型总会提到各种模型结构, 炼丹技巧, 或者推理性能等等, 但是最核心的还是原始高质量数据的获得</p><div align="center"><img src="/imgs/Large Language Model/0.png" width="100%"/></div><p>intuitively,<br>1.从互联网找到一切可用的高质量数据, 然后进行清洗工作, 具体来说就是 filtering &amp; selection: 过滤掉各种噪声数据和 dirty word, 过滤的方法其实是启发式+模型的结合, 完全取决于对数据的一种理解; 接下来就是去重, 去重需要做到各种级别: sentence-level, doc-level 或者 url-level 等等; 去重之后就要进行隐私保护, 可以理解为一种特殊的过滤<br>2.语料质量搞干净之后, 就需要 token 化, 这里涉及到一些算法<br>3.在具体训练的时候还会遇到很多问题, 比如数据混合的比例和 scaling law 的实验测试  </p><p>当上述数据 ready 之后, 就可投入训练了, 训练过程分为三个阶段  </p><blockquote><p>Self-Supervised Pre-Training<br>=&gt; Supervised Fine-Tuning<br>=&gt; Reinforcement Learning from Human Feedback</p></blockquote><p>1.Self-Supervised Pre-Training (自监督预训练): 收集大量的互联网数据, 利用收集到的无标注数据 training, training 就是在训练样本上做逐个 token 预测, 然后计算 loss 和 gradients, 然后做 optimization<br>2.Supervised Fine-Tuning (SFT, 有监督微调):<br>如果只有第一个阶段无监督的训练, 模型会异想天开扯一堆乱七八糟的东西, 不能拿来为人类所用 (我们先假设我们的目标是为我们所用); 因此 SFT 直接有监督学习我们要的那些个答案; SFT 的过程是必须的, 目前最强的 GPT4 花了 8 个月时间去做 SFT; 需要大量的人工标注的数据, 其实在这给人类已经注入了人工先验; 但是只要涉及到人工的答案, 很多问题没有标准答案, 很多问题有很多种表达形式比如说   </p><p>Q: When was Tsinghua University founded ?<br>模型预测: In 1911, Tsinghua University was founded in Beijing.<br>Label: Tsinghua University was founded in 1911.  </p><p>3.Reinforcement Learning from Human Feedback (RLHF)<br>让模型学习某些 (人类) 偏好 preference, 给多个输出给个打分, 给定人类的偏好, 更好给到模型的灵活性  </p><h2 id="LLM-怎么-inference"><a href="#LLM-怎么-inference" class="headerlink" title="LLM 怎么 inference?"></a>LLM 怎么 inference?</h2><p>LLM Model 输出在所有 token 上的概率分布, 输出过程其实就从分布中采样  </p><h2 id="Emergent-Abilities-涌现能力"><a href="#Emergent-Abilities-涌现能力" class="headerlink" title="Emergent Abilities 涌现能力"></a>Emergent Abilities 涌现能力</h2><p>当 LLM 的参数量超过某个特定的阈值, LLM 就能展现出新的能力, 这种新的能力在小模型完全不具备<br>emergent 这个词, 本质上不是大模型里面提出的, 来自于统计物理学和复杂系统领域: 一个一个的个体行为很简单, 但是个体组成的系统, 会呈现出来超越个体的更强的, 更复杂的能力. 自然界很多这种现象, 水分子会凝结成非常复杂的雪花的形状, 单个水分子行为非常简单, 但是整体形成了非常复杂的规则. 地球的大气和人类的大脑都是非常复杂的系统, 理论上也能学习到相关能力  </p><p>涌现出来什么能力呢 ? 几个例子:<br>1.In-Context Learning: 仅仅给 LLM 几个示例 (sample), 作为一种上下文 (context), 就能类比似的计算出来<br>2.Instruction Following: 我们给 LLM 一个非常复杂的指令, 我们的输入可以非常长, 要求非常多, 逻辑都非常复杂, LLM 能按照你一堆复杂的指令给出来<br>3.Chain-of-Thought: LLM 不仅能输出结果, 还能输出原来任务分解成多个子任务或者中间结果, 一步一步的推理出来最终的结果   </p><h2 id="Why-LLM-work-in-Everywhere-为啥-LLM-是真的能-work"><a href="#Why-LLM-work-in-Everywhere-为啥-LLM-是真的能-work" class="headerlink" title="Why LLM work in Everywhere? 为啥 LLM 是真的能 work?"></a>Why LLM work in Everywhere? 为啥 LLM 是真的能 work?</h2><p>有两个假设<br>1.Everything can be tokenized.<br>2.Every token can be learned.  </p><p>intuitively,<br>1.LLM 能 work 或者说能通用的一个前提假设是: 一切数据都可 token 化, 且这些 token 的表达都可以被学习  </p><h2 id="LLM-A-Way-to-AGI-LLM-是走向通用人工智能一条路径"><a href="#LLM-A-Way-to-AGI-LLM-是走向通用人工智能一条路径" class="headerlink" title="LLM: A Way to AGI LLM 是走向通用人工智能一条路径"></a>LLM: A Way to AGI LLM 是走向通用人工智能一条路径</h2><p>我们从另一个技术发展的角度去思考, 如何走向一个通用的人工智能? 以终为始, 假设我们真要走向一个通用的人工智能, 具体点就是有个很强的通用的模型, 我们幻想下这个模型应该是一个什么的状态? 总接下来这个 [通用] 实现了三个 [统一]<br>1.Unified architecture for various domains. 统一架构, 现在 Transformer 架构一统天下<br>2.Unified model for various tasks. 真正的多任务处理<br>3.Unified model for various modalities. 真正的多模态  </p><p>这么强大的模型, 放在几年前是不敢想象的, 但是 LLM 的兴起是符合上面的三个规律的, 因此让人感觉语言模型可能真的是一条走向 AGI 的路经, LLM 的严谨规律和上面的三个理念能有高度的契合, 令人充满想象空间  </p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. A Survey of Large Language Models.<br>[2]. Large Language Models: A Survey.  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/Large Language Model/llm.png&quot; width=&quot;80%&quot;/&gt;
&lt;/div&gt;

&lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;header</summary>
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>Decoder-only transformer</title>
    <link href="http://example.com/2024/05/30/Decoder-only%20transformer/"/>
    <id>http://example.com/2024/05/30/Decoder-only%20transformer/</id>
    <published>2024-05-30T12:00:00.000Z</published>
    <updated>2025-04-28T11:04:15.181Z</updated>
    
    <content type="html"><![CDATA[<div align="center"><img src="/imgs/Decoder-only transformer/dec.png" width="80%"/></div><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>1.Recap Transformer<br>2.decoder-only 架构<br>3.为什么要用 decoder-only 的架构 ?</p><h2 id="Recap-Transformer"><a href="#Recap-Transformer" class="headerlink" title="Recap Transformer"></a>Recap Transformer</h2><p>采用 encoder-decoder 结构如下图</p><div align="center"><img src="/imgs/Decoder-only transformer/0.png" width="60%"/></div><h2 id="decoder-only-架构"><a href="#decoder-only-架构" class="headerlink" title="decoder-only 架构"></a>decoder-only 架构</h2><p>相比最原始的 encoder-decoder Transformer 架构, decoder-only 把 encoder 这边全部删除 (连带着 encoder-decoder self attention 的连线模块), 可以理解为是多个 decoder block 堆叠组成的, 其中最基础的 decoder-block 的组成就是 masked self-attention 上面跟一个 FFN 层</p><div align="center"><img src="/imgs/Decoder-only transformer/1.png" width="60%"/></div><h2 id="为什么要用-decoder-only-的架构"><a href="#为什么要用-decoder-only-的架构" class="headerlink" title="为什么要用 decoder-only 的架构 ?"></a>为什么要用 decoder-only 的架构 ?</h2><p>理解这个问题有几个理解层次<br>1.首先思考用的为什么是 decoder 结构, 而不是 encoder 结构? 换句话说 encoder-only 不行吗 ?  </p><blockquote><p>why the decoder? The choice of using the decoder architecture (as opposed to the encoder) for LMs is not arbitrary. The masked self-attention layers within the decoder ensure that the model cannot look forward in a sequence when crafting a token’s representation. In contrast, bidirectional self-attention (as used in the encoder) allows each token’s representation to be adapted based on all other tokens within a sequence.</p><p>Masked self-attention is required for language modeling because we should not be able to look forward in the sentence while predicting the next token. Using masked self-attention yields an autoregressive architecture (i.e., meaning that the model’s output at time t is used as input at time t+1) that can continually predict the next token in a sequence.</p></blockquote><p>intuitively,<br>1.解码器中的 mask self-attention 确保模型在学习 token 的表示时不能向前偷看序列, 因为我们采用的是 autoregressive 的架构, 即 t 时刻的输出用在了 t +1 时刻的输入里; 如果要用 encoder 里面的双向的 self-attention 的结构, 就是那就允许 token 学习的时候看到前后序列所有的信息  </p><div align="center"><img src="/imgs/Decoder-only transformer/2.png" width="80%"/></div><p>2.这里有个值得额外思考的地方是, 对于某些特定下游任务, 比如我们只做句子分类, 这种任务没有必要进行 mask attention, 使用双向的 self-attention 对于学习来说其实是有益的  </p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. <a href="https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2?open=false#%C2%A7decoder-only-transformers">https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2?open=false#%C2%A7decoder-only-transformers</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/Decoder-only transformer/dec.png&quot; width=&quot;80%&quot;/&gt;
&lt;/div&gt;

&lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;he</summary>
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>Ubuntu 22.04 Upgrade Log</title>
    <link href="http://example.com/2024/03/22/Ubuntu%2022.04%20Upgrade%20Log/"/>
    <id>http://example.com/2024/03/22/Ubuntu%2022.04%20Upgrade%20Log/</id>
    <published>2024-03-22T02:11:00.000Z</published>
    <updated>2025-04-28T09:20:54.413Z</updated>
    
    <content type="html"><![CDATA[<div align="center"><img src="/imgs/Ubuntu 22.04 Upgrade Log/0.jpg" width="80%"/></div><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>1.安装 Ubuntu 22.04<br>2.git 配置<br>3.terminal: oh my zsh + plugins<br>4.ide: vim + vscode + sublime<br>5.Firefox 设置默认开启视频声音<br>6.微信<br>7.适配 4K 显示的 175% display<br>8.Keyboard input method system 选择 Fcitx 4<br>9.浏览器选择 Google Chrome  </p><h2 id="安装-Ubuntu-22-04"><a href="#安装-Ubuntu-22-04" class="headerlink" title="安装 Ubuntu 22.04"></a>安装 Ubuntu 22.04</h2><p>1.下载 Ubuntu 22.04 LTS: <a href="https://ubuntu.com/download/desktop">https://ubuntu.com/download/desktop</a><br>2.使用 Rufus 制作 Ubuntu 启动盘: <a href="https://zhuanlan.zhihu.com/p/498100251">https://zhuanlan.zhihu.com/p/498100251</a><br>3.安装的时候参考这个界面, 和实际的界面略有不同, 但也可参考: <a href="https://ubuntu.com/tutorials/install-ubuntu-desktop#1-overview">https://ubuntu.com/tutorials/install-ubuntu-desktop#1-overview</a></p><h2 id="git-配置"><a href="#git-配置" class="headerlink" title="git 配置"></a>git 配置</h2><pre><code class="lang-bash">sudo apt-get install -y gitgit config --global user.name goldandrabbitgit config --global user.email goldandrabbit@foxmail.comgit config --listssh-keygen -t rsa -C &quot;goldandrabbit@foxmail.com&quot; gedit ~/.ssh/id_rsa.pub</code></pre><p>然后 github 登录账号新建公钥  </p><p>最后在 ~/ 目录下 .gitconfig 配置 git 的 alias</p><pre><code class="lang-bash">[user]  name = goldandrabbit  email = goldandrabbit@foxmail.com[alias]  co = checkout  ci = commit  br = branch  st = status</code></pre><h2 id="terminal-oh-my-zsh-plugins"><a href="#terminal-oh-my-zsh-plugins" class="headerlink" title="terminal: oh my zsh + plugins"></a>terminal: oh my zsh + plugins</h2><p>通过 curl 请求 oh my zsh</p><pre><code class="lang-bash">sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot;</code></pre><p>查看所有的 shell, 最下面应该新增了 zsh 的 2 个 shell</p><pre><code class="lang-bash">cat /etc/shells</code></pre><p>设置 zsh 为默认的 shell</p><pre><code class="lang-bash">sudo chsh /bin/zsh</code></pre><p>检查默认的 shell</p><pre><code class="lang-bash">echo $SHELL</code></pre><p>增加字体显示工具</p><pre><code class="lang-bash">sudo apt-get install fonts-powerline</code></pre><p>.zshrc 更换主题为 agnoster</p><pre><code class="lang-bash"># ZSH_THEME=&quot;robbyrussell&quot;ZSH_THEME=&quot;agnoster&quot;</code></pre><p>然后在 terminal 设置里面设置成 Solarized-dark/Solarized 主题  </p><p>增加自动补全和语法高亮插件, zsh-autosuggestions 和 zsh-syntax-highlighting  </p><pre><code class="lang-bash">git clone https://github.com/zsh-users/zsh-autosuggestions $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-autosuggestionsgit clone https://github.com/zsh-users/zsh-syntax-highlighting.git</code></pre><p>在 .zshrc 里面启用上面 2 个插件</p><pre><code class="lang-bash">plugins=(  git  zsh-autosuggestions  zsh-syntax-highlighting)</code></pre><h2 id="ide-vim-vscode-sublime"><a href="#ide-vim-vscode-sublime" class="headerlink" title="ide: vim + vscode + sublime"></a>ide: vim + vscode + sublime</h2><p>先来安装 vim</p><pre><code class="lang-bash">sudo apt-get install vim</code></pre><p>在 .zshrc 里面增加 vim 的 alias </p><pre><code class="lang-bash">alias v=&quot;vim&quot;</code></pre><p>在 ~/ 目录下新建 .vimrc 配置</p><pre><code class="lang-bash">set number</code></pre><p>不使用软件商店里面的 code, 因为是一个阉割版本, 直接上官网 <a href="https://code.visualstudio.com/docs/setup/linux">https://code.visualstudio.com/docs/setup/linux</a>  </p><pre><code class="lang-bash">sudo dpkg -i path_to_deb_file</code></pre><p>sublime 安装参考 <a href="https://www.sublimetext.com/docs/linux_repositories.html">https://www.sublimetext.com/docs/linux_repositories.html</a></p><pre><code class="lang-bash">wget -qO - https://download.sublimetext.com/sublimehq-pub.gpg | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/sublimehq-archive.gpg &gt; /dev/nullecho &quot;deb https://download.sublimetext.com/ apt/stable/&quot; | sudo tee /etc/apt/sources.list.d/sublime-text.listsudo apt-get updatesudo apt-get install sublime-text</code></pre><p>在 .zshrc 里面增加 subl 的 alias </p><pre><code class="lang-bash">alias s=&quot;subl&quot;</code></pre><h2 id="Firefox-设置默认开启视频声音"><a href="#Firefox-设置默认开启视频声音" class="headerlink" title="Firefox 设置默认开启视频声音"></a>Firefox 设置默认开启视频声音</h2><p>如果不设定, b站等网页每次打开视频都需要反复开启声音, 需要开启 Autoplay<br>settings =&gt; Privacy &amp; Security =&gt; Autoplay 打开</p><h2 id="微信"><a href="#微信" class="headerlink" title="微信"></a>微信</h2><p>截止 2024.03.30 微信 for linux 正式版本在官网还没有发布, 下面这个版本可以使用<br><a href="https://github.com/lovechoudoufu/wechat_for_linux">https://github.com/lovechoudoufu/wechat_for_linux</a></p><h2 id="适配-4K-显示的-175-display"><a href="#适配-4K-显示的-175-display" class="headerlink" title="适配 4K 显示的 175% display"></a>适配 4K 显示的 175% display</h2><p>1.默认浏览器和 vscode 对于 4K 显示不友好, 需要去 setting =&gt; display 里面设置 175%</p><h2 id="Keyboard-input-method-system-选择-Fcitx-4"><a href="#Keyboard-input-method-system-选择-Fcitx-4" class="headerlink" title="Keyboard input method system 选择 Fcitx 4"></a>Keyboard input method system 选择 Fcitx 4</h2><p>1.Settings =&gt; Region &amp; Language =&gt; 如果没有就添加简体中文汉语, 添加完最下面应该有灰色的 汉语 (中国) =&gt; 最下面 Keyboard input method system 选择 Fcitx 4, 也就是替换了 iBus =&gt; 点击 Apply System Wide, 然后重启系统<br>2.右上角白色的小键盘里面 Configure 选择第一个输入法是 pinyin 并设置字体大小 18, Global Config 选择 ctrl + Space 切换输入法 </p><h2 id="浏览器选择-Google-Chrome"><a href="#浏览器选择-Google-Chrome" class="headerlink" title="浏览器选择 Google Chrome"></a>浏览器选择 Google Chrome</h2><p>1.为什么不选择 Firefox ? 经常莫名的不能打字, 让我一度认为是系统的问题或者键盘的问题<br>2.为什么不选择 Opera ? 界面可能突然打不开, 重启也无法修复<br>因此看起来没得选择, 只能用 Chrome  </p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. <a href="https://maxim-danilov.github.io/make-linux-terminal-great-again/">https://maxim-danilov.github.io/make-linux-terminal-great-again/</a>  Make Linux terminal great again (Terminator + Oh My ZSH + autosuggestions + highlighting + Agnoster theme + powerline fonts + solarized colors) .<br>[2]. <a href="https://www.sublimetext.com/docs/linux_repositories.html">https://www.sublimetext.com/docs/linux_repositories.html</a> </p>]]></content>
    
    
      
      
    <summary type="html">&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/Ubuntu 22.04 Upgrade Log/0.jpg&quot; width=&quot;80%&quot;/&gt;
&lt;/div&gt;

&lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;head</summary>
      
    
    
    
    <category term="Coding" scheme="http://example.com/categories/Coding/"/>
    
    
  </entry>
  
  <entry>
    <title>2023 Annual Summary</title>
    <link href="http://example.com/2024/02/20/2023%20Annual%20Summary/"/>
    <id>http://example.com/2024/02/20/2023%20Annual%20Summary/</id>
    <published>2024-02-20T12:00:00.000Z</published>
    <updated>2025-04-28T09:20:54.393Z</updated>
    
    <content type="html"><![CDATA[<h2 id="年度关键词"><a href="#年度关键词" class="headerlink" title="年度关键词"></a>年度关键词</h2><center><font color="#1E90FF" size="5"><strong>Deep, and more deep</strong></font></center><h2 id="工作内容-打造工业界最-solid-的-ROAS-预估模型方案"><a href="#工作内容-打造工业界最-solid-的-ROAS-预估模型方案" class="headerlink" title="工作内容: 打造工业界最 solid 的 ROAS 预估模型方案"></a>工作内容: 打造工业界最 solid 的 ROAS 预估模型方案</h2><p>今年核心工作目标很 plain &amp; simple, 如何打造工业界最 solid 的精排 ROAS 预估模型? 4 个视角去牵引做功的方向  </p><p>1.<strong>外循环兼容多行业业务差异的统一预估模型</strong><br>外循环广告业务显著特点之一是”不同行业广告投放具备显著的业务差异性”, 差异性反映在 LTV 预估任务之上表现为: 不同行业回传辅助反馈事件及其影响程度的差异性/不同行业有效特征 (分布) 空间的差异性/不同行业深转样本稀疏程度差异性, 主要有以下 4 个迭代方向;<br>(i). 行业回传事件的高效辅助任务建模. 初期迭代中聚焦于基础 task relationship 建模的有效性, 目标识别并建模不同任务在用户行为动线之间的关系; 中后期迭代, 致力于构建更通用的knowledge transfer &amp; specific task 兼备的模型结构<br>(ii). 行业自适应的特征筛选模型. 针对单次模型迭代对不同行业效果提升幅度存在差异问题, 建设面向行业自适应特征的特征筛选模型, 抽象出面向行业自适应特征筛选模块, 将行业特征和行业特点的表示学习直接反馈到模型结构之上; 从关键的用户行为序列特征的角度建模的方法论上, 面向行业自适应的 retrieve-modeling 长序列建模, 提升模型效果上限<br>(iii). 面向行业特性的长期特征工程专项. 梳理并集成不同行业的特征大图, 配合产运打造统一的特征工程模板 pipeline, 并集成高效特征选择工具, 构建一体化的的数据价值验证框架<br>(iv). 面向全行业的中台模型基座. 将建模视角从单一的样本组织转移到 industry-based task 任务组织, 采用 curriculum learning 等框架建模 task 之间的 difficulty ranking, 集成task 之后建立起 Training Scheduler, 能够更灵活地帮助不同的行业同学进行短平快的模型优化  </p><p>2.<strong>序关系和预估准度兼容建模性质下的预估最优性保证</strong><br>面向 ROAS 类广告出价产品, 不同于前链路预估相关出价产品（唤端/激活/表单）, 客户维度 ROAS 属于最末端链路面向效果的出价诉求; 聚焦投中表现看, 因竞价环境的高频变化以及突发性发生的付费行为, 后验 ROAS 以及成本达成存在更显著的波动性. 因此对 LTV 预估任务对预估准度具备更高的精度要求和稳定性要求; 前期大量工作聚焦于 LTV 预估的序关系建模, 后续会重点加强准度和序关系建模兼容的性质, 深入探索准度建模和序关系建模两个关键问题的兼容性质的最优性保证<br>(i). 针对预估分布/预估准度优化的 Muti-view LTV优化. 当前模型引入了回归损失作为优化目标, 但针对 LTV 的的预估值分布的特殊性缺少针对性的建模, 引入更多 view 下的回归目标, 例如通过核密度估计等方法, 有效利用分布信息做值准度建模; 充分采用 contrastive learning 等方法对 大R 和 小R 进行中间结果建模, 充分缓解深度转化中存在的稀疏性问题<br>(ii). Ord2Seq 连续值预估建模范式. 目标打破原有 hard layer transfer 建模带来效果天花板, 采用 ord2seq 框架统筹建模多阶段独立分类框架  </p><p>3.<strong>带有噪声标签的 ROAS 模型学习</strong><br>短视频外循环广告业务显著特点之一是 “广告主对深度转化行为存在显著的 mis-report”, 由于隐私原因等部分广告主存在一定的错误事件上报, 对于模型学习带来了显著的挑战, 使得深度网络模型可能会学到 corrupt representaion; 因此如何进行有效的 learning with noisy label, 是外循环 ROAS 模型优化的另一大关键挑战<br>针对上述问题主要梳理以下核心优化方向:<br>(i). 如何构建更优质纯净的数据集做模型训练. 针对 LTV 样本存在 label noise 问题,<br>a.探索如何移除 noise label 对模型训练的影响, 构建优质/纯净的训练集, 或者采用无监督预训练之后利用特征空间的相似性做样本 label 的 correction, 致力于提升更高质量表示学习过程;<br>b.探索如何弱化 noise label 对模型训练的影响, 采用置信度建模 &amp; selective reweight 的方法, 弱化 noise 信息对模型学习的影响, 提升模型的鲁棒性;<br>(ii). 如何更充分地利用有限且稀疏的信号反馈提升效果. LTV 样本在无法避免的不置信的情况下, 如何提升模型的效果, 采用 contrastive learning/transfer learning 的方式, 充分利用场景内样本以及更宽泛的回传事件样本挖掘转化信息;  </p><p>4.<strong>multi-view 多视角混合下的学习任务</strong><br>ROAS 预估模型在模型结构上, 既不是单纯的回归任务, 也不是单纯的分类问题, 而是一个融合了多个视角下的混合建模任务, 这也是 ROAS 模型的魅力所在. 年终总结暂不铺开  </p><h2 id="工作方法论"><a href="#工作方法论" class="headerlink" title="工作方法论"></a>工作方法论</h2><p>1.算法工程师这份工作赋予了我什么关键能力与关键习惯. 自 2019.06 年, 从事广告算法工程师已经 4.7 年, 不妨在这个时间节点系统地回顾一下, [算法工程师] 这个工作或者这个 title, 赋予了我哪些比较本质的能力和思考问题的方式 ? 即使有一天不做算法工程师了, 哪些做事的方法会给下一个阶段职业生涯提供更强的项目推进的模式  </p><p>(i). <strong>可控试错成本下的快速迭代</strong><br>快速迭代始终是任何产品, 或者任何算法, 在成长期间存在的唯一状态; 一定要在短时间内高度聚焦自己的注意力, 充分验证自己的固有的认知, 猜测的认知, 以及更宽范围内猜测/拿不准的认知; 迭代的更快, 才能更快地接近效率更高的本质的那一种方法论; 在这个过程中只需要满足试错的成本是可控的, 带来的风险不足以摧毁整个产品的生命, 那么就是要大胆的投入在更密集迭代里面; 在算法工程师的世界里面, 我们称之为 [实验]; 我们的实验如果能做的更快一点, 我们就能积累一点点更快的优势, done is better than perfect.  </p><p>(ii). <strong>认可对抗不确定性的价值</strong>: 长期与不确定性的近身肉搏成就了最终的产出价值.<br>任何有价值的工作, 看起来都是存在大量的不确定性的; 比如买一碗面, 为什么一碗放了辣椒和豌豆的面, 就有来自天南海北不同的顾客长期喜欢呢? 售卖一杯奶茶, 为什么只是放了几块新鲜的水果和大量的芝士就能把公司做到上市 ? 这些工作的共性都存在很强的不确定性, 只不过从有些人从这些不确定性里面找到了某些确定性的因素; 从第一天做算法工程师, 我们都在和一个又一个 [不确定性] 在近身肉搏, 只不过在一次一次不确定性中的抉择, 把我们分成了不同的样子; 不确定性会带来压力, 但是价值也确定性的存在在不确定性中; 这可能是我为什么认可或者喜欢算法工程师这个工作的原因, 过程中压力一直存在, 克服了一个又一个困难之后也确定性地存在一些短暂的成就感, 然后向着更大的不确定性去探索    </p><p>(iii). <strong>巧立名目</strong><br>巧立名目这个词, 在中文的语境下是指 [指用欺骗的手段设立各种名目以达到不正当目的] , 但这里我要强行将这个词赋予一个新的褒义的, 正向的, 新时代的含义; 因为对任何事物的刷新认知的过程, 都是从某个不同于常态思维的, 创新的角度, 或者说一个偏门的角度进行开荒, 然后通过长期的迭代和修正, 最终变成一种长久的价值; 具备永久的价值在初期均是 [巧立名目]; 有些时候的离经叛道的开荒路径, 往往是通向事物更本质, 更具备高效生产力的, 打开天花板的, 探索更长久价值的入口所在; 如果一个目标在一开始就是什么条件都是成熟的, 可满足的, 理由明确的, 那么这个事情能带来收益空间是很值得怀疑的  </p><p>(iv). <strong>convincing matters most</strong><br>即使是长期专注于技术研究/迭代的工程师, 也需要相当高水准的 convincing 的能力, 面向不同受众的 convincing 是工作中非常关键的部分; 当我们在 convincing 的时候, 要注重给受众建立一些符合认知的 intuition, 这个 intuition 可能在受众在接受我们的 idea 之前可能是没有的, 缺失的, 或者模糊的, 甚至是有偏的; 在广告算法的工作范围内, 数据是很强的 impact 工具, 任何情况下要把来龙去脉, 前因后果 connect 起来, 并且拉到 bigger picture 下去 带着 context 讨论问题, 是持久要做的事情. 我在这里总结把 convincing 能力分成两个部分, 一部分取决于对事物的本质的认知水平, 另一部分是日常生活中的自我呈现, 且认为后者的占比在很多情况下影响更大  </p><p>(vi). <strong>训练场不带手机</strong><br>近几年工作的广告核心算法团队里面, 不乏有一些优秀的同学. 这些 “优秀的同学”, 都具备一些比较 solid 的素质, 但抽丝剥茧一下, 我理解这些优秀同学最共性的, 最显著的核心竞争力, 仍然还是对于做事的高度专注状态, 能够较长地保持; 这一点我是 buy in 的, 仅从 2023 自我总结来看, 我和心目中很专注的同学相比, 能做到 neck and neck, 但也清楚自己在专注度上的提升空间, 还是能加大药劲, 进一步提升的.   </p><p>2.<strong>more deeper ?</strong><br>广告系统, 或者广告算法系统是一个非常复杂的动态系统, 相比一些职级更高, 影响力更大的老板或者同事, for some case 上他们更具备相对 deeper 的思考.<br>(i). 复杂系统下的耦合与解耦. 就以精排模型优化来说, 多任务建模/延迟转化/序关系建模/准度建模/序列建模/竞争环境建模可能是现在大家广泛讨论的几个问题, 长期来看结合生成模型/因果推断等开放性问题也有较大讨论空间, 因为思考角度的复杂性, 导致模型的损失函数还在以较大的密度去增加, 因此至少有一个可以回头思考的点是, 当前的思考的角度下, 对已有的其他几个优化的角度会产生怎样的影响 ? 是强冲突的影响, 还是弱冲突的影响<br>(ii). 找寻下一个关键的 motivation. 技术的迭代, 说穿了其实就是两件事, 1.认定某个 make sense 的 motivation 2.基于认定的 motivation 找对合适的方法/途径/手段; 时间迭代长了, 越来越来认为找到合理的 motivation 是最正确的线路, 有点类似于 “道 v.s. 术” 类似的概念, 训练自己的大脑能够 capture 下一个关键的 motivation 是未来训练自己成长的关键.  </p><h2 id="价值观升级"><a href="#价值观升级" class="headerlink" title="价值观升级"></a>价值观升级</h2><p>1.<strong>反着理解一切, 加速认知效率</strong><br>(i). 今年建立了个比较关键的价值观, 就是很多事情最先反着理解的人往往是最早吃到螃蟹的, 反着理解很多事, 其实就是最快加强的认知的方法, 坚持某些 [奇奇怪怪] 的理念或者想法, 坚持到有收益, 或者验证出来有收益, 那就是正确的道路; 加速认知迭代的过程, 本质上就是更快的理解那些之前认为不可能的, 大家都不这么想的, 这时候一定要 “我就要这么想, 我没问题, 错的是大家”. 这里有个最基础的假设是, 如果所有的人都能看懂看明白的东西, 那就根本没价值; 如果所有人都认为是有前途的机会, 那就根本不是机会; 如果所有人都看好某个东西, 那就压根没办法从这个东西上获利; 有些话一定要可以告诉自己, 这个话一定得反着听; 嗯你说的没错, 反着来就对了.<br>(ii). 主动认知迭代: 多告诉自己迭代下认知. 另外有些事情, 其实很难一步到位反着理解到底, 往往会处于一种模糊的, 曲折的, 学不懂, 跟不上, 看不明白的状态. 想持有某些特例独行的想法, 也得一定程度上说服自己, 说服自己很难一步到位. 所以一个执行层面的方法是, 我一定要迭代下对 xx 的认知, 好像记录日记本一样, 今天观测到它的数据, 认知更新一下, 明天观测到它的数据, 认知再来一下, 后天观测到新的数据, 认知再来一下; 说服自己加大认知的频率, 可能是一种有效方法.   </p><p>2.<strong>学习拼多多的执行力/组织能力</strong><br>作为在阿里搞过 4 年电商业务的同学, 今年 pdd 市值首次超越阿里, 关于 pdd 一直有着比较多的思考, 就好比足球上我们想学巴西/学德国/学意大利/学日本一样, 其实很难学到什么东西. 但是有一个关键的输入 from 某个老板, 什么都不用学 (其实你也学不会), 就学一点就行了: 学习 pdd 的执行力; pdd 将一家公司分成了三类人:<br>(i). 想注意的人. 这批人对市场理解深刻, 我是十分佩服, 很少的几苗人. 但短期内我能成为这类人的可能性不大.<br>(ii). 做执行的人.<br>(iii). 做监工的人.<br>在这种机制下, 这三类人各自做各自的事情, 尤其是第 2 类人和第 3 类人, 他们主要的优势就是专注的执行力, 机制保证了他们不去考虑第 (i) 类人的问题, 然后把执行力拉满. 于此回想 2018 年在阿里实习时候, 似乎期望哪怕招聘一个实习生, 都想搞成第 (i) 类人的要求并培养成第 (i) 类人的思维模式.  </p><p>3.<strong>人生的目标是做有趣的事情, 因为我们肯定是最无趣的一代</strong><br>2023 年我突然意识到一件事情, 就是我们这一代人在后人眼里也是非常无趣的一代: 每天上班到很晚, 一年到头很努力地工作, 但是也没有发财; 做出来的事情/成果, 后代肯定觉得: 这东西还值得天天加班吗? 毫无创造力, 毫无影响力呀; 真是土到不行土到掉渣还活在自己的回忆里面的一代人. 所以, 既然是这样一个结局, 我考虑尽可能让自己过得更有趣一点, 不管是工作还是生活, 将趣味性还是拉到很高的优先级, 搞点有意思的事情 (就是写年终总结, 我也是尽可能当更趣的一个) 等到20年后再回头看, 某种程度上已经赢了.  </p><p>4.<strong>我幻想 v.s. 我推测</strong><br>[幻想] 这个词通常很少量地出现在特定的场景, 比如小孩子爱幻想 xx 之类的. 但事实上, 成人一般用 [我想] [我认为] [我觉得] 这些词来不经意之间表述事实意义上 [幻想] 含义; 所以在今后的表达中, 为了追求表达的准确性, 我把 [幻想] 这个词扩展到更多的场景里面, 与之相对的是 [推测], 比如有一些较强的事实依据或者相关数据支持的推断里面.  </p><p>5.<strong>轻的打败重的, 除非重不可</strong><br>2023 年写作的量越来越大, 有一个较多的体感就是 vscode 这工具真的太好用了, 忽然有一天打开其他的 ide 简直难受, 必须要夸赞一下这么强大的 ide. 给我的感触最深的是, 这种轻量级的产品, 可以把重的产品降维打击到没有任何还手之力. 越来越喜欢这种产品设计上的轻盈感或者说工具设计上的简约感带来的做事的丝滑感.  </p><p>6.<strong>拿下捷豹, 才能拿下雪佛兰</strong><br>上世纪 4A 广告公司打入汽车市场, 是这些公司发展的一个重要的里程碑. 这里有一个比较基础的规律, 一个重要的里程碑的获得, 一定有之前的一个重要的里程碑. 我不要聚焦追求幻想中的某一个/欲望中最后一个重要的里程碑, 而是追求之前的某一个必经到的里程碑. 然后让自己再尝试进入下一个 level. </p><p>7.<strong>The magic you are looking for is in the work you’re avoiding</strong><br>这句话无需建立一种理解, 每次在脑子里多默读回荡几遍, 就一定能感受到它说什么.  </p><p>8.<strong>找寻 “后劲很大” 的东西</strong><br>2023 经历了几个 “后劲很大” 的东西: 最后的生还者2, 风骚律师, 一天深潜 2 次搞到流鼻血, 被海胆在手上扎了一堆恐怖的刺 (都没看清楚海胆藏在哪) , 关了灯还会突然倒着开的过山车, 口感巨细腻的鹅肝, 空调开到非常变态的候机室… 都 “后劲很大”, 获得的乐趣感和沉浸感是前所未有的. 与之相比, 大城市买房/投资这些事我是感觉没什么意思, 多做点 “后劲很大” 的事情.  </p><h2 id="高效学习方法论"><a href="#高效学习方法论" class="headerlink" title="高效学习方法论"></a>高效学习方法论</h2><p>1.<strong>把一个东西学明白, 首先得欣赏它</strong><br>这一点其实在 2022 年终总结有初步提到, 但是还想再更深一层说明持续高效的学习是一个自然的认知加强过程; 我们通常说 [欣赏] 一个东西, 比如欣赏一幅画, 欣赏一个话剧, 欣赏一个音乐, 仿佛是要 [具备了不低的专业水平/基础认知] 之后, 才能 [欣赏]; 这里我要推翻一下这种理念, 之所以能学明白一个东西, 还是因为某个在模糊认知瞬间下的突然觉醒, 突然醒悟, 之后带来的满足感和回味感, 自然而然变成一种认同感; 然后基于这种 [默认的认同感], 开始密集的, 高强度的学习, 然后继续找到 [更多的乐趣] 或者 [更深层次的认同感] 或者 [更深层次的认同感]</p><p>2.<strong>Intuitively and Exhaustively Explained</strong><br>[直觉上的认知] 和 [详尽的认知], 是一枚硬币的两面. 2023 通过又一年的直觉认知训练, 显著提高了学习的效率, 做的不错. 这俩一个是哥哥, 一个是弟弟, 哥哥主输出, 弟弟打辅助, 就没有学不会的东西.  </p><p>3.<strong>问问自己没有跳过什么必经的阶段 ?</strong><br>一种复杂的能力或者说不容易轻易模仿的能力, 比如 acm 金牌, 数学竞赛冠军. 除了这些人具备的天赋以外, 学习过程中往往是存在一种分阶梯的线性提升思维: 一层扎实, 一层扎实, 一层扎实地递进; 因此想锻炼一种独有的 solid 能力, 能应付各种不同的问题, 多问问自己没有跳过什么必经的阶段? 过程中有没有缺失某种更符合自然直觉的必经的阶段?  </p><h2 id="日常生活中的自我呈现"><a href="#日常生活中的自我呈现" class="headerlink" title="日常生活中的自我呈现"></a>日常生活中的自我呈现</h2><p>1.<strong>我想成为什么样的人和人们对我有何种期待矛盾</strong><br>人的情绪都来自于上面这一个关键的矛盾, 有时候我们会说服自己, 做一个”我想成为的自己”, 有时候我们会说服自己 “成为xx期待中的自己”, 二者总会有存在某个矛盾的瞬间, 使人感到焦虑和迷茫. 对于这一点如何平衡, 暂时没有形成属于我自己的自洽的准则或者理解. 但是区分清楚人生中是存在这两者的不同的, 是走向更好的自己的基础一步.  </p><p>2.<strong>Emotional Value is all I provide</strong><br>情绪价值, 也是今年思考比较多的一个词, 例如像我这种爱写笔记爱总结的, 给我自己提供了大量的价值满足自己, 推进了 “我想成为什么样的人”; but sometimes, 一个人在群体中能向其他个体的提供的核心价值, 其实主要还是情绪价值, 或者说情绪价值 is all we need.  这又属于 “人们对我有何种期待” 的范畴; 最简单的, 美国人用 bro 或者 body 这两个词称呼别人, 中国人喊你叫 “x哥/大佬/x总”, 其实都存在情绪价值上的巨大的差别, 而且这种情绪价值是独立于种族和文化的. 所以, 我 (如果愿意) 能给别人提供的价值, 还是一种情绪价值. 既然是情绪价值, 那做产品, 做技术, 还是做公司, 其实从这个角度去思考就走到了某种正确的道路上.  </p><p>3.<strong>精准营销是是我们这种人创造出来的概念</strong><br>广告行业有个基础理解, 广告存在一种最基本的机制, 类似于创造 [幸福] [爱] [关怀] 这种 concept, 并主动地赋予它一定的意义, 最终的目的, 是把与之相关的想要的东西更高效地卖出去; 在现在人工智能全力推进的今天, 我们又在产品/服务上赋予了 [智能] 这样的标签. 这种标签唯一的意义, 也是让本来也都平庸的服务, 更高效的卖出去. 精准营销, 本来是有一部分人提出的理想的 concept, 但是通过我们一线工程师的手把它实现交付成可以执行的产品, 本质上变成了我们创造出来的概念.  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;年度关键词&quot;&gt;&lt;a href=&quot;#年度关键词&quot; class=&quot;headerlink&quot; title=&quot;年度关键词&quot;&gt;&lt;/a&gt;年度关键词&lt;/h2&gt;&lt;center&gt;
&lt;font color=&quot;#1E90FF&quot; size=&quot;5&quot;&gt;
&lt;strong&gt;Deep, and mo</summary>
      
    
    
    
    <category term="Rethinking" scheme="http://example.com/categories/Rethinking/"/>
    
    
  </entry>
  
  <entry>
    <title>CREAD A Classification-Restoration Framework with Error Adaptive Discretization for Watch Time Prediction in Video Recommender Systems</title>
    <link href="http://example.com/2024/01/20/CREAD%20A%20Classification-Restoration%20Framework%20with%20Error%20Adaptive%20Discretization%20for%20Watch%20Time%20Prediction%20in%20Video%20Recommender%20Systems/"/>
    <id>http://example.com/2024/01/20/CREAD%20A%20Classification-Restoration%20Framework%20with%20Error%20Adaptive%20Discretization%20for%20Watch%20Time%20Prediction%20in%20Video%20Recommender%20Systems/</id>
    <published>2024-01-20T12:20:00.000Z</published>
    <updated>2025-04-28T09:20:54.399Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Motivation-amp-Contribution"><a href="#Motivation-amp-Contribution" class="headerlink" title="Motivation &amp; Contribution"></a>Motivation &amp; Contribution</h2><p>1.给出一种时长预估任务上较为完备的 framework formulation: CREAD<br>2.分析了离散化分桶过程中的误差来源, 认为两种误差不能同时减小<br>3.提出一个观看时长预估分类任务中的自适应分桶的框架  </p><h2 id="Challenges-of-Discretization-分桶离散化的关键挑战"><a href="#Challenges-of-Discretization-分桶离散化的关键挑战" class="headerlink" title="Challenges of Discretization 分桶离散化的关键挑战"></a>Challenges of Discretization 分桶离散化的关键挑战</h2><div align="center"><img src="/imgs/CREAD A Classification-Restoration Framework with Error Adaptive Discretization for Watch Time Prediction in Video Recommender Systems/0.png" width="40%"/></div><p>1.离散化的操作引入了两种误差:<br>(i). Learning Error 学习误差. 什么是”学习误差”? 因采用更多的分桶导致的样本数量有限导致的单个分桶效果的天花板; 如果我们增加分桶数 $M$ , 会导致每个分桶内的样本就会减少, 限制了分类性能<br>(ii). Restoration Error 恢复误差: 这种恢复方式省略了每个桶的详细的概率密度, 会引入误差  </p><div align="center"><img src="/imgs/CREAD A Classification-Restoration Framework with Error Adaptive Discretization for Watch Time Prediction in Video Recommender Systems/1.png" width="50%"/></div><p>值得思考的是, 上述两种误差不能同时减小</p><h2 id="EAD"><a href="#EAD" class="headerlink" title="EAD"></a>EAD</h2><p>等距分桶</p><script type="math/tex; mode=display">t_m=\frac{m}{M} T_{\max}</script><p>等频分桶</p><script type="math/tex; mode=display">t_m-\Psi^{-1}(\frac{m}{M})</script><p>合并为</p><script type="math/tex; mode=display">t_m=\Psi^{-1}[\gamma(\frac{m}{M})]</script><p>2.CREAD 的缩写是 Classification-Restoration framework with Error-Adaptive-Discretization</p><div align="center"><img src="/imgs/CREAD A Classification-Restoration Framework with Error Adaptive Discretization for Watch Time Prediction in Video Recommender Systems/2.png" width="45%"/></div><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. CREAD: A Classification-Restoration Framework with Error Adaptive Discretization for Watch Time Prediction in Video Recommender Systems.<br>[2]. <a href="https://zhuanlan.zhihu.com/p/675844142">https://zhuanlan.zhihu.com/p/675844142</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Motivation-amp-Contribution&quot;&gt;&lt;a href=&quot;#Motivation-amp-Contribution&quot; class=&quot;headerlink&quot; title=&quot;Motivation &amp;amp; Contribution&quot;&gt;&lt;/a&gt;Mot</summary>
      
    
    
    
    <category term="Ads_RecSys" scheme="http://example.com/categories/Ads-RecSys/"/>
    
    
  </entry>
  
  <entry>
    <title>从找事理论到 TRIZ</title>
    <link href="http://example.com/2024/01/11/%E4%BB%8E%E6%89%BE%E4%BA%8B%E7%90%86%E8%AE%BA%E5%88%B0%20TRIZ/"/>
    <id>http://example.com/2024/01/11/%E4%BB%8E%E6%89%BE%E4%BA%8B%E7%90%86%E8%AE%BA%E5%88%B0%20TRIZ/</id>
    <published>2024-01-11T12:11:00.000Z</published>
    <updated>2025-04-28T09:20:54.417Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>1.找工作 v.s. 找事做<br>2.找到红缨枪的问题, 红布才有价值<br>3.What is TRIZ? 什么是 TRIZ 理论 ?<br>4.TRIZ 处理问题的 2 个原则<br>5.Generalizing Problems and Solutions 泛化问题, 泛化方案<br>6.Don’t accept compromises, eliminating contradictions 不接受折中, 消除矛盾  </p><h2 id="找工作-v-s-找事做"><a href="#找工作-v-s-找事做" class="headerlink" title="找工作 v.s. 找事做"></a>找工作 v.s. 找事做</h2><p>1.对于年轻人来说, 最重要的不是”找工作”, 而是”找事”做; 不是找工作, 而是”找事”. 区别在哪?<br>2.”找工作”, 意味着接受一份公司给我的任务, 做完了, 拿到属于我的报酬; 我和社会之间, 始终搁着公司这个组织, 这个媒介, 这个中间人, 这个社会供需关系的代理, 这个agent; 在”找工作”的设定下, 我不用判断我做的事情有没有社会价值, 因为公司已经帮你判断了是有价值的; 我不用判断自己的天赋和能力是否满足这个要做的事情, 因为公司帮我判断了适不适合这个<br>3.”找事做”, 意味着视线要穿透公司这一层, 直接地去思考, 我做的事情, 为社会解决了什么问题, 贡献了什么价值. 那个价值是什么?<br>4.职业是一个生态系统, 有很多物种, 很多物种之间相互依赖. 这就意味着只要有一个能让别人依赖的技能, 就能找到清晰的职业路径. 比如”娱乐圈”是个生态系统, 当不了导演没关系, 导演不是需要制片主任吗? 可以先从这个工作干起, 如果制片主任当不上, 难度太大了, 制片主任总需要场记, 可以先想办法当场记  </p><p>intuitively,<br>1.”找事做”模型, 一方面用于求职过程的这个事情, 同时适合用于在公司工作中的”找事做”模型, 或者说工作中的”找问题”模型<br>2.”找事做”, 这个事情说起来简单, 但是没有经过”长期找事/找问题训练”的工作思维, 往往做事的时候不能够在有限的时间里面把握”问题”主线并拿到令老板结果; 比如在广告系统的算法模型组, 可能有10+个同学都在负责各个出价产品模块的模型, 大家有个共同的基础的价值: 为公司提升预期收入; 没错, 这确实是这件事的价值, 但是为什么有的同学晋升快, 有的同学晋升慢呢? 这里面核心关键点就在于”有没有快速找到那个当务之急最重要的问题, 然后把这个问题的解决方案推进过程中使自己成为 “1号位”, 并交付令所有人难以挑战的结果; 比如广告模型相比推荐模型的一个点在于需要密切关心”预估准度”这件事情, 因为在业务上预估准度过高/过低/不稳定都会带来各种投放上的问题, 这时候”我”的校准解决方案就是最好的”抓手” (阿里人说法, 个人理解来自于单词 “handler”), 有了这个”抓手”; 我们模型组作为商业化算法就具备了可用的一类问题的关键能力, 就好比厨师每天处理复杂的大型排骨有了个专用的拆骨刀 (而不是其他的完全没法用的刀)<br>3.在工作中, 如果没有找到这样的”问题”就下手做, 往往是吃力不讨好; 如果问题找的不够核心/不够关键/不够引起老板的重视, 那么就需要持续锻炼自己的”找问题”的能力, 最基础的方案就是快速换一个问题来做, 不管前面的问题有没有解决好, 因为问题没找对, 就好比足球射门只有力度发力够了, 但是射门角度打偏了/打飞了/打高了, 索性我就不浪费力气射门了<br>4.这里我想到一个例子:  穆里尼奥就是典型的基层干起的足球教练, 在称为欧洲冠军教练之前是一个俱乐部的翻译. 我们知道, 足球教练和足球运动员之间的沟通是非常关键的. 他从翻译入手, 培养了建立 足球运动员 和 教练之间沟通的关键能力. 从翻译这样的基础工作做起, 从教练身上学习到如何做临时决策, 比如临时换人等. 也能直接学习到应该以怎样的态度和时机和球员表达指令. 为后来成为冠军教练奠定了很强的基础.  </p><div align="center"><img src="/imgs/从找事理论到 TRIZ/0.png" width="60%"/></div><h2 id="找到红缨枪的问题-红布才有价值"><a href="#找到红缨枪的问题-红布才有价值" class="headerlink" title="找到红缨枪的问题, 红布才有价值"></a>找到红缨枪的问题, 红布才有价值</h2><p>1.我的每个技能点, 都一定是某个问题的解决方案, 关键在于, 找到那个问题.<br>2.找工作, 关键在于有技能, 并持续提升自己技能. 但是, 技能这个东西, 哪有个尽头? 学习这个东西, 哪有个尽头? 我可以无止境地考证, 无止境地学习, 学到下辈子也学不完. 内卷不就是这么来的吗? 读书无用不就是这么来的吗?<br>3.在提升技能的同时, 必须分出一点精力, 实际上我们要分出 [占比不低的一部分精力/时间], 去找找, 我的技能所对应的问题<br>4.任何看到的现存的事务, 最初都是某个问题的解决方案, 比如红缨枪上的那块红布, 就是士兵手滑的解决方案<br>5.只有当问题和方案一起出现时, 我才会意识到这个解决方案的价值, 我们把红缨枪上的红缨扯下来, 不告诉你这是干啥的, 我们不可能理解这块红布有啥作用或者有啥奇妙之处, 当红布和红缨枪组合在一起的时候, 我们立马感觉到了红缨枪上的红缨的价值是这么有用<br>6.很多时候, 光有答案, 一点不值钱 (孤立地看那块红布); 只有答案和问题结合在一起才值钱  </p><div align="center"><img src="/imgs/从找事理论到 TRIZ/1.png" width="20%"/></div><h2 id="What-is-TRIZ-什么是-TRIZ-理论"><a href="#What-is-TRIZ-什么是-TRIZ-理论" class="headerlink" title="What is TRIZ? 什么是 TRIZ 理论 ?"></a>What is TRIZ? 什么是 TRIZ 理论 ?</h2><p>上面谈到的理论, 说明了人应该”找问题=&gt;匹配方案”这种思想; 将这个思想放到更大的背景下, 这种思想其实有个更成熟的理论体系, 在该理论下更系统的提出了这种”找问题-匹配方案”的普适的哲学思想: TRIZ 理论.  </p><blockquote><p>TRIZ is the Russian acronym for the “Theory of Inventive Problem Solving,” an international system of creativity developed in the U.S.S.R. between 1946 and 1985, by engineer and scientist Genrich S. Altshuller and his colleagues.</p><p>Following Altshuller’s insight, the theory developed on a foundation of extensive research covering hundreds of thousands of inventions across many different fields to produce an approach that defines generalizable patterns like inventive solutions and the distinguishing characteristics of the problems these inventions have overcome.  </p><p>In other words, whatever problem you’re facing, somebody, somewhere, has already solved it (or one very like it). Creative problem solving involves finding that solution and adapting it to your problem.</p></blockquote><p>intuitively,<br>1.TRIZ 是一个俄罗斯人: Altshuller, 提出的一种理论, TRIZ 是 “Theory of Inventive Problem Solving” 的一种缩写, 翻译过来就是 “发明式的问题解决理论”, 这个概念不好翻译, 第一次听不好感知到在说什么; 一个中文翻译为”萃思”，但这种翻译仍然不够直白, 听完还是不知道是要表达什么; 我们先看下 TRIZ 理论提出背景: Altshuller 这个人思考一个核心问题: 我如果想要解决一个问题, 或者创造性的发明一个东西, 能通过学习的方式解决或者创造发明出来吗 ?<br>2.为了得到这个问题的答案, Altshuller 研究了几十万个解决方案 (来自于专利), 这些发明来自于各行各业, 然后期望能概括出来一些共有的模式; 换句话说, 他想总结一些关于 [解决问题] 或者 [创造] 的普适的原则和模式; 注意, 这里提到的 [解决问题] 或者 [创造] 其实指的是一回事: 都是解决问题; 比如一个问题已经有很成熟的解决方案了, 那么这个问题已经解决了, 比如如何定位我们在地球的位置, GPS 就是一个完美的解决方案; 另一个问题: 如何实现一个完美的自动驾驶, 这个问题目前暂时没有完美的解决方案, 我们需要 “创造” 出一个解决方案<br>3.以上的思考过程, 已将 TRIZ 思想的提出背景交代完毕, 那我们具体如何利用 TRIZ 原则去解决问题 ?</p><h2 id="TRIZ-处理问题的-2-个原则"><a href="#TRIZ-处理问题的-2-个原则" class="headerlink" title="TRIZ 处理问题的 2 个原则"></a>TRIZ 处理问题的 2 个原则</h2><p>1.TRIZ 具体理论内容是提出了一堆表格式的问题/思考方案/相应的矛盾总结, 但感觉太机械和理想了 (可以感受下下面的一个表格), 有点普适哲学的意思, 归根到底还得实践出真知; 但这里不妨从 TRIZ 原则上去体会一下 TRIZ 解决问题的哲学思想  </p><div align="center"><img src="/imgs/从找事理论到 TRIZ/2.png" width="60%"/></div>2.如何使用 TRIZ 原则去解决问题, 总共分两个核心的概念:  (i). Generalizing Problems and Solutions 泛化问题, 泛化方案  (ii). Don't accept compromises, eliminating contradictions 不接受折中, 消除矛盾  ## Generalizing Problems and Solutions 泛化问题, 泛化方案> (i). Problems and solutions are repeated across industries and sciences. By representing a problem as a "contradiction", you can predict creative solutions to that problem.> (ii). Patterns of technical evolution tend to repeat themselves across industries and sciences.> (iii). Creative innovations often use scientific effects outside the field where they were developed.<div align="center"><img src="/imgs/从找事理论到 TRIZ/3.png" width="60%"/></div><p>intuitively,<br>1.不同问题在跨越各个行业 (各类科学) 中, 都是重复出现的; 不同的解决方案, 在跨越各个行业 (各类科学) 中, 也是重复出现的; 例如: A 行业遇到的问题在 B 行业也有, B 行业的解决方法在 C 行业也存在. 这里我们把问题统统定义为”矛盾”<br>2.各个行业的技术都在发生演变, 但是各个行业中的演变也存在相似的规律. 所谓的创新, 都往利用了其开发领域之外的某些科学效应<br>3.他这里得出一个关键的结论: 无论在遇到什么时刻, 无论遇到什么人, 无论遇到怎样的问题. 其实这个 [遇到的问题] 总是有人做过相似的问题了. 无非就是找到这个问题和对应的问题答案, 然后再你的问题上面适配一下, 所谓 creativity (创造/创造力), 其实就是做最后一步的适配问题的过程; 其实仔细想想, 这种找问题-找方案-适配方案的完整过程, 就是一个人在解决任何问题的过程中, 从上帝视角看到的样子  </p><h2 id="Don’t-accept-compromises-eliminating-contradictions-不接受折中-消除矛盾"><a href="#Don’t-accept-compromises-eliminating-contradictions-不接受折中-消除矛盾" class="headerlink" title="Don’t accept compromises, eliminating contradictions 不接受折中, 消除矛盾"></a>Don’t accept compromises, eliminating contradictions 不接受折中, 消除矛盾</h2><p>矛盾, 可以分成两个类型:<br><strong>Technical contradictions</strong>. These are classical engineering “trade-offs” where you can’t reach the desired state because something else in the system prevents it. In other words, when something gets better, something else automatically gets worse. For example:</p><blockquote><p>(i). The product gets stronger (good), but the weight increases (bad).<br>(ii). Service is customized to each customer (good), but the service delivery system gets complicated (bad).<br>(iii). Training is comprehensive (good), but it keeps employees away from their assignments (bad).</p></blockquote><p><strong>Physical (or “inherent”) contradictions</strong>. These are situations in which an object or system suffers contradictory, opposite requirements. Everyday examples include:</p><blockquote><p>(i). Software should be complex (to have many features), but simple (to be easy to learn).<br>(ii). Coffee should be hot (to be enjoyed), but cool (to avoid burning the drinker).<br>(iii). An umbrella should be large (to keep the rain off), but small (to be maneuverable in a crowd).</p></blockquote><p>intuitively,<br>1.矛盾分为了两类矛盾<br>(i). 技术上的矛盾. 其实就是工程上的 “trade-off” 这种概念, 这里比较偏向于归因为某种系统性的问题, 我们有时候永远无法达到某种确定的理想的状态, 有时候总是一个变好, 一个变差. 比如一个产品我们想弄得坚固耐用点, 但是太坚固有时候就变重了; 我们想给每个用户提供 “个性化” 的服务, 有点类似于当老师搞 1对1 教学, 但是如果都 1对1 人多了就干不成了; 给员工搞培训是能提高某些员工的能力的, 但是培训太多就导致实际产出反而少了.<br>(ii). 物理上 (内在的) 的矛盾. 有些场景上, 其实需求存在对立的, 相反的需求.  软件系统通常是复杂的, 但是太复杂其他人上手就麻烦了; 咖啡通常是热的, 但是太烫有时候容易烫伤了; 雨伞应该搞得大一点就能防止被淋到, 但是太大在人群里面用着不方便.<br>2.从我的理解, 这两类矛盾其实也可以合并一下, 都是识别关键问题并作出决策的过程.<br>3.这里明确提出了: 不要妥协, 不要接受折中, 消除矛盾就是要做的, 你选定的方案就是最终的最好的方案. 我理解这里还是比较强调其实人的主观能动性的意思, 矛盾是永恒存在的, 从来没有消除过, 我们所能做的就是我们力所能及能做的消除矛盾, 给出属于自己的答案.  </p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. 得到头条. 176期 | 找工作就是找麻烦.<br>[2]. <a href="https://en.wikipedia.org/wiki/TRIZ#Basic_principles">https://en.wikipedia.org/wiki/TRIZ#Basic_principles</a>.  TRIZ wiki 介绍.<br>[3]. <a href="https://www.mindtools.com/amtcc5f/triz">https://www.mindtools.com/amtcc5f/triz</a>. TRIZ A Powerful Methodology for Creative Problem Solving.<br>[4]. <a href="https://www.slideshare.net/ybaronov/triz-overview-yaroslav">https://www.slideshare.net/ybaronov/triz-overview-yaroslav</a>. TRIZ overview and examples.  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h2&gt;&lt;p&gt;1.找工作 v.s. 找事做&lt;br&gt;2.找到红缨枪的问题, 红布才有价值&lt;br&gt;3.What</summary>
      
    
    
    
    <category term="Reading" scheme="http://example.com/categories/Reading/"/>
    
    
  </entry>
  
  <entry>
    <title>Joint Optimization of Ranking and Calibration with Contextualized Hybrid Model</title>
    <link href="http://example.com/2023/12/30/Joint%20Optimization%20of%20Ranking%20and%20Calibration%20with%20Contextualized%20Hybrid%20Model/"/>
    <id>http://example.com/2023/12/30/Joint%20Optimization%20of%20Ranking%20and%20Calibration%20with%20Contextualized%20Hybrid%20Model/</id>
    <published>2023-12-30T12:43:00.000Z</published>
    <updated>2025-04-28T09:20:54.406Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>1.提出一种 CTR 模型中解耦校准能力和排序能力的损失: 将原本 1 个自由度的 logit 改成 2 个自由度的 logit 作差的形式<br>2.提出同时优化校准和排序的损失函数 (注意这里的校准和业务通常 CTR 校准不是一个概念, 目标和通常意义下的二分类排序损失 + LTR 损失联合建模更接近)  </p><h2 id="Two-types-of-capabilities-that-CTR-CVR-needs-to-have-点击率-转化率模型需要的两类能力"><a href="#Two-types-of-capabilities-that-CTR-CVR-needs-to-have-点击率-转化率模型需要的两类能力" class="headerlink" title="Two types of capabilities that CTR/CVR needs to have 点击率/转化率模型需要的两类能力"></a>Two types of capabilities that CTR/CVR needs to have 点击率/转化率模型需要的两类能力</h2><p>1.CTR 模型需要兼备两种能力, 1 个是校准能力 (calibration ability) 另 1 个是 (ranking ability), 其中校准能力可以写成下面的式子<br>2.注意这里的校准不是业务指标通常意义上的校准, 业务通常意义上的校准能力是 pcoc/calN/ECE 上的保证, 这里的校准指的仍然是做二分类排序能力; 为什么这里叫做校准? 因为本文要引出 LTR 的排序能力对模型效果的收益, LTR 的排序能力在这里用了 ranking ability, 相对本文研究的 ranking ability, 原始的二分类排序的能力称之为校准能力  </p><script type="math/tex; mode=display">\mathcal l_{\text{calib}}=-\sum_{x,y}\log\ \hat p(y|x)=-\sum_{x,y}\log\frac{\exp(f_\theta(x))}{1+\exp(f_{\theta}(x))}=-\sum_{x,y}\log\frac{1}{1+\exp(-f_{\theta}(x))}</script><p>如果这 2 种能力我们同时想要应该怎么做 ? 一种直接的思路就是融合这 2 个 loss , 但是直觉上这 2 种 loss 直接相加是不合理的  </p><h2 id="Joint-Optimization-of-Ranking-and-Calibration-排序和校准同时优化"><a href="#Joint-Optimization-of-Ranking-and-Calibration-排序和校准同时优化" class="headerlink" title="Joint Optimization of Ranking and Calibration 排序和校准同时优化"></a>Joint Optimization of Ranking and Calibration 排序和校准同时优化</h2><div align="center"><img src="/imgs/Joint Optimization of Ranking and Calibration with Contextualized Hybrid Model/0.png" width="80%"/></div><blockquote><p>The intuition of introducing an additional degree of freedom is to alleviate the conflict between the optimization of ranking and calibration.</p><p>To avoid representing different meanings with the same logit, the proposed JRC extends the output logit from 1 dimension to 2 dimensions, $f:\mathbb R^{D}\rightarrow \mathbb R^{2}$</p></blockquote><p>intuitively,<br>1.作者认为 CTR 模型不能兼顾训练准度和校准的原因是我们有两个想要的目标, 但在默认的设定下只有一个 logit; 假如模型的输出有两个 logit, (也就是对 logit 增加一个自由度), 就能把校准和排序的目标分解开, 怎么拆开呢?<br>2.怎么把一个 logit 拆成 2 个? 把原本模型的输出从 1 个 logit 强行拆分成两个 logit , 一个是点击状态 logit, 一个是不点击状态 logit; 我们约定模型参数为 $\theta$ , 用 $f<em>{\theta}(x)[0]$ 表示非点击状态下的 logit, 且用 $f</em>{\theta}(x)[1]$ 表示点击状态下的 logit  </p><script type="math/tex; mode=display">\hat p(y|x)=\frac{1}{1+\exp(-\underbrace{(f_{\theta}(x)[1]-f_{\theta}(x)[0])}_{两个\text{logit}的差作为预估点击概率, 取代原本1个自由度下的f_{\theta}(x)})}</script><p>我们有了上述新的2个自由度的建模方式之后, 相应的 pointwise 的校准 loss 如下所示, 这个式子其实就是等价于两个 logits 采用了 softmax 的计算  </p><script type="math/tex; mode=display">\begin{aligned}\mathcal{l}_{\text{calib}}&=-\sum_{x,y}{\log}\ \hat p(y|x)\\&=-\sum_{x,y}\log \frac{\overbrace{\exp(f_{\theta}(x)[y])}^{正样本用f_\theta(x)[1], 负样本用f_\theta(x)[0]}}{\exp(f_{\theta}(x)[1])+\exp(f_{\theta}(x)[0])} (原文用这个式子表达) \\&=-\sum_{x,y} [\ y \cdot \log \hat p(y=1|x)-(1-y) \log (1-\hat p(y=1|x))] \ (等价于CE损失)\end{aligned}</script><p>3.这里我们不妨倒着想一下校准这个公式是怎么想出来的 ?  这里是要还原一下 softmax 的本质, softmax 这个函数其实就是 sigmoid 在多分类下的推广; softmax 和 sigmoid 在二分类任务上有区别吗? 完全没有, 只是有个数学上等价推导; 这里猜测作者在思考解耦 ranking 和 calibration 的时候, 还是不想放弃属于 Listwise 的建模的 naive 思想: 经典Listwise 的建模方式我们刚才提到了本质上是建模的是元素排在 Top1 的概率, 模型计算 loss 执行的是 softmax 的计算过程, 然后再这样的 softmax 和 二分类的 sigmoid 的关系中找到一种可联系二分类的的方法  </p><script type="math/tex; mode=display">\begin{align}output(x_1)&=\frac{1}{1+e^{-x_1}}=\text{sigmoid}(x_1) \\&=\frac{e^{x_1}}{e^{x_1}+e^{x_2}}=\frac{1}{1+e^{-\underbrace{(x_1-x_2)}_{写成z_1}}}=\text{softmax}(x_1-x_2)  \\output(z_1)&=\frac{1}{1+e^{-z_1}}\Leftrightarrow output(x_1) \\\end{align}</script><p>4.有了校准损失的解耦公式, 那么 listwise 的 ranking 损失应该怎么表达? 选择一个 session , 目标是对点击 logit 和 未点击 logit 都增加一个 listwise 的 loss 提升排序建模, 其中让正样本的点击 logit 大于 session 内其他样本的点击 logit , 让负样本的非点击 logit 大于同 session 内样本的非点击的 logit; 具体怎么做?<br>(i). 对于正样本, 对比的是点击logits 和其他样本的点击logits, 也就是 $f<em>\theta(x)[1]$<br>(ii). 对于负样本, 对比的是非点击logits 和其他样本的非点击logits, 也就是 $f</em>\theta(x)[0]$  </p><script type="math/tex; mode=display">l_{rank}=-\log \frac{\exp(f_\theta (x)[y])}{\sum_{x^{\prime}\in{\text{session}_x}}\exp(f_\theta({x^{\prime})}[y])}  , y\in\{0,1\}</script><p>其实我们对比下 softmax 函数, 本质上是没有区别的  </p><script type="math/tex; mode=display">\text{softmax}(x)=\frac{\exp(x_i)}{\sum_{j=1}^n\exp(x_j)}</script><p>综合两个 loss , 得到最终的 JRC loss 如下  </p><script type="math/tex; mode=display">l_{\text{final}}=\alpha \mathcal l_{\text{calib}}+(1-\alpha) \mathcal l_{\text{rank}}</script><h2 id="JRC-pseudo-code-JRC代码实现"><a href="#JRC-pseudo-code-JRC代码实现" class="headerlink" title="JRC pseudo code JRC代码实现"></a>JRC pseudo code JRC代码实现</h2><p>代码如下</p><pre><code class="lang-python"># B: batch size, label: [B, 2], context_index: [B, 1]# Feed forward computation to get the 2-dimensional logits # and compute LogLoss -log p(y|x, z)logits = feed_forward(inputs)ce_loss = mean(CrossEntropyLoss(logits, label))# Mask: shape [B, B], mask[i,j]=1 indicates the i-th sample # and j-th sample are in the same contextmask = equal(context_index, transpose(context_index))# 铺开 logits 和 label [B,2] =&gt; [B, B, 2]logits = tile(expand_dims(logits, 1), [1, B, 1])y = tile(expand_dims(label, 1), [1, B, 1])# Set logits that are not in the same context to -infy = y * expand_dims(mask, 2)logits = logits + (1-expand_dims(mask, 2))*-1e9 y_neg, y_pos = y[:,:,0], y[:,:,1]l_neg, l_pos = logits[:,:,0], logits[:,:,1]# Compute listwise generative loss -log p(x|y, z)loss_pos = -sum(y_pos * log(softmax(l_pos, axis=0)), axis=0) loss_neg = -sum(y_neg * log(softmax(l_neg, axis=0)), axis=0) ge_loss = mean((loss_pos + loss_neg) / sum(mask, axis=0))# 合并JRC的 lossloss = alpha * ce_loss + (1 - alpha) * ge_loss</code></pre><h2 id="Listwise-softmax-loss-v-s-JRC-loss"><a href="#Listwise-softmax-loss-v-s-JRC-loss" class="headerlink" title="Listwise softmax loss v.s. JRC loss"></a>Listwise softmax loss v.s. JRC loss</h2><p>对比两种损失函数</p><div align="center"><img src="/imgs/Joint Optimization of Ranking and Calibration with Contextualized Hybrid Model/1.png" width="80%"/></div><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. Joint Optimization of Ranking and Calibration with Contextualized Hybrid Model.<br>[2]. KDD’23 | 排序和准度联合优化：一种基于混合生成/判别式建模的方案. 阿里妈妈技术. <a href="https://zhuanlan.zhihu.com/p/639569672">https://zhuanlan.zhihu.com/p/639569672</a>.<br>[3]. KDD’23 | 阿里, 排序和校准联合建模: 让listwise模型也能用于CTR预估. 蘑菇先生. <a href="https://zhuanlan.zhihu.com/p/638414676">https://zhuanlan.zhihu.com/p/638414676</a>.  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h2&gt;&lt;p&gt;1.提出一种 CTR 模型中解耦校准能力和排序能力的损失: 将原本 1 个自</summary>
      
    
    
    
    <category term="Ads_RecSys" scheme="http://example.com/categories/Ads-RecSys/"/>
    
    
  </entry>
  
  <entry>
    <title>Multi-Scenario Ranking with Adaptive Feature Learning</title>
    <link href="http://example.com/2023/11/20/Multi-Scenario%20Ranking%20with%20Adaptive%20Feature%20Learning/"/>
    <id>http://example.com/2023/11/20/Multi-Scenario%20Ranking%20with%20Adaptive%20Feature%20Learning/</id>
    <published>2023-11-20T12:08:00.000Z</published>
    <updated>2025-04-28T09:20:54.408Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>1.梳理四种多场景建模的范式: 辅助任务范式, 分塔建模范式, 专家网络范式以及本文提出的 MARIA 范式, 并且认为传统的多场景模型的设计 (前三种范式) 均忽略了在特征这个层面上的自适应, 限制模型效果的上限<br>2.针对多场景自适应模型, 提出一种在特征层面做自适应场景建模的模型结构 MARIA, 将场景自适应建模下沉到特征层面, 能够针对不同的场景刻画不同特征的影响  </p><div align="center"><img src="/imgs/Multi-Scenario Ranking with Adaptive Feature Learning/0.png" width="60%"/></div><h2 id="Four-Paradigm-of-Multi-Scenario-Ranking-多场景排序模型的4种范式"><a href="#Four-Paradigm-of-Multi-Scenario-Ranking-多场景排序模型的4种范式" class="headerlink" title="Four Paradigm of Multi-Scenario Ranking 多场景排序模型的4种范式"></a>Four Paradigm of Multi-Scenario Ranking 多场景排序模型的4种范式</h2><div align="center"><img src="/imgs/Multi-Scenario Ranking with Adaptive Feature Learning/1.png" width="100%"/></div><p>intuitively,<br>1.辅助任务范式: 将场景特征单独拆出来建辅助任务塔, 然后将辅助塔的结果和主塔做融合, 学习目标是是残差<br>2.分塔建模范式: shared_bottom + 分塔结构, 把所有的共享信息都集中在底座上, 场景信息建模汇聚在不同的塔上<br>3.专家网络范式: shared_bootom 保持不变, 将分塔结构替换成了 针对型设计的 gate 模块去做领域自适应<br>4.Maria范式: 在特征层面设计更复杂的场景自适应结构, 将场景自适应建模下沉到特征层面, 并通过逐级地模块增强场景自适应  </p><h2 id="Formulation"><a href="#Formulation" class="headerlink" title="Formulation"></a>Formulation</h2><p>1.$h_b$ 表示行为序列 (已通过 Transformer 完成序列特征的提取) 特征<br>2.$u$ 表示用户相关的特征, $e_u$ 是其中的 user embedding, 另外还有 $L$ 个用户特征 field<br>3.$t$ 表示 trigger 相关的特征, 当且仅当有 trigger 的场景才有这个特征, 另外还有 $O$ 个特征<br>4.$c$ 表示上下文特征, 上下文总共有 $N_c$ 个特征<br>5.$x$ 表示 target 商品/广告, 其中 $e_x$ 表示商品的 embedding, 另外还有 $P$ 个商品特征<br>6.$e_s$ 表示场景的 embedding<br>7.$N_s$ 总共有 $N_s$ 个场景  </p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>1.MARIA 模型分为 3 个部分, 从下往上 a / b / c 这三个部分各自有一定程度上的场景刻画建模操作, 堆叠起来构造成一个完整的结构; 在 a =&gt; b =&gt; c 这三个部分  </p><div align="center"><img src="/imgs/Multi-Scenario Ranking with Adaptive Feature Learning/2.png" width="100%"/></div><h2 id="Embedding-amp-Encoder-Layer"><a href="#Embedding-amp-Encoder-Layer" class="headerlink" title="Embedding &amp; Encoder Layer"></a>Embedding &amp; Encoder Layer</h2><p>1.商品id/用户id/trigger/context 和它们对应的的基础属性 concate 得到商品/用户/trigger/context embedding, 以商品和用户为例  </p><script type="math/tex; mode=display">x=[e_x||a_x^1||\cdots||a_x^P] \\u=[e_u||a_u^1||\cdots||a_u^L] \\</script><p>2.行为序列过 Transformer 得到一个表征 $h_b$<br>3.各种 embedding 和行为序列表征 concat, 得到基础特征输入 $Q$ , 为了表述容易理解, 把下面的 concat 起来的每个特征组, 称为 [特征分组] , 总共有 5 [特征分组]  </p><script type="math/tex; mode=display">Q=[h_b||\bf u||x_i||t||c]</script><h2 id="Feature-Scaling-特征缩放"><a href="#Feature-Scaling-特征缩放" class="headerlink" title="Feature Scaling 特征缩放"></a>Feature Scaling 特征缩放</h2><p>特征缩放是想在 [原子特征] 上做缩放操作, 这里的 [原子特征] 定义为某个具体 sparse_id / dense 特征, 用 $N_Q=L+P+O+N_c+4$ 来表示基础特征 $Q$ 有 $N_Q$ 个[原子特征], 然后用一个向量  $\alpha \in \mathbb R^{N_Q}$, 来对这 $N_Q$ 个分组生成对应的标量缩放系数 $\alpha_i$ </p><script type="math/tex; mode=display">\begin{aligned}Q_S&=[\alpha_ 1 Q_1||\alpha_ 2 Q_2||\ldots||\alpha _{N_Q}Q_{NQ}]\\&=[\alpha_ 1 h_b||\alpha_ 2 e_u ||\ldots||\alpha_ {N_Q} c_u^{N_c}]\end{aligned}</script><p>这个 $\alpha \in \mathbb R^{N_Q}$ 怎么学出来的呢? 用一层神经网络 sigmoid 激活后的值 乘以外加的一个 $\lambda$ 系数</p><script type="math/tex; mode=display">\alpha=\lambda \cdot \text{sigmoid}(\text{FCN}([freeze(Q)||e_u||e_{x_i}||e_s]))</script><p>intuitively,<br>1.我们看下这个式子, $freeze()$ 函数表示 stop_gradient op, 把原始 $Q$ 固定住参数不参与梯度更新, 不更新的原因是避免过拟合和梯度冲突, 并且和 user embedding 和 item embedding 加上场景 embedding 拼起来做出来一个 attention weight<br>2.这里有个疑问是 $Q$ 里面是包括原来的 $e_u$ 和 $e_x$ embedding 的, 所以如果按照公式里面这么写, 岂不是 $Q$ 的局部还是会修改? 文章没有没明确解释这个操作如何处理<br>3.我们将输出的结果, 一个和 $Q$ 完全相同的表征取出来记录为 $Q_S$ , 这个新表征吸收了在不同特征分组下的 $\alpha$ 参数, 可以理解为完成了一个非常基础的特征分组的场景自适应, 记录一个新的表示为  </p><script type="math/tex; mode=display">Q_S=[\hat h_b||\hat u||\hat x_i||\hat t||\hat c]</script><h2 id="Feature-Refinement-特征精细化"><a href="#Feature-Refinement-特征精细化" class="headerlink" title="Feature Refinement 特征精细化"></a>Feature Refinement 特征精细化</h2><p>1.特征精细化希望进一步提取每个 [特征分组] 的语义信息, 具体来讲, 对每个 [特征分组] 搞若干个属于这个分组的 feature refiner, 这个 refiner 是一个1层神经网络<br>2.这里抽象的层次相比之前的 Feature Scaling 的语义维度高了一层, 我们总共有5个特征分组, 我们就对这5个分组生成对应的 refiner, 每个 refiner 对应有对应的维度<br>3.比如对于 [用户行为序列这个分组] 我们搞 3个 refiner, 相当于把行为序列的特征长度 concat 到原来的 3 倍  </p><p>下面 formulation 写的是 $\beta$ 的生成过程, 并且以 $\hat h_b$ 为代表说明了怎么加权</p><script type="math/tex; mode=display">\begin{aligned}\beta&=\text{Gumbel Softmax}(sigmoid(\text{FCN}([\hat h_b||e_s]))), \beta \in \mathbb R^{N_b} \\\tilde{h}_b&=[\beta_ 1 FC_1(\hat h_b)||\ldots||\beta_ {N_b}FC_{N_b}(\hat h_b)]\end{aligned}</script><p>4.到这为止, 我们生成 (输出) 的表达为  </p><script type="math/tex; mode=display">Q_R=[\tilde{h}_b||\tilde{u}||\tilde{x}_i||\tilde{t}||\tilde{c}]</script><p>5.什么是 Gumbel softmax?  </p><blockquote><p>Gumbel softmax 允许模型中有从离散的分布（比如类别分布categorical distribution）中采样的这个过程变得可微，从而允许反向传播时可以用梯度更新模型参数</p></blockquote><h2 id="Feature-Correlation-Modeling-特征相关性建模"><a href="#Feature-Correlation-Modeling-特征相关性建模" class="headerlink" title="Feature Correlation Modeling 特征相关性建模"></a>Feature Correlation Modeling 特征相关性建模</h2><p>1.我们对现在的 5 个[特征分组], 还想学一个 [特征分组之间] 相关关系, 核心原理是对每两两分组 pair 学一个内积表示 [特征分组] 和 [特征分组] 之间的关系; 为了想用内积, 要把每个分组的表达搞到一个相同的维度 $d_r$ 才能去计算向量内积<br>2.因此第1步我们先对每个分组过一层网络拿到相同维度的表达, 分别是 $\hat h_b,\hat u,\hat x_i,\hat t,\hat c$ 它们的维度都是 $d_r$, 然后生成一个两两内积层  </p><script type="math/tex; mode=display">Q_C=[\hat h_b\cdot \hat u||\hat h_b\cdot \hat x_i||\cdots||\hat t\cdot \hat c]</script><p>3.内积层只是为了捕获特征相关性信息, 我们还是要带上原来的特征  </p><script type="math/tex; mode=display">Q_{f}=[Q_{R}||Q_C]</script><p>4.到此为止, 我们已经把特征层面的事情已经都做完了, 实现了逐层增加的特征对场景的自适应效果, 并且也建模了特征之间复杂的跨域关系  </p><h2 id="MMoE-Layer-多专家混合网络层"><a href="#MMoE-Layer-多专家混合网络层" class="headerlink" title="MMoE Layer 多专家混合网络层"></a>MMoE Layer 多专家混合网络层</h2><p>1.MMoE 这一层想在网络层面对场景进行一次自适应, 总共设置了 $N_e$ 个 experts, 对于然后构造了一个 MMoE 中间层, 其中每个 expert 是独立的全连接层网络<br>2.为什么说是对场景自适应, gate 网络的 softmax 的输入来自于 $W_ge_s$</p><script type="math/tex; mode=display">h_N=\sum_{j=1}^{N_e}gf_j(Q_f)=\sum_{j=1}^{N_e}softmax(W_ge_s)</script><h2 id="Prediction-and-Model-Optimization-分塔融合预测和损失函数"><a href="#Prediction-and-Model-Optimization-分塔融合预测和损失函数" class="headerlink" title="Prediction and Model Optimization 分塔融合预测和损失函数"></a>Prediction and Model Optimization 分塔融合预测和损失函数</h2><p>最终的多任务输出, 采用了一个分塔融合的策略: 用场景单独隐藏层 + $\alpha$ 倍的场景共享层去做融合, 融合的结果最终过一层全连接层输出最终的结果 $\hat y_{ui}$  </p><script type="math/tex; mode=display">\begin{aligned}&h_f=h_{specific}^{s}+\alpha _s h_{shared} \\&\hat y_{ui}=FCN(h_f) \\&h_{specific}=FCN^{s}_{sp}(h_N) \\&h_{shared}=FCN_{sp}(h_N)\end{aligned}</script><p>融合系数 $\alpha_ s$ 是如何产出的? 充分利用场景的 embedding $e_s$ 计算场景的相似性分数, 具体来说, 针对每个场景的 embedding 和其他所有的场景的 embedding 做内积再求和然后归一化  </p><p>intuitively,<br>1.采用这种分塔融合的方式, 类似于直接预估模型 ensemble 融合的方式, 这里的系数 $\alpha_s$ 起到较强的融合作用; 我理解这个融合参数的给出不一定需要一个端到端学习的过程, 因为会给模型学习带来更多的复杂度, 如果采用离线计算的结果直接作用, 可能会降低模型复杂度  </p><script type="math/tex; mode=display">\alpha _s=\frac{1}{N_s-1}\sum_{j=1,s_j\ne s}^{N_s}e_s\cdot e_{s_j}</script><h2 id="From-My-Perspective"><a href="#From-My-Perspective" class="headerlink" title="From My Perspective"></a>From My Perspective</h2><p>1.该模型从特征层面在多场景自适应上的探索更优的效果值得尝试<br>2.该模型采用了较多模块去耦合, 端到端建模的难度非常大, 直觉上耦合的系数过多; 在利用的时候考虑可以组件成独立的模块, 然后再在模型上去做逐层 (a/b/c) 的实验  </p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. Multi-Scenario Ranking with Adaptive Feature Learning.<br>[2]. SIGIR’23 | 基于特征自适应的多场景预估建模. <a href="https://zhuanlan.zhihu.com/p/641895931">https://zhuanlan.zhihu.com/p/641895931</a>.  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h2&gt;&lt;p&gt;1.梳理四种多场景建模的范式: 辅助任务范式, 分塔建模范式, 专家网络范式</summary>
      
    
    
    
    <category term="Ads_RecSys" scheme="http://example.com/categories/Ads-RecSys/"/>
    
    
  </entry>
  
  <entry>
    <title>Towards Deeper, Lighter and Interpretable Cross Network for CTR Prediction</title>
    <link href="http://example.com/2023/11/20/Towards%20Deeper,%20Lighter%20and%20Interpretable%20Cross%20Network%20for%20CTR%20Prediction/"/>
    <id>http://example.com/2023/11/20/Towards%20Deeper,%20Lighter%20and%20Interpretable%20Cross%20Network%20for%20CTR%20Prediction/</id>
    <published>2023-11-20T12:08:00.000Z</published>
    <updated>2025-04-28T09:20:54.412Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>1.提出一个 <a href="https://goldandrabbit.github.io/2023/07/13/DCN%20V2%20Improved%20Deep%20&amp;%20Cross%20Network%20and%20Practical%20Lessons%20for%20Web-scale%20Learning%20to%20Rank%20Systems/">DCN V2</a> 的升级版本: gdcn, 引入了信息门控组件，自适应地学习上一层阶交叉结果的重要性</p><h2 id="GDCN"><a href="#GDCN" class="headerlink" title="GDCN"></a>GDCN</h2><p>1.GDCN 核心实现 </p><pre><code class="lang-python">x_&#123;l+1&#125;=x_0 * (w_l+b_l) * sigmoid(w_g * x_l) + x_l</code></pre><p>intuitively,<br>1.GDCN 结构在 DCN 结构上引入了信息门控组件，自适应地学习上一层阶交叉结果的重要性: 我们期望该过程可以放大更重要特征，减轻不重要特征的影响; 随着交叉层数量的增加，每个交叉层的信息门过滤下一阶交叉特征，并有效地控制信息流</p><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><p>自己实现的 tf 版本</p><pre><code class="lang-python">&quot;&quot;&quot;tensorlow online version&quot;&quot;&quot;def dcn_stack_net(self, input_layer, cross_layer_num, output_size):  &quot;&quot;&quot;  x_l = x_0 \odot (W \cdot x_l + b) + x_l  \odot 代表 element-wise 的tensor 乘法, 通常 tf.multiply() 实现  &quot;&quot;&quot;  name = &#39;dcn&#39;  act_fun = tf.nn.tanh  with tf.variable_scope(&quot;dcn_stack_net&quot;):    x_0 = input_layer    x_l = x_0    input_size = input_layer.shape1.value    for i in range(cross_layer_num):      w      = tf.get_variable(shape=[input_size, input_size], name=f&quot;&#123;name&#125;_kernerl_&#123;i+1&#125;&quot;, initializer=tf.random_normal_initializer(stddev=1.0 / math.sqrt(float(input_size))), trainable=True)      b      = tf.get_variable(shape=[input_size], name=f&quot;&#123;name&#125;_bias_&#123;i+1&#125;&quot;, initializer=tf.zeros_initializer, trainable=True)      dot_   = tf.multiply(x_0, tf.add(tf.matmul(x_l, w), b))      x_l    = tf.add(dot_, x_l)      # x_l    = act_fun(x_l)    output_w = tf.get_variable(shape=[input_size, output_size], name=f&quot;&#123;name&#125;_output_w&quot;)    output_layer = tf.matmul(x_l, output_w)    return output_layerdef gdcn_stack_net(self, input_layer, cross_layer_num, output_size):  &quot;&quot;&quot;  formulaiton: c_&#123;l+1&#125; = c_0 \odot (w_l + b_l) \odot sigmoid(w_g \cdot x_l) + x_l  code: x_l = x_0 \odot (w_l + b_l) \odot sigmoid(w_g \cdot x_l) + x_l  &quot;&quot;&quot;  name = &#39;gdcn&#39;  with tf.variable_scope(&quot;gdcn_stack_net&quot;):    x_0 = input_layer    x_l = x_0    input_size = input_layer.shape1.value    for i in range(cross_layer_num):      wl = tf.get_variable(shape=[input_size, input_size], name=f&quot;&#123;name&#125;_wl_&#123;i+1&#125;&quot;, initializer=tf.random_normal_initializer(stddev=1.0 / math.sqrt(float(input_size))), trainable=True)      wg = tf.get_variable(shape=[input_size, input_size], name=f&quot;&#123;name&#125;_wg_&#123;i+1&#125;&quot;, initializer=tf.random_normal_initializer(stddev=1.0 / math.sqrt(float(input_size))), trainable=True)      b  = tf.get_variable(shape=[input_size], name=f&quot;&#123;name&#125;_b_&#123;i+1&#125;&quot;, initializer=tf.zeros_initializer, trainable=True)      dot1 = tf.multiply(x_0, tf.add(tf.matmul(x_l, wl), b))      dot2 = tf.nn.sigmoid(tf.matmul(x_l, wg))      x_l  = tf.add(tf.multiply(dot1, dot2), x_l)    output_w = tf.get_variable(shape=[input_size, output_size], name=f&quot;&#123;name&#125;_output_w&quot;)    output_layer = tf.matmul(x_l, output_w)    return output_layer</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. Towards Deeper, Lighter and Interpretable Cross Network for CTR Prediction.  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h2&gt;&lt;p&gt;1.提出一个 &lt;a href=&quot;https://goldandrabbit.</summary>
      
    
    
    
    <category term="Ads_RecSys" scheme="http://example.com/categories/Ads-RecSys/"/>
    
    
  </entry>
  
  <entry>
    <title>Self-Attention with Relative Position Representations</title>
    <link href="http://example.com/2023/10/20/Self-Attention%20with%20Relative%20Position%20Representations/"/>
    <id>http://example.com/2023/10/20/Self-Attention%20with%20Relative%20Position%20Representations/</id>
    <published>2023-10-20T12:08:00.000Z</published>
    <updated>2025-04-28T11:04:15.181Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>1.针对 transformer 结构, 提出一种相对位置编码的方法, 能够在 self-attention 过程中将相对位置信息直接编码到单个 token 的表示学习中<br>2.提出高效实现融入相对位置编码方法下的 scaled dot self-attention 实现方式  </p><h2 id="Recap-Why-position-embedding-回顾为什么需要-position-embedding"><a href="#Recap-Why-position-embedding-回顾为什么需要-position-embedding" class="headerlink" title="Recap: Why position embedding ? 回顾为什么需要 position embedding ?"></a>Recap: Why position embedding ? 回顾为什么需要 position embedding ?</h2><p>为什么要引入 position embedding ? 有时候一句话中相同的 token 因为具有不同的位置, 含义完全不同, 找几个极端的体会一下:</p><blockquote><p>Can you can a can as a canner can can a can ? 你能像个罐头工人一样装罐头吗<br>一把把把把把住了<br>我也想过过过过过过的生活</p></blockquote><p>稍微正常一点, 比如我们要对一句话 “I think therefore I am” 这句话编码, 第 1 个 token I 和第 4 个 token I, 在不同的上下文位置上因此有着不同的含义, 它俩因为不同的语境的位置, 应该有这不同的信息成分  </p><h2 id="Relation-aware-Self-Attention-感知相对位置关系的自注意力机制"><a href="#Relation-aware-Self-Attention-感知相对位置关系的自注意力机制" class="headerlink" title="Relation-aware Self-Attention 感知相对位置关系的自注意力机制"></a>Relation-aware Self-Attention 感知相对位置关系的自注意力机制</h2><p>回顾 self-attention 操作, 先设定只有 1 个 head, 对于 长度为 $n$ 的 token 序列 $x=(x_1,\ldots,x_n)$ , 其中 $x_i\in \mathbb R^{d_x}$, 计算一个等长度的结果 $z=(z_1,\ldots,z_n)$, 其中 $z_i\in \mathbb R^{d_z}$, 核心 attention 操作为</p><script type="math/tex; mode=display">\begin{align}z_i=&\sum_{j=1}^n\alpha_{ij}(x_jW^V) \\\alpha_{ij}=&\frac{\exp e_{ij}}{\sum_{k=1}^n\exp e_{ik}} \\e_{ij}=&\frac{(x_iW^Q)(x_jW^K)^T}{\sqrt{d_z}}\end{align}</script><p>引入相对位置编码, 目的显式建模 pairwise 级别的 token 之间的相对位置关系, 假设在 $x<em>i$ 和 $x_j$ 之间的 edge (将 token 抽象成 graph 中的 nodes) 引入两套相对位置表达 $a</em>{ij}^V, a_{ij}^{K} \in \mathbb R^{d_a}$, 注意到相比传统的 self attention: 式子 (1) v.s. (4), 式子 (3) v.s. (6), 新增引入的相对位置编码分别影响到了 $V$ 和 $K$ 之上  </p><script type="math/tex; mode=display">\begin{align}z_i=&\sum_{j=1}^n\alpha_{ij}(x_jW^V+a_{ij}^V) \\\alpha_{ij}=&\frac{\exp e_{ij}}{\sum_{k=1}^n\exp e_{ik}} \\e_{ij}=&\frac{(x_iW^Q)(x_jW^K+a_{ij}^K)^T}{\sqrt{d_z}}\end{align}</script><h2 id="Relative-Position-Representations-相对位置表达"><a href="#Relative-Position-Representations-相对位置表达" class="headerlink" title="Relative Position Representations 相对位置表达"></a>Relative Position Representations 相对位置表达</h2><p>所以这两套表达 $a<em>{ij}^V, a</em>{ij}^{K} \in \mathbb R^{d_a}$, 是怎么学出来的呢 ?</p><p>假设 $i$ 为我们研究的某个序列中间的位置, 它的左侧相对位置可以表示成 $i-k,\ldots,i-4, i-3, i-2, i-1$, 它的右侧侧相对位置可以表示成 $i+1, i+2, i+3, i+4, \ldots, i+k$  </p><blockquote><p>We hypothesized that precise relative position information is not useful beyond a certain distance</p></blockquote><p>我们先确定一个最大的相对值 $k$, 我们假设相对位置如果超过 $k$ 的话就会距离太远没意义了, 所以我们根据相对值 $k$ 做个 clip, 在这种设定下, 我们可以保证至多有 $2k+1$ 范围个 edges  </p><script type="math/tex; mode=display">a_{ij}^K=w_{\text{clip}(j-i,k)}^{K} \\a_{ij}^V=w_{\text{clip}(j-i,k)}^{V} \\\text{clip}(x,k)=\max(-k,\min(k,x))</script><p>目标学出来 $w^K=(w<em>{-k}^K,\ldots,w</em>{k}^K)$ 和 $w^V=(w<em>{-k}^V,\ldots,w</em>{k}^V)$, 其中 $w_i^K,w_i^V\in \mathbb R^{d_a}$</p><p>在实现的时候, 式子 (6)  有个更高效的实现方式</p><script type="math/tex; mode=display">\begin{align}e_{ij}=&\frac{(x_iW^Q)(x_jW^K+a_{ij}^K)^T}{\sqrt{d_z}} \\\Rightarrow e_{ij}&=\frac{(x_iW^Q)(x_jW^K)^T+(x_iW^Q)(a_{ij}^K)^T}{\sqrt{d_z}}\end{align}</script><h2 id="Core-Code-核心代码分析"><a href="#Core-Code-核心代码分析" class="headerlink" title="Core Code 核心代码分析"></a>Core Code 核心代码分析</h2><p>更新 scaled dot self attention 的实现方式</p><pre><code class="lang-python">def _relative_attention_inner(x, y, z, transpose):  # from https://github.com/tensorflow/tensor2tensor/blob/9e0a894034d8090892c238df1bd9bd3180c2b9a3/tensor2tensor/layers/common_attention.py#L1556-L1587  &quot;&quot;&quot;Relative position-aware dot-product attention inner calculation.  This batches matrix multiply calculations to avoid unnecessary broadcasting.  Args:    x: Tensor with shape [batch_size, heads, len, length or depth].    y: Tensor with shape [batch_size, heads, len, depth].    z: Tensor with shape [len, length, depth].    transpose: Whether to transpose inner matrices of y and z.     Should be true if last dimension of x is depth, not length.  Returns:    A Tensor with shape [batch_size, heads, length, length or depth].  &quot;&quot;&quot;  batch_size = tf.shape(x)[0]  heads = x.get_shape().as_list()[1]  length = tf.shape(x)[2]  xy_matmul = tf.matmul(x, y, transpose_b=transpose)                        # [b, heads, len, length or depth]  x_t = tf.transpose(x, [2, 0, 1, 3])                                       # [len, b, heads, length or depth]  x_t_r = tf.reshape(x_t, [length, heads * batch_size, -1])                 # [len, b * heads, length or depth]  x_tz_matmul = tf.matmul(x_t_r, z, transpose_b=transpose)                  # [len, b * heads, length or depth]  x_tz_matmul_r = tf.reshape(x_tz_matmul, [length, batch_size, heads, -1])  # [len, b, heads, length or depth]  x_tz_matmul_r_t = tf.transpose(x_tz_matmul_r, [1, 2, 0, 3])               # [b, heads, len, length or depth]  return xy_matmul + x_tz_matmul_r_t</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. Self-Attention with Relative Position Representations.<br>[2]. <a href="https://zhuanlan.zhihu.com/p/397269153">https://zhuanlan.zhihu.com/p/397269153</a>.<br>[3]. <a href="https://medium.com/@_init_/how-self-attention-with-relative-position-representations-works-28173b8c245a">https://medium.com/@_init_/how-self-attention-with-relative-position-representations-works-28173b8c245a</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h2&gt;&lt;p&gt;1.针对 transformer 结构, 提出一种相对位置编码的方法, 能够</summary>
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>RankFlow Joint Optimization of Multi-Stage Cascade Ranking Systems as Flows</title>
    <link href="http://example.com/2023/10/15/RankFlow%20Joint%20Optimization%20of%20Multi-Stage%20Cascade%20Ranking%20Systems%20as%20Flows/"/>
    <id>http://example.com/2023/10/15/RankFlow%20Joint%20Optimization%20of%20Multi-Stage%20Cascade%20Ranking%20Systems%20as%20Flows/</id>
    <published>2023-10-15T05:08:00.000Z</published>
    <updated>2025-04-28T09:20:54.409Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>1.多阶段一致性训练方案, 级联排序模型优化, 缓解全投放链路的 sample selection bias (SSB) 问题  </p><h2 id="Formulation"><a href="#Formulation" class="headerlink" title="Formulation"></a>Formulation</h2><p>1.总共有多个排序阶段的模型, 其中第 $i$ 个排序阶段为 $R<em>i$, 比如有 3 个阶段 Recall -&gt; Pre-Ranking -&gt; Ranking, 分别对应的是 $R</em>{i-1}$, $R<em>{i}$ , $R</em>{i+1}$ , 每个阶段的样本的分布为 $\text{Pr}_i(X,Y)$</p><div align="center"><img src="/imgs/RankFlow Joint Optimization of Multi-Stage Cascade Ranking Systems as Flows/0.png" width="40%"/></div><p>2.这个文章有一个基础设定: 优化的分布一致性最终目标是对齐到 impression/display (是否曝光) 这个维度, 即用 impression (log) 作为一致性 ground truth, 类似业务中的下发率预估模型 ; 通常每个Rank阶段都有自己的优化目标, 在这种设定下导致每个阶段的优化目标和最终进入曝光这个目标的分布是存在差异的; 作者提出一种多阶段联合训练的方法去建模各个 Ranking 阶段具备”统一对齐性质地”进入到曝光的分布一致性  </p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><blockquote><p>The major paradigm of RankFlow is fitting each ranker towards its own stage-specific data distribution, with the help of other stages.</p><p>the training data of each stage is generated by its preceding stages while the guidance signals come from not only the logs but also the succeeding ranker. </p></blockquote><p>intuitively,<br>1.整个排序系统分为多个阶段 (召回=&gt;粗排=&gt;精排=&gt;曝光/展示),  RankFlow 的核心思想是对具体某个阶段来说, 一方面建模该阶段进入到曝光的数据分布, 另一方面要借助后续排序阶段的数据分布去进行分布统一性建模, 且两个建模过程采取同时优化的范式, 最终的收益是是能使得对全链路上下游各个阶段保持较好的分布一致性<br>2.具体来说, 每个阶段的训练样本都来自于前面的阶段, 训练的监督信号不仅仅用这个阶段的最终曝光 label 做优化, 还要加上后面那个阶段的 label 信息  </p><p>Data Generation 数据生成<br>1.每个阶段用上一个阶段你的结果截断出来的 TopK 个结果做样本, 其实就是如果是精排用的样本是进入精排的样本=就是全量粗排的 TopK 截断; 如果是粗排就是用的进入粗排的样本 (这里默认进入粗排的样本和全量召回的样本不是一个概念, 用的是召回的样本做了 TopK 截断的样本, 其实就是默认召回阶段也有一个统一的相关性分)<br>2.因为每个阶段用了上一个阶段的样本截断生成, 所以第一个阶段比如召回阶段是没办法生成样本的, 因为它不存在上一个阶段搞相关性截断, 所以只能用随机采样的方式做生成  </p><p>Self-Learning Flow 自学习流<br>1.每个阶段的训练 label 的生成, 其实采用的是以终为始的思想, 我们最终的目标是预测一个 $\langle q,d \rangle$ 最后会不会进入曝光, 所以如果最终进入曝光, 那么设置为正样本; 如果最终没有曝光, 那么是负样本<br>2.构造 loss 的时候用的是一个交叉熵损失的形式, 正负 label 已经有了, 预估分采用的是属于该阶段的相关性分数, 也就是基于模型 $R_i$ 预估出来的分  </p><p>Tutor-Learning Flow 监督下的学习流<br>1.除了 self-learning 的生成之外  </p><div align="center"><img src="/imgs/RankFlow Joint Optimization of Multi-Stage Cascade Ranking Systems as Flows/1.png" width="100%"/></div><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. RankFlow: Joint Optimization of Multi-Stage Cascade Ranking Systems as Flows.  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h2&gt;&lt;p&gt;1.多阶段一致性训练方案, 级联排序模型优化, 缓解全投放链路的 sample select</summary>
      
    
    
    
    <category term="Ads_RecSys" scheme="http://example.com/categories/Ads-RecSys/"/>
    
    
  </entry>
  
  <entry>
    <title>SHARK A Lightweight Model Compression Approach for Large-scale Recommender Systems</title>
    <link href="http://example.com/2023/10/11/SHARK%20A%20Lightweight%20Model%20Compression%20Approach%20for%20Large-scale%20Recommender%20Systems/"/>
    <id>http://example.com/2023/10/11/SHARK%20A%20Lightweight%20Model%20Compression%20Approach%20for%20Large-scale%20Recommender%20Systems/</id>
    <published>2023-10-11T12:43:00.000Z</published>
    <updated>2025-04-28T09:20:54.409Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>1.提出一种轻量级模型压缩方法<br>2.提出一个轻量级的特种重要性评估器  </p><p>总共有 $|d|$ 个样本, $x\in d$ 是一个样本, 每个样本共有 $N$ 个特征, 其中第 $i$ 个特征真实值是 $v_i^*$, 我们用全量 $x$ 的全量特征计算一个损失函数为</p><script type="math/tex; mode=display">loss(v_1^*,\cdots,v_N^*)</script><p>对第 $i$ 个特征, 它的一个采样值是 $v_i^{\prime}$, 我们可以看做从某个分布中采样得到的, 比如从全集数据集中采样或者在训练过程中某个 batch 中采样, 被采集到的概率为 $p(v_i^{\prime})$;<br>为评估第 $i$ 个特征的重要性: 我们可以先跑样本 $x$ 在全量真实特征的 loss; 然后再执行采样特征 $i$ 的过程, 计算一个新的 loss;</p><script type="math/tex; mode=display">\begin{aligned}\text{importance}(i)=&\frac{1}{|d|}\sum_{x\in d}\text{importance}(i, x)\\=&\frac{1}{|d|}\sum_{x\in d}\text{shuffle-feature-i-loss}-\text{origin-loss} \\=&\frac{1}{|d|}\sum_{x\in d}\sum_{v_i\in s,v_i\sim s}loss(v_1^*,\cdots,v_N^*)p(v_i)-loss(v_1^*,\cdots,v_N^*)\end{aligned}</script><p>拆解复杂性来看, 需要遍历一遍特征空间, 也就是以 $O(N)$ 复杂度计算所有特征, 我们能否将对每个特征改成并行评估, 也就是目标将 $O(N)$ 改成 $O(1)$;</p><p>回顾一下 Taylor expansion 的定义, 函数 $f$ 在 $x=a$ 处的 $n$ 次泰勒多项式实现了用 $n$ 次多项式去逼近函数 $f$ 在 $x=a$ 的一个结果;</p><script type="math/tex; mode=display">T_n(x)=f(a)+\frac{f^{'}(a)}{1!}(x-a)+\frac{f^{''}(a)}{2!}(x-a)^2+\cdots+\frac{f^{(n)}(a)}{n!}(x-a)^{n}</script><p>我们迁移 Taylor expansion 思想到特征重要性评估任务上, 核心衡量指标是以全量特征作为输入的损失函数的情况; 将损失函数视为函数 $f$, 考察的自变量位置就是这个特征的变化位置 $v_i^{*}$, 逼近的结果就是它的一个 $n$ 次展开, 例如我们就只考察 1 阶展开  </p><script type="math/tex; mode=display">\begin{aligned}{\rm importance}(i,x)=&\frac{\partial loss(v_1^*,\cdots,v_N^*)}{\partial v_i^*}\sum_{v_i^{\prime}\in s} p(v_i^{\prime}) (v_i^{\prime}-v_i^*) \\=&\frac{\partial loss(v_1^*,\cdots,v_N^*)}{\partial v_i^*}(E(v_i)-v_i^*)\end{aligned}</script><p>exhuasitvely,<br>1.用期望 $E(v_i)$ 去替代 $v_i^{\prime}$<br>2.在训练的过程中, 采用 mini_batch 训练的方法, 每个 batch 可以计算一个 loss, 然后将 loss 并行对所有特征计算一个重要性结果  </p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference."></a>Reference.</h2><p>[1]. SHARK: A Lightweight Model Compression Approach for Large-scale Recommender Systems</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h2&gt;&lt;p&gt;1.提出一种轻量级模型压缩方法&lt;br&gt;2.提出一个轻量级的特种重要性评估器 </summary>
      
    
    
    
    <category term="Ads_RecSys" scheme="http://example.com/categories/Ads-RecSys/"/>
    
    
  </entry>
  
  <entry>
    <title>Say Good Night To Insomnia</title>
    <link href="http://example.com/2023/09/21/Say%20Good%20Night%20To%20Insomnia/"/>
    <id>http://example.com/2023/09/21/Say%20Good%20Night%20To%20Insomnia/</id>
    <published>2023-09-21T12:43:00.000Z</published>
    <updated>2025-04-28T09:20:54.409Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>1.Some Basic Facts About Sleep and Insomnia 关于睡眠和失眠的基础事实<br>2.Establishing Sleep-Promoting Habits 建设睡眠提效习惯  </p><h2 id="Some-Basic-Facts-About-Sleep-and-Insomnia-关于睡眠和失眠的基础事实"><a href="#Some-Basic-Facts-About-Sleep-and-Insomnia-关于睡眠和失眠的基础事实" class="headerlink" title="Some Basic Facts About Sleep and Insomnia 关于睡眠和失眠的基础事实"></a>Some Basic Facts About Sleep and Insomnia 关于睡眠和失眠的基础事实</h2><p>1.我们每天都在睡觉, 但是其实通常我们对睡觉这件事的科学原理知之甚少, 同时我们对失眠这件事的原理更是知之甚少, 因此我们先得搞一下流言终结者, 先正确地认识一下睡眠这件事情, 然后扑灭掉我们脑子里面关于睡眠或者失眠广泛存在的谎言<br>2.这里我们重点要理解的两个概念是 [睡觉的五个阶段] 和 [体温和睡眠的关系] , 理解了这两个概念能为我们正确处理睡眠这件事情打下方法论的基础  </p><h4 id="Five-Stages-of-Sleep-睡觉的五个阶段"><a href="#Five-Stages-of-Sleep-睡觉的五个阶段" class="headerlink" title="Five Stages of Sleep 睡觉的五个阶段"></a>Five Stages of Sleep 睡觉的五个阶段</h4><p>睡觉其实分为多个复杂的阶段, 是一段非常有趣的 “生命历程”, 这里我们研究的是非常正常的睡一晚上会经历哪些阶段: 闭上眼睛身体放松的前几分钟, 这时候出现 $\alpha$ 脑电波<br>(i). 第一阶段的睡眠, 是一种 “drowsy, relaxed state between waking and sleeping” 身体更加放松, 肌肉紧张感减轻, 呼吸和心跳都会减缓, 体温下降, 眼睛翻转缓慢, 出现一个新的脑电波 $\theta$ 脑电波 “You may also experience fragmentary thoughts or a sensation analogous to daydreaming” 可能会浮现碎片的想法或者有类似白日做梦的感觉, 这个第一阶段阶段只有很短的几分钟<br>(ii). 第二阶段的睡眠, 比第一阶段睡得深入一点, 脑电波是 [睡眠梭状波] 或者 $K$ 复合波; 怎么理解这个阶段呢?   </p><blockquote><p>intermittent attempts by the brain to preserve awareness before we “turn off” to our surroundings. Stage 2 is regarded as a light stage of sleep, for we are easily awakened from it.</p></blockquote><p>我们关闭周围环境前: 我们大脑还在间歇性地保持意识, 第二阶段也是一个很浅的睡眠阶段, 这个阶段经历 30-45 分钟<br>(iii) 和 (iv). 第三阶段和第四阶段, 是深睡眠阶段, 脑电波是十分缓慢的 $\delta$ 波, 呼吸/耗氧量/心率/血压都睡降到最低点, 对外界毫无意识, 很难醒过来, 这个阶段持续 45 分钟  </p><p>(v). 45分钟之后进入有梦睡眠, 这段时间叫做 Rapid Eye Movement (REM, 快速眼动期), 做梦的时候眼睛快速转动 (但是我们不知道是不是转动的方向盯着梦境里面的一些场景哈哈), 做梦的时候大脑和身体十分活跃, 心率/血压/呼吸率升高, 变得毫无规律, 脑电波加快, 大脑血流量急剧上升, 所以这么活跃这玩意还叫睡眠吗? 所以 REM 又叫做 paradoxical sleep  </p><blockquote><p>这里插一句题外话, REM 阶段其实还是伴有其他的行为: 男性阴茎勃起和女性阴蒂充血; 也催生了一个检测阳痿的技术: 阳痿分为生理上的心理上的阳痿, 检测是那种我们就看 REM 阶段睡眠里面有没有勃起, 如果没有勃起, 那可能生理上出问题了很严重; 如果有勃起那么就是心理上的因素导致的阳痿  </p></blockquote><p>睡眠的历程, 其实就是 [阶段1-4] 和 [REM阶段] 的合并为1轮周期, 然后循环往复重复多轮周期, 睡眠好的人要经历4-6轮次这样的循环交替, 平均来看每一轮周期结构大概是[5%阶段一, 50%阶段二, 20% 深睡, 25% REM], 轮次和轮次之间有个变化: 前面的轮次深睡长, REM短, 后面的轮次深睡短, REM 越来越长  </p><h2 id="Establishing-Sleep-Promoting-Habits-建设睡眠提效习惯"><a href="#Establishing-Sleep-Promoting-Habits-建设睡眠提效习惯" class="headerlink" title="Establishing Sleep-Promoting Habits 建设睡眠提效习惯"></a>Establishing Sleep-Promoting Habits 建设睡眠提效习惯</h2><blockquote><p>Just as we are creatures of habit, our sleep is a creation of habits.</p></blockquote><p>1.人是习惯的动物, 睡眠也是一种习惯的产物. 我们要学习一些强化大脑睡眠系统的习惯或者行为, 让 [床] 和 [睡眠] 建立一种紧密的关系<br>2.当我们建立一种优秀的习惯的时候, 首先要定义我们的核心目标或者核心指标是是什么? [睡眠时间长度] 不是区分 [高质量睡眠] 和 [失眠] 的正确标准, 这里定义什么是 Sleep efficiency (睡眠效率): 入睡时间和在床上时间的比值  </p><script type="math/tex; mode=display">\text{Sleep efficiency}=\frac{\text{Time Asleep}}{\text{Time in Bed}}</script><p>3.这个作者总结了建设睡眠提效习惯的几个方法, 分别是<br>(i).<br>(ii).<br>(iii).</p><h4 id="A-Regular-Rising-Time-固定起床时间-不补觉"><a href="#A-Regular-Rising-Time-固定起床时间-不补觉" class="headerlink" title="A Regular Rising Time 固定起床时间, 不补觉"></a>A Regular Rising Time 固定起床时间, 不补觉</h4><p>1.有很多失眠的人, 经常在周末或者失眠之后要补觉, 这种做法不对, 长期来看, 会导致更严重的失眠; 正确的做法是: 永远固定固定的起床时间, 永远不去补觉<br>2.这个论点第一次听起来挺反直觉的, 作者解释了一下原因: 主要补觉这种行为会影响到 body-temperature rhythm (体温节奏) 问题, 假设我们周六晚上失眠了, 周日补觉, </p><h4 id="Stimulus-Control-Techniques-刺激控制下的睡眠法"><a href="#Stimulus-Control-Techniques-刺激控制下的睡眠法" class="headerlink" title="Stimulus-Control Techniques 刺激控制下的睡眠法"></a>Stimulus-Control Techniques 刺激控制下的睡眠法</h4><p>1.我们有很多日常的行为, 其实是一种在 [周围环境刺激下的] 行为, 这种 [刺激控制] 英文里面叫做 stimulus-control, 我们的很多行为就是 under sitimulus-control 的, 比如走进去电影院看电影, 就默认好像要买一桶爆米花, 有的时候中午我们吃了不少饭, 下午看电影, 我们通常还会忍不住买一桶爆米花; 白天电话铃想起来, 日常接电话 (比如收快递/老板安排活), 感觉是很合理的, 晚上突然想起来一个电话, 就会感觉到不安 (仿佛有什么紧急的事情) 这种 [白天/黑夜] 对 [某个事情] 就会产生某种 [刺激控制]<br>2.白天的很多刺激会影响我们的很多行为, 同理, 晚上环境/某些事情产生的刺激也会影响我们的 [睡眠行为]; 这些刺激中最常见的一个刺激就是 [床] 对睡眠的刺激, 对于睡得好没失眠问题的人的来说, [床] 对 [睡眠] 的刺激是一种正向的刺激, 刺激更快的入睡; 对于那种经常失眠的人来说, [床] 是个反向的 [刺激], 失眠者经常在客厅电视机前睡着, 但是上了床就清醒过来了<br>3.[强制入睡(的心理暗示)] 也会让 [床] 发出 [清醒暗示], 睡不着的时候迫使自己入睡, 但是强制入睡通常会自己的身体和精神更加兴奋; 有人研究过, 分两组人, 想比一比这两组人谁睡得更快, 和其中一组人说先睡着的人给钱奖励, 和另一组人说正常入睡就行, 结果发现给钱的这波人睡得显然更慢  </p><p>所以作者在前人的基础上改进并提出了一些方法, 来切断 [床] 和 [失眠] 之间的联系, 然后建立 [床] 和 [睡眠] [困倦] 之间的关系</p><p>1.卧室仅仅用来 [睡觉] 和 [性行为]. 把 [清醒] 逐出卧室, 建立 [卧室] 和 [困倦] / [睡眠] 之间的关系<br>2.make sure you feel drowsy when you turn the lights off to go to sleep. [关灯] 的那个动作, 必须保证自己是 [昏昏欲睡] 状态下的关灯动作  </p><blockquote><p>learn to identify your internal cues for drowsiness (eyelids dropping, head nodding, yawning, reading the same line in a book several times and not understanding it) rather than relying on external cues such as the clock, your bed partner’s bedtime, or the end of the evening news. Your goal is to associate your bed with drowsiness.</p></blockquote><p>学着去识别一些属于我们自己的 [内在的困意的表现] 比如点头打瞌睡啊, 打哈欠啊, 阅读文字半天没理解是说啥, 这种线索, 不是那种 [闹钟响起来]</p><p>3.如果在床上 20-30 分钟睡不着, 或者半夜醒来 20-30 分钟内睡不着怎么处理 ?  </p><blockquote><p>If you don’t fall asleep within twenty to thirty minutes, or if you awaken during the night and don’t fall back to sleep within that time, don’t lie in bed tossing and turning (since focusing on the clock only heightens anxiety about falling asleep, the time limit should be estimated). Instead, either go to another room or sit up in bed and engage in a quiet, relaxing activity such as watching television or reading a book, magazine, or catalog until you are drowsy, then attempt to go to sleep again. Repeat this process as often as necessary until you fall asleep.</p></blockquote><p>不要在床上翻来覆去, 辗转反侧: 直接去另一间房间睡, 或者在床上安静地看一会书  </p><blockquote><p>You may also be concerned that reading in bed or getting out of bed when you can’t sleep will disrupt your partner. Most partners will endure this minor inconvenience if your sleep and mood improve as a result. Tossing and turning is more disruptive to most partners than reading in bed or getting out of bed.</p></blockquote><p>有时候在床上读书我们会担心会不会打扰到爱人 ? 如果这种行为可以改善我们的睡眠或者情绪, 大部分人都能接受这种小小的不方便的, 再者说来, 在床上翻来覆去辗转反侧, 相比安静的看书更打扰人  </p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. Say Good Night to Insomnia: The Six-Week, Drug-Free Program Developed At Harvard Medical School.  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h2&gt;&lt;p&gt;1.Some Basic Facts About Sleep and Insomnia 关于</summary>
      
    
    
    
    <category term="Reading" scheme="http://example.com/categories/Reading/"/>
    
    
  </entry>
  
  <entry>
    <title>Scenario-Adaptive Feature Interaction for Click-Through Rate Prediction</title>
    <link href="http://example.com/2023/09/20/Scenario-Adaptive%20Feature%20Interaction%20for%20Click-Through%20Rate%20Prediction/"/>
    <id>http://example.com/2023/09/20/Scenario-Adaptive%20Feature%20Interaction%20for%20Click-Through%20Rate%20Prediction/</id>
    <published>2023-09-20T12:43:00.000Z</published>
    <updated>2025-04-28T10:43:58.318Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>1.提出一种 Scenario-Adaptive 场景自适应的 CTR 预估网络 SATrans, 创新点是设计了显式的场景特征 encoder 模块, 并基于 Transformer 结构设计了如何将场景特征和场景无关特征之间的特征交叉  </p><h2 id="Formulation"><a href="#Formulation" class="headerlink" title="Formulation"></a>Formulation</h2><ul><li>数据集 $\mathcal D={(x<em>i, y_j)}^{|\mathcal D|}</em>{j=1}$ </li><li>特征表示为 ${\bf x}=[{\bf x}^s; {\bf x}^a]=[{\bf x}<em>1^s;\ldots;{\bf x}_M^s;{\bf x_1}^a;\ldots;{\bf x}^{a}</em>{N-M}]$ 分为场景 [相关的特征] 和 [场景无关特征]  </li></ul><h2 id="Multi-Scenario-Modeling-多场景建模"><a href="#Multi-Scenario-Modeling-多场景建模" class="headerlink" title="Multi-Scenario Modeling 多场景建模"></a>Multi-Scenario Modeling 多场景建模</h2><p>整体结构分成两个部分<br>1.Scenario Encoder 场景编码器, 本质是将场景特征做一个基础的编码, 提供给后续特征交叉使用<br>2.Scenario-Adaptive Interaction (SAI) Layer 场景自适应交互层   </p><div align="center"><img src="/imgs/Scenario-Adaptive Feature Interaction for Click-Through Rate Prediction/0.png" width="50%"/></div><p>该模型工作原理如下:<br>1.场景编码器将 [场景相关特征] ${\bf x}^s$ 作为输入获得一个固定长度的场景 embedding $\bf s$, 另外将所有特征 $\bf x$ 过 embedding 层 获得 ${\bf e}=[{\bf e_1};\ldots;{\bf e_N}]$<br>2.堆叠 $l$ 个场景特征交叉层, 每个交叉层将场景相关特征和原始特征 (或者后面的隐层) 做场景特征自适应交叉, 得到 $l+1$ 层, 最终达到一个场景自适应交叉的效果  </p><h2 id="Scenario-Feature-Encoder-场景特征编码器"><a href="#Scenario-Feature-Encoder-场景特征编码器" class="headerlink" title="Scenario Feature Encoder 场景特征编码器"></a>Scenario Feature Encoder 场景特征编码器</h2><p>关于场景特征编码器作者比较了3种设计, 依次是 independent embedding =&gt; encoding network =&gt; embedding network with Structural Position IDs</p><div align="center"><img src="/imgs/Scenario-Adaptive Feature Interaction for Click-Through Rate Prediction/1.png" width="40%"/></div><p>1.独立的场景 embedding 存在不能进行场景共享的问题, 不建议采用; 生成 embedding是 ${\bf s}$<br>2.encoding 网络这种方法增加了一个变换, 得到的固定长度的编码  </p><script type="math/tex; mode=display">{\bf s}={\bf W}_s \text{ReLU}({\bf e}^s)</script><p>3.在 encoding 网络的基础上增加了2个相应的结构信息, 生成的是一对 embedding $\bf s_Q$ 和 $\bf s_k$</p><script type="math/tex; mode=display">{\bf s}_{l,s}={\bf W}_s \text{ReLU}(\text{Concat}({\bf e}^s, {\bf p}_{l,h}))</script><p>$l$ 这个结构信息标识的是在之后的场景自适应交互层的第几层<br>$h$ 这个结构信息标识的是 $h\in {Q, K}$ position 信息 ${\bf p}_{l,h}$ 是要和后面的交互层的 $Q$ 还是 $P$ 去做运算</p><h2 id="Scenario-Adaptive-Interacting-Layer-场景自适应交互层"><a href="#Scenario-Adaptive-Interacting-Layer-场景自适应交互层" class="headerlink" title="Scenario-Adaptive Interacting Layer 场景自适应交互层"></a>Scenario-Adaptive Interacting Layer 场景自适应交互层</h2><p>1.首先抛开场景这个因素, 如果单纯设计特征交叉可以使用 multi-head self-attention的机制<br>2.然后将这种自动特征交叉的机制, 可以扩展到兼容场景特征和全部特征这两类特征上, 从结构由简单到复杂, 依次设计了以下三种结构:  </p><p>mha 如何做特征交叉 ? 对于 第 $i$ 个特征 $h_i$ (第1层情况下 $h_i=e_i$) 和 $h_j$ 个特征做 transformer 的 $K, Q, V$ 的  self-attention 操作</p><script type="math/tex; mode=display">\begin{aligned}&\alpha_{i,j}=\frac{\text{exp}(\phi^{h}(h_i,h_j))}{\sum_{k}^{N}\text{exp}(\phi^{h}(h_i,h_k))} \\&\phi^{h}(h_i,h_j)=\langle{\bf W_Q}^{(h)}h_i,{\bf W_K}^{(h)}h_j\rangle \\&\phi^{(h)}\langle\cdot,\cdot\rangle代表点积, 右上角 (h) 指示属于哪个head \\&\hat h_i^{(h)}=\sum_{l}^M\alpha_{i,j}^{h}({\bf W_{V}}^{(h)}h_l)\end{aligned}</script><p>如何将 mha 这种扩展到 $h_i,h_j,{\bf s_Q}^{(h)},{\bf s_K}^{(h)}$ 这种四元素的 attention score ? 单独设计一个网络产生 weight</p><script type="math/tex; mode=display">\phi_{s,a}^{h}(h_i,h_j,{\bf s}_Q^{(h)},{\bf s}_K^{(h)})</script><div align="center"><img src="/imgs/Scenario-Adaptive Feature Interaction for Click-Through Rate Prediction/2.png" width="100%"/></div><p>为了设计这个交互结构, 从简单到复杂, 用了3种方法<br>(i). SA-Gate (Bit-wise)<br>(ii). SA-Bilinear (Bilinear)<br>(iii). SA-MetaNet (Nonlinear)  </p><p>对于第 1 种: SA-Gate (Bit-wise) , 引入 gate 去做激活, 然后用 inner product 做 attention score</p><script type="math/tex; mode=display">\begin{aligned}&\phi_{s,a}^{h}(h_i,h_j,{\bf s_Q}^{(h)},{\bf s}_K^{(h)})=\langle\sigma({\bf s_Q}^{(h)})\circ({\bf W_Q}^{(h)}h_i),\sigma({\bf s_K}^{(h)})\circ({\bf W_K}^{(h)}h_j)\rangle \\&\circ \ \ \text{denotes element-wise product} \\&\sigma \ \ \text{denotes sigmoid function}\end{aligned}</script><p>对于第 2 种: SA-Bilinear (Bilinear), 先引入的是 1 个场景感知矩阵 ${\bf S}$ , 这个矩阵的维度是由原始的场景 embedding 强行 reshape 过来的, 然后再结合用双线性公式算 attention score  </p><script type="math/tex; mode=display">\begin{aligned}&\phi_{s,a}^{h}(h_i,h_j,{\bf s}_Q^{(h)},{\bf s}_K^{(h)})=({\bf W_Q}^{(h)}h_i)^{\rm T}{\bf S} \ ({\bf W_K}^{(h)}h_i) \\&\text{where} \ \ {\bf S}=\text{Reshape}({\bf s_{Q}}^{(h)})\in \mathbb R^{d\times d}  , \ \ {\bf s_{Q}}^{(h)}={\bf s_{K}}^{(h)}\end{aligned}</script><p>对于第 3 种: SA-MetaNet (Nonlinear) 需要看下细节</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. Scenario-Adaptive Feature Interaction for Click-Through Rate Prediction.  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h2&gt;&lt;p&gt;1.提出一种 Scenario-Adaptive 场景自适应的 CTR 预估</summary>
      
    
    
    
    <category term="Ads_RecSys" scheme="http://example.com/categories/Ads-RecSys/"/>
    
    
  </entry>
  
</feed>

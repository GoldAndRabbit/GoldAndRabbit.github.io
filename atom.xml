<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>G&amp;R Blog</title>
  
  <subtitle>G&amp;R Blog</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2025-04-28T09:20:54.407Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>G&amp;R</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Linux Access the Open Internet</title>
    <link href="http://example.com/2025/02/01/Linux%20Access%20the%20Open%20Internet/"/>
    <id>http://example.com/2025/02/01/Linux%20Access%20the%20Open%20Internet/</id>
    <published>2025-02-01T12:43:00.000Z</published>
    <updated>2025-04-28T09:20:54.407Z</updated>
    
    <content type="html"><![CDATA[<div align="center"><img src="/imgs/Linux Access the Open Internet/0.png" width="60%"/></div><h2 id="Linux-Access-the-Open-Internet"><a href="#Linux-Access-the-Open-Internet" class="headerlink" title="Linux Access the Open Internet"></a>Linux Access the Open Internet</h2><p>1.Hiddify-Linux<br>下载 Hiddify-Linux-x64.AppImage from <a href="https://d2.freessr2.xyz/Hiddify-Linux-x64.AppImage">https://d2.freessr2.xyz/Hiddify-Linux-x64.AppImage</a></p><pre><code class="lang-bash">chmod 777 Hiddify-Linux-x64.AppImage./Hiddify-Linux-x64.AppImage</code></pre><p>2.SS 节点配置<br>从这里获取 <a href="https://github.com/Alvin9999/new-pac/wiki/ss%E5%85%8D%E8%B4%B9%E8%B4%A6%E5%8F%B7">https://github.com/Alvin9999/new-pac/wiki/ss%E5%85%8D%E8%B4%B9%E8%B4%A6%E5%8F%B7</a></p><p>然后 ss 配置复制到剪贴板</p><pre><code class="lang-bash">ss://YWVzLTI1Ni1nY206ZG9uZ3RhaXdhbmcuY29t@46.17.42.24:23355#dongtaiwang.com%E8%8A%82%E7%82%B9ss</code></pre><p>Reference<br>[1]. <a href="https://github.com/Alvin9999/new-pac/wiki/Linux%E7%B3%BB%E7%BB%9F%E7%BF%BB%E5%A2%99%E6%96%B9%E6%B3%95">https://github.com/Alvin9999/new-pac/wiki/Linux%E7%B3%BB%E7%BB%9F%E7%BF%BB%E5%A2%99%E6%96%B9%E6%B3%95</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/Linux Access the Open Internet/0.png&quot; width=&quot;60%&quot;/&gt;
&lt;/div&gt;

&lt;h2 id=&quot;Linux-Access-the-Open-Internet&quot;&gt;&lt;a </summary>
      
    
    
    
    <category term="Coding" scheme="http://example.com/categories/Coding/"/>
    
    
  </entry>
  
  <entry>
    <title>2024 Annual Summary</title>
    <link href="http://example.com/2024/12/29/2024%20Annual%20Summary/"/>
    <id>http://example.com/2024/12/29/2024%20Annual%20Summary/</id>
    <published>2024-12-29T01:00:00.000Z</published>
    <updated>2025-04-28T09:20:54.393Z</updated>
    
    <content type="html"><![CDATA[<h2 id="年度关键词"><a href="#年度关键词" class="headerlink" title="年度关键词"></a>年度关键词</h2><center><font color="#1E90FF" size="5"><strong>Wabi-sabi</strong></font></center><h2 id="视频总结"><a href="#视频总结" class="headerlink" title="视频总结"></a>视频总结</h2><center><iframe src="//player.bilibili.com/player.html?isOutside=true&aid=113737193227374&bvid=BV1vj6eYhE6T&cid=27599308006&p=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height="300"></iframe></center><h2 id="关于-Annual-Summary-改进方案"><a href="#关于-Annual-Summary-改进方案" class="headerlink" title="关于 Annual Summary 改进方案"></a>关于 Annual Summary 改进方案</h2><p>年终总结至今已有 7 年, 重新思考下年度总结的定位, 近几年年终总结是有价值的, 但不够高效, 原因在于<br>1.之前年终总结内容中集中于 [工作内容回顾] [工作方法论总结] [价值观念和学习效率方法论上的转变阐述]，更偏向于 [阐述]，有点像是国外个人效率提升书籍的风格, 但构建年终总结的初衷还不是对外作为个人内容的宣发, 是对于自己效率提升的精神原则强化, 是对自己看的内容, 是训练大脑构建一种有监督的网络, 形成一种做事层面的个人习惯和肌肉记忆<br>2.人之所以走向更高效的产出和更专业的运作, 或者变得更优秀, 关键的词是 “做事” 或者说 “执行”, 而不是 “阐述”, 实践出真知, 坚持多做事, 几乎 (不总结) 是一种显然更高效的状态, 总结的方式应该只有一种: “趁热总结”, 总结的维度压缩到”上个月”, “上周” 这样的时间节点, 赶上的是 “趁热总结”, 有且仅有 “趁热” 的总结才是有意义的总结, 然后快速转向下一个方向去做 “执行” 或者 “试错”; 因此谈到 “总结” 方向应该集中在, 聚焦于 [做事过程中调整的速度] 和 [做事的量] 仅仅这两个方面<br>3.文字是一种总结方式, 今年开始保留文字, 增加视频内容总结<br>4.年中总结具体的做法改进方案如下:<br>(i). 年终总结保留, 减少 [经历和理念阐述] 的篇幅到 2000 字以内, 文字部分写作时间加上上线不得超过 2h<br>(ii). 拍摄视频内容总结  </p><h2 id="工作内容"><a href="#工作内容" class="headerlink" title="工作内容"></a>工作内容</h2><p>1.<strong>Tiktok 广告算法</strong> 国际化广告业务中的关键思考:<br>(i). <strong>重隐私保护下的技术选型</strong> 业务算法重度依赖数据, 而在隐私保护政策下, 如何合理地满足隐私规则的条件下, 去完成算法优化这个问题成为投入占比 40% 以上的问题; 一个好的海外业务算法工程师, 首先是一个隐私保护规则的专家 &amp; 隐私保护构建之上的技术专家<br>(ii). <strong>地理文化差异下的业务算法策略</strong> 大胆激进探索局部的策略, 好比山地作战我们打游击, 平原战场我们堆火炮, 不可以一视同仁. 东南亚的发展中国家的兄弟们不可能有美帝公民消费能力, 从小灌输品牌心智的欧洲人也很不会崇尚发展中国家口中的 “性价比”; 从来没有教科书上的正确的算法策略, 只有最大胆小心求证的算法策略<br>(iii). <strong>团队效率提升来自于高效迭代 pipeline 的构建</strong> 国内广告算法基础建设上构建了稳定有效的 pipeline, 但海外业务的 pipeline 在基础建设上经常出现模块间相互 block 的现象, 导致整体的效率难以做到最大化, 但在高压和急迫的业务目标下通常需要作出一定程度的妥协, 进而无法形成较强的飞轮效应; 其实不仅局限于广告业务, 任何增长很快的业务或者产品, 无一例外都是在紧张的业务迭代中构建了下限很高的 pipeline, 进而使得迭代能够以高效的方式去运行  </p><h2 id="工作方法论"><a href="#工作方法论" class="headerlink" title="工作方法论"></a>工作方法论</h2><p>1.<strong>高级对比工程师</strong><br>对于一个工程师来说, 一切工作皆为 “对比 (学习)”, 贯穿多年的工作形态是 “对比”, 例如<br>(i). 不同方法的 AB 对比<br>(ii). 业务现状的新旧对比, 反馈数据的新旧对比<br>(iii). 同一个技术问题不同解决方法的对比<br>当我们在 “对比” 的时候, 我们就是在执行 “工作”, 高效工作的常态就是更高效地 “对比”  </p><p>2.<strong>打磨是决定任务完成质量中最重要的事情</strong> 打磨实现的效果是, 通过 [实验] 或者 [持续训练] 得到一种解决方案或者习惯, 使得<br>(i). 避免运气的干扰<br>(ii). 避免意外状况的干扰<br>(iii). 避免环境机制内波动的干扰<br>(iv). 避免任何不扎实的因素影响到最终的结果  </p><p>3.<strong>一脚刹车, 一脚油门</strong> 在做事关键的节奏上, 主动慢一下. 一个高效执行的节奏应该是一个 “快慢刀”, 在快速的推进中的某些节点上是很慢的, 在很 “慢” 节奏的理解和推翻中, 马上又很快地去快马加鞭地求证. 在这种设定下, 可能我们会经常出现 “哎等等, 你再说一遍, 哎等等, 你再说一遍, 这个东西到底是怎样的” 类似的场景 </p><p>4.<strong>“理事” 和 “干事” 的比例优化</strong> 把做一件事做成可以分为 “理事” 和 “干事” 两类状态, 其中<br>(i). 理事: 梳理清楚业务的现状是什么, 要达成的目标是什么, 收益空间来自于是什么, 实现的方法是什么, 可能的风险是什么. 以及为了梳理清楚上面的几个 “xx 是什么” 而进行的调研和分析, 包括数据分析和意见咨询, 和源码阅读等. 所谓 “不可不察也”, 就是说明 理事的重要性<br>(ii). 干事: 实际的执行. 除了理事以外的剩余的事情<br>这里这两件事的投入时间比例决定了一个人能力的高低, 能力越强的人, 理事占比越高, 理事更加清晰有效, 能够从复杂的业务和复杂的系统中一眼看出来风险, 关键点, 本质, 运动规律和未来展望; 这里不是说理事的人干事不行, 优秀人才都默认能 100% 完成 “干事” 的事, 在我的视野下, 优秀的人 “理事” 的精力消耗占比不应该低于 50%, 维持在 60%-70%, 追求 80%-90%  </p><p>5.<strong>算法工程师的两类问题</strong> 作为算法工程师, 面对一个问题, 需要判断一个 (要解决的) 问题是 (更应该被定义为) 数学上的道理解决的问题还是一个系统问题 (系统链路, 系统模块之上的和所需要的信息量上的道理解决的问题)  </p><p>6.<strong>两类招聘小弟的选择原则</strong><br>(i). 能持续提供聪明的 idea 的<br>(ii). 不能持续提供聪明的 idea, 但是能持续超级勤奋, 不怕脏活累活  </p><h2 id="价值观升级"><a href="#价值观升级" class="headerlink" title="价值观升级"></a>价值观升级</h2><p>1.<strong>wabi-sabi</strong> 来自于日本的一种哲学理念, 中文没有任何靠谱的翻译, 广泛应用于艺术设计和公司管理和生活理念, 大意是 “默认认可万物的不完美性和持续变化性, 进而获得更持久的进步或者更和谐的状态” 首先认可万物都是处于一种不完美的状态, 为什么优秀的日本人是不断努力改进和追求目标呢? 因为当前这个状态是不完美的, 所以我每天早晨起来去改进这件事, 但是这个不完美不是我的一种 “错误”, 是一种普遍存在的哲学状态  </p><p>2.<strong>start your day in you own life and not in somebody else’s life</strong> 社交媒体泛滥的时代, 我们很容易被卷入 somebody else’s life, 比如 xx 获奖了, xx 房子降价了, 某个大佬又说什么名言金句了. 这都是 somebody else’s life, 我们每天都应 start day in own life, 时间放在自己的生活上  </p><p>3.<strong>耐心是最牛逼的品质</strong>, 生活中一切有挑战性的问题都是耐心练习, 每天都要做耐心练习, 并且把所有遇到的挑战当作是 “耐心练习的机会”, 啊上帝你真好, 赐给了我一个真么好的机会让我练习耐心 </p><p>4.<strong>趁热总结</strong> 世界上只有一种有效的总结, 叫做 “趁热总结”, 总结当且仅仅当和下一步执行紧紧结合在一起, 总结才有意义, 和吃鱼差不多, 不管什么鱼只有新鲜的才好吃; 不管是项目还是自我提升的过应该类似一个车轮一样, 有 2 个点, 想法 (总结) =&gt; 执行  =&gt; 想法 (总结) =&gt; 执行 的滚轮, 如果不滚起来, 单纯的 “总结” 没有任何意义   </p><p>5.<strong>Thinking like a farmer</strong> 把自己的一年的工作当作是春天种地, 夏天劳作, 秋天收获的过程, 每天和每天的之间的差异就在于一天内从早到晚的过程, 比如今天我种下了多少颗种子, 翻了多少土, 翻了多少土回家的时候, 晚上不去思考今年我会收获多少种子能赚多少钱, 或者今年可能收获很少可能赚不了多少钱  </p><p>6.<strong>宁静致远</strong>: 什么是宁静致远? 宁静这个词实在是抽象, 来自我们中国的概念总是有着很高的理解成本, 需要更多的阅历和更多的思考才能感受到前人的智慧. 但今年第一次有了一些体验. 好比一个由多个零件构成的快速运转的机器, 局部有很精确的处理, 达到全局的有效化  </p><p>7.<strong>have a wonderful rest of you day</strong> 这是一个来自斯坦福同事写道的一句话, 我发现理论出身水平越高的人, 更十分擅长提供情绪价值, 虽然是一句很简单的话, 但让我记住了很久  </p><h2 id="学习方法论"><a href="#学习方法论" class="headerlink" title="学习方法论"></a>学习方法论</h2><p>1.<strong>“学东西”其实是个伪命题</strong>, “学东西” 是个非常低级别的概念, 因为没有和目标结合的 “学东西” 毫无意义, 有且仅有目标的时候, 学东西这个概念才是生效的  </p><p>2.<strong>问, 是唯一能做的关键事情</strong> 任何公开的资料都能我们都可从互联网上获取, 但是能否主动的提问, 以及能和谁去提问, 拉开了我们的学习的效率差距. 我们能做的只有更主动的问. 就像苏格拉底一样, 我知道 “我不知道” 是最重要的事情, 因为我知道我不知道, 所以我可以和你讨论, 和你提问, 我能通过和你讨论或者提问有所收获;</p><p>3.<strong>无缝插入的提问</strong> 随机插入的任何时间地点任何时间任何形式的提问, 并交谈中强行插入询问, 比约会 &amp; 开会, 约饭要高效的多, 一方面不会考虑立场和其他的因素, 给到一个更直接更本能的描述; 同时也不需要消耗更多的时间  </p><p>4.<strong>学习一个东西, 本质是学习这个东西的边界</strong> 学习这件事情, 学习的知识的内容不是最重要的, 学习到的内容的能力的边界是最重要的, 当我们能完全理解清楚我们要学习的内容的能力边界, 学习这件事情就结束了; 注意这里不是说对于一个内容不了解它内部的结构和本质, 因为如果不了解本质, 也就无法知道完整的边界; 之所以强调这个概念, 是为了将 “学习” 和 “世界” 做连接本身比沉浸在细节中重要的多   </p><p>5.<strong>论文笔记要以面带点</strong>. paper notes 的方式不够高效, 能转化成方向性的梳理的 paper notes, 一定用方向性的 paper notes, 不单独写 paper notes  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;年度关键词&quot;&gt;&lt;a href=&quot;#年度关键词&quot; class=&quot;headerlink&quot; title=&quot;年度关键词&quot;&gt;&lt;/a&gt;年度关键词&lt;/h2&gt;&lt;center&gt;
&lt;font color=&quot;#1E90FF&quot; size=&quot;5&quot;&gt;
&lt;strong&gt;Wabi-sabi&lt;/s</summary>
      
    
    
    
    <category term="Rethinking" scheme="http://example.com/categories/Rethinking/"/>
    
    
  </entry>
  
  <entry>
    <title>Ubuntu 24.04 Upgrade Log</title>
    <link href="http://example.com/2024/10/30/Ubuntu%2024.04%20Upgrade%20Log/"/>
    <id>http://example.com/2024/10/30/Ubuntu%2024.04%20Upgrade%20Log/</id>
    <published>2024-10-30T12:00:00.000Z</published>
    <updated>2025-04-28T09:20:54.413Z</updated>
    
    <content type="html"><![CDATA[<div align="center"><img src="/imgs/Ubuntu 24.04 Upgrade Log/0.png" width="80%"/></div><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>1.解决中文输入法直接挂掉: 设置输入法<br>2.解决界面卡顿: nvidia-driver 重安装<br>3.解决鼠标右键不能用: 重装 gnome-tweaks  </p><h2 id="解决中文输入法直接挂掉-设置输入法"><a href="#解决中文输入法直接挂掉-设置输入法" class="headerlink" title="解决中文输入法直接挂掉: 设置输入法"></a>解决中文输入法直接挂掉: 设置输入法</h2><p>1.去 Region &amp; Language 里面修改引擎为 IBus, (印象中之前是设置为 Fcitx4 的)<br>2.输入法的 Preference 里面修改 mode 为 full pinyin (升级完成自动变成了 Double yinpin)  </p><h2 id="解决界面卡顿-nvidia-driver-重安装"><a href="#解决界面卡顿-nvidia-driver-重安装" class="headerlink" title="解决界面卡顿: nvidia-driver 重安装"></a>解决界面卡顿: nvidia-driver 重安装</h2><p>发现升级完成之后界面直接变卡了, 原因是 nvidia-driver 挂了, 锁定原因的现象是  </p><pre><code class="lang-bash">nvida-msi</code></pre><p>发现没有输出结果, 解决方式是</p><pre><code class="lang-bash">sudo ubuntu-drivers install</code></pre><p>然后重新启动</p><pre><code class="lang-bash">reboot</code></pre><p>发现界面变得正常丝滑起来了</p><h2 id="解决鼠标右键不能用-重装-gnome-tweaks"><a href="#解决鼠标右键不能用-重装-gnome-tweaks" class="headerlink" title="解决鼠标右键不能用: 重装 gnome-tweaks"></a>解决鼠标右键不能用: 重装 gnome-tweaks</h2><p>发现鼠标右键直接不能用了</p><pre><code class="lang-bash">sudo apt install gnome-tweaks</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. <a href="https://ubuntu.com/server/docs/nvidia-drivers-installation">https://ubuntu.com/server/docs/nvidia-drivers-installation</a>  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/Ubuntu 24.04 Upgrade Log/0.png&quot; width=&quot;80%&quot;/&gt;
&lt;/div&gt;

&lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;head</summary>
      
    
    
    
    <category term="Coding" scheme="http://example.com/categories/Coding/"/>
    
    
  </entry>
  
  <entry>
    <title>gold_cv</title>
    <link href="http://example.com/2024/10/25/gold_cv/"/>
    <id>http://example.com/2024/10/25/gold_cv/</id>
    <published>2024-10-24T16:00:00.000Z</published>
    <updated>2025-10-25T13:29:28.093Z</updated>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="密码错误！" data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="ca94b07af86b805cd87135f814e20cb872e6d7153ba3952ae9dd8ec0b704db1f">448e2aa852ea4e1d3d1feb3e4ff9338bd643e60b238e1e9a07c061e88b99bfa1002d46a45c30f75fbdb451aa97c3fdda11781e564d738f5d36f47cf021b40a1e</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">密码错误，请再试一次！</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <summary type="html">这是一篇加密文章，输入密码以继续阅读。</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Coding Config</title>
    <link href="http://example.com/2024/10/05/Coding%20Config/"/>
    <id>http://example.com/2024/10/05/Coding%20Config/</id>
    <published>2024-10-04T18:01:00.000Z</published>
    <updated>2025-05-05T12:11:30.773Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>1.vim<br>2.git<br>3.zshrc</p><h2 id="vim"><a href="#vim" class="headerlink" title="vim"></a>vim</h2><pre><code class="lang-bash">syntax enable        &quot; 启用语法高亮set number           &quot; 显示行号set tabstop=4        &quot; Tab键缩进4空格set expandtab        &quot; Tab转空格set autoindent       &quot; 自动缩进set cursorline       &quot; 高亮当前行colorscheme monokai</code></pre><h2 id="git"><a href="#git" class="headerlink" title="git"></a>git</h2><pre><code class="lang-bash">[user]    name = goldandrabbit    email = goldandrabbit@foxmail.com[alias]    st = status    ci = commit    co = checkout    br = branch    df = diff</code></pre><h2 id="zshrc"><a href="#zshrc" class="headerlink" title=".zshrc"></a>.zshrc</h2><pre><code class="lang-bash"># If you come from bash you might have to change your $PATH.# export PATH=$HOME/bin:$HOME/.local/bin:/usr/local/bin:$PATH# Path to your Oh My Zsh installation.export ZSH=&quot;$HOME/.oh-my-zsh&quot;# Set name of the theme to load --- if set to &quot;random&quot;, it will# load a random theme each time Oh My Zsh is loaded, in which case,# to know which specific one was loaded, run: echo $RANDOM_THEME# See https://github.com/ohmyzsh/ohmyzsh/wiki/Themes# ZSH_THEME=&quot;robbyrussell&quot;ZSH_THEME=&quot;agnoster&quot;  ## Set list of themes to pick from when loading at random# Setting this variable when ZSH_THEME=random will cause zsh to load# a theme from this variable instead of looking in $ZSH/themes/# If set to an empty array, this variable will have no effect.# ZSH_THEME_RANDOM_CANDIDATES=( &quot;robbyrussell&quot; &quot;agnoster&quot; )# Uncomment the following line to use case-sensitive completion.# CASE_SENSITIVE=&quot;true&quot;# Uncomment the following line to use hyphen-insensitive completion.# Case-sensitive completion must be off. _ and - will be interchangeable.# HYPHEN_INSENSITIVE=&quot;true&quot;# Uncomment one of the following lines to change the auto-update behavior# zstyle &#39;:omz:update&#39; mode disabled  # disable automatic updates# zstyle &#39;:omz:update&#39; mode auto      # update automatically without asking# zstyle &#39;:omz:update&#39; mode reminder  # just remind me to update when it&#39;s time# Uncomment the following line to change how often to auto-update (in days).# zstyle &#39;:omz:update&#39; frequency 13# Uncomment the following line if pasting URLs and other text is messed up.# DISABLE_MAGIC_FUNCTIONS=&quot;true&quot;# Uncomment the following line to disable colors in ls.# DISABLE_LS_COLORS=&quot;true&quot;# Uncomment the following line to disable auto-setting terminal title.# DISABLE_AUTO_TITLE=&quot;true&quot;# Uncomment the following line to enable command auto-correction.# ENABLE_CORRECTION=&quot;true&quot;# Uncomment the following line to display red dots whilst waiting for completion.# You can also set it to another string to have that shown instead of the default red dots.# e.g. COMPLETION_WAITING_DOTS=&quot;%F&#123;yellow&#125;waiting...%f&quot;# Caution: this setting can cause issues with multiline prompts in zsh &lt; 5.7.1 (see #5765)# COMPLETION_WAITING_DOTS=&quot;true&quot;# Uncomment the following line if you want to disable marking untracked files# under VCS as dirty. This makes repository status check for large repositories# much, much faster.# DISABLE_UNTRACKED_FILES_DIRTY=&quot;true&quot;# Uncomment the following line if you want to change the command execution time# stamp shown in the history command output.# You can set one of the optional three formats:# &quot;mm/dd/yyyy&quot;|&quot;dd.mm.yyyy&quot;|&quot;yyyy-mm-dd&quot;# or set a custom format using the strftime function format specifications,# see &#39;man strftime&#39; for details.# HIST_STAMPS=&quot;mm/dd/yyyy&quot;# Would you like to use another custom folder than $ZSH/custom?# ZSH_CUSTOM=/path/to/new-custom-folder# Which plugins would you like to load?# Standard plugins can be found in $ZSH/plugins/# Custom plugins may be added to $ZSH_CUSTOM/plugins/# Example format: plugins=(rails git textmate ruby lighthouse)# Add wisely, as too many plugins slow down shell startup.# plugins=(git)plugins=(git zsh-autosuggestions zsh-syntax-highlighting)source $ZSH/oh-my-zsh.sh# User configuration# export MANPATH=&quot;/usr/local/man:$MANPATH&quot;# You may need to manually set your language environment# export LANG=en_US.UTF-8# Preferred editor for local and remote sessions# if [[ -n $SSH_CONNECTION ]]; then#   export EDITOR=&#39;vim&#39;# else#   export EDITOR=&#39;nvim&#39;# fi# Compilation flags# export ARCHFLAGS=&quot;-arch $(uname -m)&quot;# Set personal aliases, overriding those provided by Oh My Zsh libs,# plugins, and themes. Aliases can be placed here, though Oh My Zsh# users are encouraged to define aliases within a top-level file in# the $ZSH_CUSTOM folder, with .zsh extension. Examples:# - $ZSH_CUSTOM/aliases.zsh# - $ZSH_CUSTOM/macos.zsh# For a full list of active aliases, run `alias`.## Example aliases# alias zshconfig=&quot;mate ~/.zshrc&quot;# alias ohmyzsh=&quot;mate ~/.oh-my-zsh&quot;alias c=&quot;clear&quot;alias b=&quot;cd ..&quot;alias bb=&quot;cd ~&quot;alias v=&quot;vim&quot;alias s=&#39;&quot;/Applications/Sublime Text.app/Contents/SharedSupport/bin/subl&quot;&#39;alias subl=&#39;&quot;/Applications/Sublime Text.app/Contents/SharedSupport/bin/subl&quot;&#39;alias p=&quot;python3&quot;alias hxtest=&quot;cd /Users/gold/repos/hexo &amp;&amp; hexo clean &amp;&amp; hexo g &amp;&amp; hexo s&quot;alias hx=&quot;cd /Users/gold/repos/hexo &amp;&amp; hexo clean &amp;&amp; hexo g &amp;&amp; hexo d&quot;alias gpm=&quot;git add -A &amp;&amp; git ci -m &quot;update&quot; &amp;&amp; git push origin main&quot;alias nbc=&#39;jupyter nbconvert --to markdown&#39;</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h2&gt;&lt;p&gt;1.vim&lt;br&gt;2.git&lt;br&gt;3.zshrc&lt;/p&gt;
&lt;h2 id=&quot;vim&quot;&gt;&lt;a </summary>
      
    
    
    
    <category term="Coding" scheme="http://example.com/categories/Coding/"/>
    
    
  </entry>
  
  <entry>
    <title>QLoRA finetune codegen350M</title>
    <link href="http://example.com/2024/10/03/QLoRA%20finetune%20codegen350M/"/>
    <id>http://example.com/2024/10/03/QLoRA%20finetune%20codegen350M/</id>
    <published>2024-10-03T12:43:00.000Z</published>
    <updated>2025-05-05T11:48:55.243Z</updated>
    
    <content type="html"><![CDATA[<h2 id="qlora-finetune-codegen350M"><a href="#qlora-finetune-codegen350M" class="headerlink" title="qlora_finetune_codegen350M"></a>qlora_finetune_codegen350M</h2><p>1.采用 QLoRA 微调模型 Salesforce/codegen-350M-mono, 是个代码生成的模型, model_card: <a href="https://huggingface.co/Salesforce/codegen-350M-mono">https://huggingface.co/Salesforce/codegen-350M-mono</a><br>2.微调数据集采用 iamtarun/python_code_instructions_18k_alpaca, dataset card: <a href="https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca">https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca</a><br>3.如何配置 LoRA 微调的方式? 相比原有的微调多一个 LoRAConfig<br>4.微调效果评估: 对比 base_model 和 lora_merged_model 生成代码的质量, 看下微调是否带来生成质量的提升  </p><pre><code class="lang-python"># 配置环境%pip install transformers datasets huggingface_hub accelerate bitsandbytes%pip install trl peft loralib wandb evaluate scikit-learn%pip install ipywidgets widgetsnbextension</code></pre><pre><code>Looking in indexes: http://mirrors.aliyun.com/pypi/simpleRequirement already satisfied: transformers in /root/miniconda3/lib/python3.10/site-packages (4.51.3)Requirement already satisfied: datasets in /root/miniconda3/lib/python3.10/site-packages (3.5.1)Requirement already satisfied: huggingface_hub in /root/miniconda3/lib/python3.10/site-packages (0.30.2)Requirement already satisfied: accelerate in /root/miniconda3/lib/python3.10/site-packages (1.6.0)Requirement already satisfied: bitsandbytes in /root/miniconda3/lib/python3.10/site-packages (0.45.5)Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (2024.11.6)Requirement already satisfied: numpy&gt;=1.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (1.26.4)Requirement already satisfied: pyyaml&gt;=5.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (6.0.1)Requirement already satisfied: packaging&gt;=20.0 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (24.1)Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from transformers) (3.14.0)Requirement already satisfied: tokenizers&lt;0.22,&gt;=0.21 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (0.21.1)Requirement already satisfied: requests in /root/miniconda3/lib/python3.10/site-packages (from transformers) (2.32.3)Requirement already satisfied: tqdm&gt;=4.27 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (4.67.1)Requirement already satisfied: safetensors&gt;=0.4.3 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (0.5.3)Requirement already satisfied: pyarrow&gt;=15.0.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (20.0.0)Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.5.0)Requirement already satisfied: fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (2024.6.0)Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.11.18)Requirement already satisfied: dill&lt;0.3.9,&gt;=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.3.8)Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (from datasets) (2.2.3)Requirement already satisfied: multiprocess&lt;0.70.17 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.70.16)Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)Requirement already satisfied: psutil in /root/miniconda3/lib/python3.10/site-packages (from accelerate) (5.9.8)Requirement already satisfied: torch&gt;=2.0.0 in /root/miniconda3/lib/python3.10/site-packages (from accelerate) (2.1.2+cu118)Requirement already satisfied: frozenlist&gt;=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets) (1.6.0)Requirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets) (2.6.1)Requirement already satisfied: yarl&lt;2.0,&gt;=1.17.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets) (1.20.0)Requirement already satisfied: attrs&gt;=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets) (23.2.0)Requirement already satisfied: async-timeout&lt;6.0,&gt;=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets) (5.0.1)Requirement already satisfied: aiosignal&gt;=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets) (1.3.2)Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets) (6.4.3)Requirement already satisfied: propcache&gt;=0.2.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets) (0.3.1)Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests-&gt;transformers) (1.26.13)Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /root/miniconda3/lib/python3.10/site-packages (from requests-&gt;transformers) (2.0.4)Requirement already satisfied: idna&lt;4,&gt;=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests-&gt;transformers) (3.4)Requirement already satisfied: certifi&gt;=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests-&gt;transformers) (2022.12.7)Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from torch&gt;=2.0.0-&gt;accelerate) (2.1.0)Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.10/site-packages (from torch&gt;=2.0.0-&gt;accelerate) (3.1.4)Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from torch&gt;=2.0.0-&gt;accelerate) (1.12.1)Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch&gt;=2.0.0-&gt;accelerate) (3.3)Requirement already satisfied: pytz&gt;=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas-&gt;datasets) (2025.2)Requirement already satisfied: python-dateutil&gt;=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas-&gt;datasets) (2.9.0.post0)Requirement already satisfied: tzdata&gt;=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas-&gt;datasets) (2025.2)Requirement already satisfied: six&gt;=1.5 in /root/miniconda3/lib/python3.10/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;datasets) (1.16.0)Requirement already satisfied: MarkupSafe&gt;=2.0 in /root/miniconda3/lib/python3.10/site-packages (from jinja2-&gt;torch&gt;=2.0.0-&gt;accelerate) (2.1.5)Requirement already satisfied: mpmath&lt;1.4.0,&gt;=1.1.0 in /root/miniconda3/lib/python3.10/site-packages (from sympy-&gt;torch&gt;=2.0.0-&gt;accelerate) (1.3.0)[33mWARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m[33m[0mNote: you may need to restart the kernel to use updated packages.Looking in indexes: http://mirrors.aliyun.com/pypi/simpleRequirement already satisfied: trl in /root/miniconda3/lib/python3.10/site-packages (0.17.0)Requirement already satisfied: peft in /root/miniconda3/lib/python3.10/site-packages (0.15.2)Requirement already satisfied: loralib in /root/miniconda3/lib/python3.10/site-packages (0.1.2)Requirement already satisfied: wandb in /root/miniconda3/lib/python3.10/site-packages (0.19.10)Requirement already satisfied: evaluate in /root/miniconda3/lib/python3.10/site-packages (0.4.3)Requirement already satisfied: scikit-learn in /root/miniconda3/lib/python3.10/site-packages (1.6.1)Requirement already satisfied: transformers&gt;=4.46.0 in /root/miniconda3/lib/python3.10/site-packages (from trl) (4.51.3)Requirement already satisfied: accelerate&gt;=0.34.0 in /root/miniconda3/lib/python3.10/site-packages (from trl) (1.6.0)Requirement already satisfied: datasets&gt;=3.0.0 in /root/miniconda3/lib/python3.10/site-packages (from trl) (3.5.1)Requirement already satisfied: rich in /root/miniconda3/lib/python3.10/site-packages (from trl) (14.0.0)Requirement already satisfied: packaging&gt;=20.0 in /root/miniconda3/lib/python3.10/site-packages (from peft) (24.1)Requirement already satisfied: tqdm in /root/miniconda3/lib/python3.10/site-packages (from peft) (4.67.1)Requirement already satisfied: torch&gt;=1.13.0 in /root/miniconda3/lib/python3.10/site-packages (from peft) (2.1.2+cu118)Requirement already satisfied: safetensors in /root/miniconda3/lib/python3.10/site-packages (from peft) (0.5.3)Requirement already satisfied: psutil in /root/miniconda3/lib/python3.10/site-packages (from peft) (5.9.8)Requirement already satisfied: huggingface_hub&gt;=0.25.0 in /root/miniconda3/lib/python3.10/site-packages (from peft) (0.30.2)Requirement already satisfied: pyyaml in /root/miniconda3/lib/python3.10/site-packages (from peft) (6.0.1)Requirement already satisfied: numpy&gt;=1.17 in /root/miniconda3/lib/python3.10/site-packages (from peft) (1.26.4)Requirement already satisfied: requests&lt;3,&gt;=2.0.0 in /root/miniconda3/lib/python3.10/site-packages (from wandb) (2.32.3)Requirement already satisfied: platformdirs in /root/miniconda3/lib/python3.10/site-packages (from wandb) (4.2.2)Requirement already satisfied: pydantic&lt;3 in /root/miniconda3/lib/python3.10/site-packages (from wandb) (2.11.4)Requirement already satisfied: sentry-sdk&gt;=2.0.0 in /root/miniconda3/lib/python3.10/site-packages (from wandb) (2.27.0)Requirement already satisfied: typing-extensions&lt;5,&gt;=4.4 in /root/miniconda3/lib/python3.10/site-packages (from wandb) (4.12.2)Requirement already satisfied: docker-pycreds&gt;=0.4.0 in /root/miniconda3/lib/python3.10/site-packages (from wandb) (0.4.0)Requirement already satisfied: click!=8.0.0,&gt;=7.1 in /root/miniconda3/lib/python3.10/site-packages (from wandb) (8.1.8)Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,&lt;7,&gt;=3.19.0 in /root/miniconda3/lib/python3.10/site-packages (from wandb) (4.25.3)Requirement already satisfied: setproctitle in /root/miniconda3/lib/python3.10/site-packages (from wandb) (1.3.6)Requirement already satisfied: gitpython!=3.1.29,&gt;=1.0.0 in /root/miniconda3/lib/python3.10/site-packages (from wandb) (3.1.44)Requirement already satisfied: setuptools in /root/miniconda3/lib/python3.10/site-packages (from wandb) (65.5.0)Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (from evaluate) (2.2.3)Requirement already satisfied: dill in /root/miniconda3/lib/python3.10/site-packages (from evaluate) (0.3.8)Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from evaluate) (3.5.0)Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from evaluate) (0.70.16)Requirement already satisfied: fsspec[http]&gt;=2021.05.0 in /root/miniconda3/lib/python3.10/site-packages (from evaluate) (2024.6.0)Requirement already satisfied: threadpoolctl&gt;=3.1.0 in /root/miniconda3/lib/python3.10/site-packages (from scikit-learn) (3.6.0)Requirement already satisfied: joblib&gt;=1.2.0 in /root/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.5.0)Requirement already satisfied: scipy&gt;=1.6.0 in /root/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.15.2)Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from datasets&gt;=3.0.0-&gt;trl) (3.14.0)Requirement already satisfied: pyarrow&gt;=15.0.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets&gt;=3.0.0-&gt;trl) (20.0.0)Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets&gt;=3.0.0-&gt;trl) (3.11.18)Requirement already satisfied: six&gt;=1.4.0 in /root/miniconda3/lib/python3.10/site-packages (from docker-pycreds&gt;=0.4.0-&gt;wandb) (1.16.0)Requirement already satisfied: gitdb&lt;5,&gt;=4.0.1 in /root/miniconda3/lib/python3.10/site-packages (from gitpython!=3.1.29,&gt;=1.0.0-&gt;wandb) (4.0.12)Requirement already satisfied: pydantic-core==2.33.2 in /root/miniconda3/lib/python3.10/site-packages (from pydantic&lt;3-&gt;wandb) (2.33.2)Requirement already satisfied: typing-inspection&gt;=0.4.0 in /root/miniconda3/lib/python3.10/site-packages (from pydantic&lt;3-&gt;wandb) (0.4.0)Requirement already satisfied: annotated-types&gt;=0.6.0 in /root/miniconda3/lib/python3.10/site-packages (from pydantic&lt;3-&gt;wandb) (0.7.0)Requirement already satisfied: idna&lt;4,&gt;=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (3.4)Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /root/miniconda3/lib/python3.10/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (2.0.4)Requirement already satisfied: certifi&gt;=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (2022.12.7)Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (1.26.13)Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from torch&gt;=1.13.0-&gt;peft) (1.12.1)Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.10/site-packages (from torch&gt;=1.13.0-&gt;peft) (3.1.4)Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from torch&gt;=1.13.0-&gt;peft) (2.1.0)Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch&gt;=1.13.0-&gt;peft) (3.3)Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers&gt;=4.46.0-&gt;trl) (2024.11.6)Requirement already satisfied: tokenizers&lt;0.22,&gt;=0.21 in /root/miniconda3/lib/python3.10/site-packages (from transformers&gt;=4.46.0-&gt;trl) (0.21.1)Requirement already satisfied: python-dateutil&gt;=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas-&gt;evaluate) (2.9.0.post0)Requirement already satisfied: tzdata&gt;=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas-&gt;evaluate) (2025.2)Requirement already satisfied: pytz&gt;=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas-&gt;evaluate) (2025.2)Requirement already satisfied: markdown-it-py&gt;=2.2.0 in /root/miniconda3/lib/python3.10/site-packages (from rich-&gt;trl) (3.0.0)Requirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /root/miniconda3/lib/python3.10/site-packages (from rich-&gt;trl) (2.18.0)Requirement already satisfied: frozenlist&gt;=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets&gt;=3.0.0-&gt;trl) (1.6.0)Requirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets&gt;=3.0.0-&gt;trl) (2.6.1)Requirement already satisfied: attrs&gt;=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets&gt;=3.0.0-&gt;trl) (23.2.0)Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets&gt;=3.0.0-&gt;trl) (6.4.3)Requirement already satisfied: yarl&lt;2.0,&gt;=1.17.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets&gt;=3.0.0-&gt;trl) (1.20.0)Requirement already satisfied: aiosignal&gt;=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets&gt;=3.0.0-&gt;trl) (1.3.2)Requirement already satisfied: async-timeout&lt;6.0,&gt;=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets&gt;=3.0.0-&gt;trl) (5.0.1)Requirement already satisfied: propcache&gt;=0.2.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets&gt;=3.0.0-&gt;trl) (0.3.1)Requirement already satisfied: smmap&lt;6,&gt;=3.0.1 in /root/miniconda3/lib/python3.10/site-packages (from gitdb&lt;5,&gt;=4.0.1-&gt;gitpython!=3.1.29,&gt;=1.0.0-&gt;wandb) (5.0.2)Requirement already satisfied: mdurl~=0.1 in /root/miniconda3/lib/python3.10/site-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;trl) (0.1.2)Requirement already satisfied: MarkupSafe&gt;=2.0 in /root/miniconda3/lib/python3.10/site-packages (from jinja2-&gt;torch&gt;=1.13.0-&gt;peft) (2.1.5)Requirement already satisfied: mpmath&lt;1.4.0,&gt;=1.1.0 in /root/miniconda3/lib/python3.10/site-packages (from sympy-&gt;torch&gt;=1.13.0-&gt;peft) (1.3.0)[33mWARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m[33m[0mNote: you may need to restart the kernel to use updated packages.Looking in indexes: http://mirrors.aliyun.com/pypi/simpleRequirement already satisfied: ipywidgets in /root/miniconda3/lib/python3.10/site-packages (8.1.3)Requirement already satisfied: widgetsnbextension in /root/miniconda3/lib/python3.10/site-packages (4.0.11)Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /root/miniconda3/lib/python3.10/site-packages (from ipywidgets) (3.0.11)Requirement already satisfied: comm&gt;=0.1.3 in /root/miniconda3/lib/python3.10/site-packages (from ipywidgets) (0.2.2)Requirement already satisfied: traitlets&gt;=4.3.1 in /root/miniconda3/lib/python3.10/site-packages (from ipywidgets) (5.14.3)Requirement already satisfied: ipython&gt;=6.1.0 in /root/miniconda3/lib/python3.10/site-packages (from ipywidgets) (8.25.0)Requirement already satisfied: exceptiongroup in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (1.2.1)Requirement already satisfied: decorator in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (5.1.1)Requirement already satisfied: typing-extensions&gt;=4.6 in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (4.12.2)Requirement already satisfied: pexpect&gt;4.3 in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (4.9.0)Requirement already satisfied: jedi&gt;=0.16 in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (0.19.1)Requirement already satisfied: matplotlib-inline in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (0.1.7)Requirement already satisfied: stack-data in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (0.6.3)Requirement already satisfied: prompt-toolkit&lt;3.1.0,&gt;=3.0.41 in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (3.0.47)Requirement already satisfied: pygments&gt;=2.4.0 in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (2.18.0)Requirement already satisfied: parso&lt;0.9.0,&gt;=0.8.3 in /root/miniconda3/lib/python3.10/site-packages (from jedi&gt;=0.16-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (0.8.4)Requirement already satisfied: ptyprocess&gt;=0.5 in /root/miniconda3/lib/python3.10/site-packages (from pexpect&gt;4.3-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (0.7.0)Requirement already satisfied: wcwidth in /root/miniconda3/lib/python3.10/site-packages (from prompt-toolkit&lt;3.1.0,&gt;=3.0.41-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (0.2.13)Requirement already satisfied: asttokens&gt;=2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from stack-data-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (2.4.1)Requirement already satisfied: executing&gt;=1.2.0 in /root/miniconda3/lib/python3.10/site-packages (from stack-data-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (2.0.1)Requirement already satisfied: pure-eval in /root/miniconda3/lib/python3.10/site-packages (from stack-data-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (0.2.2)Requirement already satisfied: six&gt;=1.12.0 in /root/miniconda3/lib/python3.10/site-packages (from asttokens&gt;=2.1.0-&gt;stack-data-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (1.16.0)[33mWARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m[33m[0mNote: you may need to restart the kernel to use updated packages.</code></pre><pre><code class="lang-python">import subprocessimport osimport wandbON_AUTODL_ENV = Trueif ON_AUTODL_ENV:    result = subprocess.run(&#39;bash -c &quot;source /etc/network_turbo &amp;&amp; env | grep proxy&quot;&#39;, shell=True, capture_output=True, text=True)    output = result.stdout    for line in output.splitlines():        if &#39;=&#39; in line:            var, value = line.split(&#39;=&#39;, 1)            os.environ[var] = valueprint(f&quot;ON_AUTODL_ENV: &#123;ON_AUTODL_ENV&#125;&quot;)TRAINER_EXP_NAME = &quot;qlora_finetune_codegen350M&quot;RUN_NOTEBOOK_NAME = &quot;qlora_finetune_codegen350M.ipynb&quot;from huggingface_hub import loginlogin(token=&quot;your keys&quot;)os.environ[&quot;WANDB_NOTEBOOK_NAME&quot;] = f&quot;&#123;RUN_NOTEBOOK_NAME&#125;.ipynb&quot;  # 替换为实际文件名wandb.init(project=f&quot;&#123;RUN_NOTEBOOK_NAME&#125;&quot;)import torchfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArgumentsfrom datasets import load_datasetif torch.backends.mps.is_available():    print(f&quot;torch.backeds.mps.is_available(): &#123;torch.backends.mps.is_available()&#125;&quot;)    device = torch.device(&quot;mps&quot; if torch.backends.mps.is_available() else &quot;cpu&quot;)elif torch.cuda.is_available():    print(f&quot;torch.cuda.is_available(): &#123;torch.cuda.is_available()&#125;&quot;)    device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</code></pre><pre><code>ON_AUTODL_ENV: True[34m[1mwandb[0m: [33mWARNING[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn&#39;t find qlora_finetune_codegen350M.ipynb.ipynb.[34m[1mwandb[0m: Currently logged in as: [33mgoldandrabbit[0m ([33mgoldandrabbit-g-r[0m) to [32mhttps://api.wandb.ai[0m. Use [1m`wandb login --relogin`[0m to force relogin</code></pre><p>Tracking run with wandb version 0.19.10</p><p>Run data is saved locally in <code>/root/z_notebooks/wandb/run-20250505_140817-0q8dlrxv</code></p><p>Syncing run <strong><a href='https://wandb.ai/goldandrabbit-g-r/qlora_finetune_codegen350M.ipynb/runs/0q8dlrxv' target="_blank">elegant-commander-6</a></strong> to <a href='https://wandb.ai/goldandrabbit-g-r/qlora_finetune_codegen350M.ipynb' target="_blank">Weights &amp; Biases</a> (<a href='https://wandb.me/developer-guide' target="_blank">docs</a>)<br></p><p>View project at <a href='https://wandb.ai/goldandrabbit-g-r/qlora_finetune_codegen350M.ipynb' target="_blank">https://wandb.ai/goldandrabbit-g-r/qlora_finetune_codegen350M.ipynb</a></p><p>View run at <a href='https://wandb.ai/goldandrabbit-g-r/qlora_finetune_codegen350M.ipynb/runs/0q8dlrxv' target="_blank">https://wandb.ai/goldandrabbit-g-r/qlora_finetune_codegen350M.ipynb/runs/0q8dlrxv</a></p><pre><code>torch.cuda.is_available(): True</code></pre><pre><code class="lang-python">import torchfrom datasets import load_datasetfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArgumentsfrom peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM</code></pre><h2 id="微调数据集-python-python-code-instructions-18k-alpaca"><a href="#微调数据集-python-python-code-instructions-18k-alpaca" class="headerlink" title="微调数据集: python python_code_instructions_18k_alpaca"></a>微调数据集: python python_code_instructions_18k_alpaca</h2><p>iamtarun/python_code_instructions_18k_alpaca 是个代码生成数据集,<br>dataset card: <a href="https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca">https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca</a></p><pre><code class="lang-bash">instructionWrite a Python code to get the third largest element in a given row.input[12, 13, 13, 45, 22, 99]outputdef third_largest(lst): if len(lst) &lt; 3: return distinct = [] for i in lst: if i not in distinct: distinct.append(i) distinct.sort(reverse=True) return distinct[2]prompt:Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: Write a Python code to get the third largest element in a given row. ### Input: [12, 13, 13, 45, 22, 99] ### Output: def third_largest(lst): if len(lst) &lt; 3: return distinct = [] for i in lst: if i not in distinct: distinct.append(i) distinct.sort(reverse=True) return distinct[2]</code></pre><pre><code class="lang-python">dataset = load_dataset(&quot;iamtarun/python_code_instructions_18k_alpaca&quot;, split=&quot;train&quot;)dataset = dataset.remove_columns([&quot;prompt&quot;])</code></pre><pre><code class="lang-python">dataset</code></pre><pre><code>Dataset(&#123;    features: [&#39;instruction&#39;, &#39;input&#39;, &#39;output&#39;],    num_rows: 18612&#125;)</code></pre><pre><code class="lang-python">dataset[0]</code></pre><pre><code>&#123;&#39;instruction&#39;: &#39;Create a function to calculate the sum of a sequence of integers.&#39;, &#39;input&#39;: &#39;[1, 2, 3, 4, 5]&#39;, &#39;output&#39;: &#39;# Python code\ndef sum_sequence(sequence):\n  sum = 0\n  for num in sequence:\n    sum += num\n  return sum&#39;&#125;</code></pre><pre><code class="lang-python">dataset[1]</code></pre><pre><code>&#123;&#39;instruction&#39;: &#39;Generate a Python code for crawling a website for a specific type of data.&#39;, &#39;input&#39;: &#39;website: www.example.com \ndata to crawl: phone numbers&#39;, &#39;output&#39;: &quot;import requests\nimport re\n\ndef crawl_website_for_phone_numbers(website):\n    response = requests.get(website)\n    phone_numbers = re.findall(&#39;\\d&#123;3&#125;-\\d&#123;3&#125;-\\d&#123;4&#125;&#39;, response.text)\n    return phone_numbers\n    \nif __name__ == &#39;__main__&#39;:\n    print(crawl_website_for_phone_numbers(&#39;www.example.com&#39;))&quot;&#125;</code></pre><pre><code class="lang-python">dataset[15]</code></pre><pre><code>&#123;&#39;instruction&#39;: &#39;Collate a machine learning model in Python that distinguishes between cats and dogs.&#39;, &#39;input&#39;: &#39;A dataset of 800 images of cats and dogs&#39;, &#39;output&#39;: &quot;import numpy as np\nimport keras\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# Create the neural network model\nmodel = Sequential()\n\n# Input layer\nmodel.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = &#39;relu&#39;))\n\n# Hidden layers\nmodel.add(MaxPooling2D(pool_size = (2, 2)))\nmodel.add(Dropout(0.3))\nmodel.add(Conv2D(64, (3, 3), activation = &#39;relu&#39;))\nmodel.add(MaxPooling2D(pool_size = (2, 2)))\nmodel.add(Dropout(0.3))\nmodel.add(Conv2D(64, (3, 3), activation = &#39;relu&#39;))\nmodel.add(MaxPooling2D(pool_size = (2, 2)))\nmodel.add(Dropout(0.3))\n\n# Output layer\nmodel.add(Flatten())\nmodel.add(Dense(units = 128, activation = &#39;relu&#39;))\nmodel.add(Dense(units = 1, activation = &#39;sigmoid&#39;))\n\n# Compile the model\nmodel.compile(loss = &#39;binary_crossentropy&#39;, optimizer = &#39;adam&#39;, metrics = [&#39;accuracy&#39;])\n\n# Create data generator\ndatagen = ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n\n# Fit the model\ntrain_generator = datagen.flow_from_directory(directory = &#39;/path/to/dataset&#39;, target_size = (64, 64), color_mode = &#39;rgb&#39;, class_mode = &#39;binary&#39;, batch_size = 32)\nmodel.fit_generator(generator = train_generator, steps_per_epoch = 800, epochs = 5, validation_data = test_generator, validation_steps = 200)&quot;&#125;</code></pre><h2 id="搭建-QLoRA-微调-Pipeline"><a href="#搭建-QLoRA-微调-Pipeline" class="headerlink" title="搭建 QLoRA 微调 Pipeline"></a>搭建 QLoRA 微调 Pipeline</h2><p>1.定义 base_model, 其实就是我们 load 预训练的模型之外包一层量化相关配置<br>这里通过配置 bnb_config 生效 4 bit 量化<br>2.定义 lora_config: 实例化一个 LoraConfig()<br>(i). 配置 lora low-rank 的 r<br>(ii). 配置 task_type, 我们选择 CASUAL_LM<br>3.Token config, 常规配置<br>4.定义 SFTTrainer, SFTTrainer 是来自于 trl 这个库<br>这里根据我们要做的代码生成任务 SFTTrainer 配置带一个代码生成的 prompt 模板, 这样我们就不需要手动处理数据适配生成模板, 然后开启微调 train()<br>5.merge adapter, 保存模型</p><pre><code class="lang-python">bnb_config = BitsAndBytesConfig(    load_in_4bit=True,    bnb_4bit_quant_type=&quot;nf4&quot;,    bnb_4bit_compute_dtype=torch.bfloat16,    bnb_4bit_use_double_quant=True)</code></pre><pre><code class="lang-python">from accelerate import Accelerator# 在训练代码前添加 accelerator 初始化accelerator = Accelerator()print(f&#39;accelerator.process_index: &#123;accelerator.process_index&#125;&#39;)</code></pre><pre><code>accelerator.process_index: 0</code></pre><h2 id="微调模型-Salesforce-codegen-350M-mono"><a href="#微调模型-Salesforce-codegen-350M-mono" class="headerlink" title="微调模型 Salesforce/codegen-350M-mono"></a>微调模型 Salesforce/codegen-350M-mono</h2><p>CodeGen 是个生成代码的模型: 350M/2B/6B/16B, 我们先微调 350M<br>model_card: <a href="https://huggingface.co/Salesforce/codegen-350M-mono">https://huggingface.co/Salesforce/codegen-350M-mono</a></p><pre><code class="lang-python">model = AutoModelForCausalLM.from_pretrained(    &quot;Salesforce/codegen-350M-mono&quot;,    quantization_config=bnb_config,    # device_map=&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;,    # device_map=device,    device_map=&#123;&quot;&quot;: accelerator.process_index&#125;,    use_cache=False,    trust_remote_code=True)model = prepare_model_for_kbit_training(model)</code></pre><pre><code>/root/miniconda3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()  return self.fget.__get__(instance, owner)()Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: [&#39;transformer.h.0.attn.causal_mask&#39;, &#39;transformer.h.1.attn.causal_mask&#39;, &#39;transformer.h.10.attn.causal_mask&#39;, &#39;transformer.h.11.attn.causal_mask&#39;, &#39;transformer.h.12.attn.causal_mask&#39;, &#39;transformer.h.13.attn.causal_mask&#39;, &#39;transformer.h.14.attn.causal_mask&#39;, &#39;transformer.h.15.attn.causal_mask&#39;, &#39;transformer.h.16.attn.causal_mask&#39;, &#39;transformer.h.17.attn.causal_mask&#39;, &#39;transformer.h.18.attn.causal_mask&#39;, &#39;transformer.h.19.attn.causal_mask&#39;, &#39;transformer.h.2.attn.causal_mask&#39;, &#39;transformer.h.3.attn.causal_mask&#39;, &#39;transformer.h.4.attn.causal_mask&#39;, &#39;transformer.h.5.attn.causal_mask&#39;, &#39;transformer.h.6.attn.causal_mask&#39;, &#39;transformer.h.7.attn.causal_mask&#39;, &#39;transformer.h.8.attn.causal_mask&#39;, &#39;transformer.h.9.attn.causal_mask&#39;]- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).</code></pre><pre><code class="lang-python">model_id = &#39;Salesforce/codegen-350M-mono&#39;tokenizer = AutoTokenizer.from_pretrained(model_id)tokenizer.pad_token = tokenizer.eos_tokentext = &quot;def hello_world():&quot;input_ids = tokenizer(text, return_tensors=&quot;pt&quot;).to(device)generated_ids = model.generate(**input_ids, max_length=128)print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))</code></pre><pre><code>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.def hello_world():    print(&quot;Hello World&quot;)hello_world()# 파이썬에서는 문자열을 반환하는 함수를 사용하여 문자열을 반환하는 함수를 사용하여 문자�</code></pre><pre><code class="lang-python">for name, module in model.named_modules():    if &quot;attn&quot; in name or &quot;attention&quot; in name:        print(f&quot;Attention layer: &#123;name&#125;&quot;)        for sub_name, sub_module in module.named_modules():            print(f&quot; - Sub-module: &#123;sub_name&#125;&quot;)</code></pre><pre><code>Attention layer: transformer.h.0.attn - Sub-module:  - Sub-module: attn_dropout - Sub-module: resid_dropout - Sub-module: qkv_proj - Sub-module: out_projAttention layer: transformer.h.0.attn.attn_dropout - Sub-module: Attention layer: transformer.h.0.attn.resid_dropout - Sub-module: Attention layer: transformer.h.0.attn.qkv_proj - Sub-module: Attention layer: transformer.h.0.attn.out_proj - Sub-module: Attention layer: transformer.h.1.attn - Sub-module:  - Sub-module: attn_dropout - Sub-module: resid_dropout - Sub-module: qkv_proj - Sub-module: out_projAttention layer: transformer.h.1.attn.attn_dropout - Sub-module: Attention layer: transformer.h.1.attn.resid_dropout - Sub-module: Attention layer: transformer.h.1.attn.qkv_proj - Sub-module: Attention layer: transformer.h.1.attn.out_proj - Sub-module: Attention layer: transformer.h.2.attn - Sub-module:  - Sub-module: attn_dropout - Sub-module: resid_dropout - Sub-module: qkv_proj - Sub-module: out_projAttention layer: transformer.h.2.attn.attn_dropout - Sub-module: Attention layer: transformer.h.2.attn.resid_dropout - Sub-module: Attention layer: transformer.h.2.attn.qkv_proj - Sub-module: Attention layer: transformer.h.2.attn.out_proj - Sub-module: Attention layer: transformer.h.3.attn - Sub-module:  - Sub-module: attn_dropout - Sub-module: resid_dropout - Sub-module: qkv_proj - Sub-module: out_projAttention layer: transformer.h.3.attn.attn_dropout - Sub-module: Attention layer: transformer.h.3.attn.resid_dropout - Sub-module: Attention layer: transformer.h.3.attn.qkv_proj - Sub-module: Attention layer: transformer.h.3.attn.out_proj - Sub-module: Attention layer: transformer.h.4.attn - Sub-module:  - Sub-module: attn_dropout - Sub-module: resid_dropout - Sub-module: qkv_proj - Sub-module: out_projAttention layer: transformer.h.4.attn.attn_dropout - Sub-module: Attention layer: transformer.h.4.attn.resid_dropout - Sub-module: Attention layer: transformer.h.4.attn.qkv_proj - Sub-module: Attention layer: transformer.h.4.attn.out_proj - Sub-module: Attention layer: transformer.h.5.attn - Sub-module:  - Sub-module: attn_dropout - Sub-module: resid_dropout - Sub-module: qkv_proj - Sub-module: out_projAttention layer: transformer.h.5.attn.attn_dropout - Sub-module: Attention layer: transformer.h.5.attn.resid_dropout - Sub-module: Attention layer: transformer.h.5.attn.qkv_proj - Sub-module: Attention layer: transformer.h.5.attn.out_proj - Sub-module: Attention layer: transformer.h.6.attn - Sub-module:  - Sub-module: attn_dropout - Sub-module: resid_dropout - Sub-module: qkv_proj - Sub-module: out_projAttention layer: transformer.h.6.attn.attn_dropout - Sub-module: Attention layer: transformer.h.6.attn.resid_dropout - Sub-module: Attention layer: transformer.h.6.attn.qkv_proj - Sub-module: Attention layer: transformer.h.6.attn.out_proj - Sub-module: Attention layer: transformer.h.7.attn - Sub-module:  - Sub-module: attn_dropout - Sub-module: resid_dropout - Sub-module: qkv_proj - Sub-module: out_projAttention layer: transformer.h.7.attn.attn_dropout - Sub-module: Attention layer: transformer.h.7.attn.resid_dropout - Sub-module: Attention layer: transformer.h.7.attn.qkv_proj - Sub-module: Attention layer: transformer.h.7.attn.out_proj - Sub-module: Attention layer: transformer.h.8.attn - Sub-module:  - Sub-module: attn_dropout - Sub-module: resid_dropout - Sub-module: qkv_proj - Sub-module: out_projAttention layer: transformer.h.8.attn.attn_dropout - Sub-module: Attention layer: transformer.h.8.attn.resid_dropout - Sub-module: Attention layer: transformer.h.8.attn.qkv_proj - Sub-module: Attention layer: transformer.h.8.attn.out_proj - Sub-module: Attention layer: transformer.h.9.attn - Sub-module:  - Sub-module: attn_dropout - Sub-module: resid_dropout - Sub-module: qkv_proj - Sub-module: out_projAttention layer: transformer.h.9.attn.attn_dropout - Sub-module: Attention layer: transformer.h.9.attn.resid_dropout - Sub-module: Attention layer: transformer.h.9.attn.qkv_proj - Sub-module: Attention layer: transformer.h.9.attn.out_proj - Sub-module: Attention layer: transformer.h.10.attn - Sub-module:  - Sub-module: attn_dropout - Sub-module: resid_dropout - Sub-module: qkv_proj - Sub-module: out_projAttention layer: transformer.h.10.attn.attn_dropout - Sub-module: Attention layer: transformer.h.10.attn.resid_dropout - Sub-module: Attention layer: transformer.h.10.attn.qkv_proj - Sub-module: Attention layer: transformer.h.10.attn.out_proj - Sub-module: Attention layer: transformer.h.11.attn - Sub-module:  - Sub-module: attn_dropout - Sub-module: resid_dropout - Sub-module: qkv_proj - Sub-module: out_projAttention layer: transformer.h.11.attn.attn_dropout - Sub-module: Attention layer: transformer.h.11.attn.resid_dropout - Sub-module: Attention layer: transformer.h.11.attn.qkv_proj - Sub-module: Attention layer: transformer.h.11.attn.out_proj - Sub-module: Attention layer: transformer.h.12.attn - Sub-module:  - Sub-module: attn_dropout - Sub-module: resid_dropout - Sub-module: qkv_proj - Sub-module: out_projAttention layer: transformer.h.12.attn.attn_dropout - Sub-module: Attention layer: transformer.h.12.attn.resid_dropout - Sub-module: Attention layer: transformer.h.12.attn.qkv_proj - Sub-module: Attention layer: transformer.h.12.attn.out_proj - Sub-module: Attention layer: transformer.h.13.attn - Sub-module:  - Sub-module: attn_dropout - Sub-module: resid_dropout - Sub-module: qkv_proj - Sub-module: out_projAttention layer: transformer.h.13.attn.attn_dropout - Sub-module: Attention layer: transformer.h.13.attn.resid_dropout - Sub-module: Attention layer: transformer.h.13.attn.qkv_proj - Sub-module: Attention layer: transformer.h.13.attn.out_proj - Sub-module: Attention layer: transformer.h.14.attn - Sub-module:  - Sub-module: attn_dropout - Sub-module: resid_dropout - Sub-module: qkv_proj - Sub-module: out_projAttention layer: transformer.h.14.attn.attn_dropout - Sub-module: Attention layer: transformer.h.14.attn.resid_dropout - Sub-module: Attention layer: transformer.h.14.attn.qkv_proj - Sub-module: Attention layer: transformer.h.14.attn.out_proj - Sub-module: Attention layer: transformer.h.15.attn - Sub-module:  - Sub-module: attn_dropout - Sub-module: resid_dropout - Sub-module: qkv_proj - Sub-module: out_projAttention layer: transformer.h.15.attn.attn_dropout - Sub-module: Attention layer: transformer.h.15.attn.resid_dropout - Sub-module: Attention layer: transformer.h.15.attn.qkv_proj - Sub-module: Attention layer: transformer.h.15.attn.out_proj - Sub-module: Attention layer: transformer.h.16.attn - Sub-module:  - Sub-module: attn_dropout - Sub-module: resid_dropout - Sub-module: qkv_proj - Sub-module: out_projAttention layer: transformer.h.16.attn.attn_dropout - Sub-module: Attention layer: transformer.h.16.attn.resid_dropout - Sub-module: Attention layer: transformer.h.16.attn.qkv_proj - Sub-module: Attention layer: transformer.h.16.attn.out_proj - Sub-module: Attention layer: transformer.h.17.attn - Sub-module:  - Sub-module: attn_dropout - Sub-module: resid_dropout - Sub-module: qkv_proj - Sub-module: out_projAttention layer: transformer.h.17.attn.attn_dropout - Sub-module: Attention layer: transformer.h.17.attn.resid_dropout - Sub-module: Attention layer: transformer.h.17.attn.qkv_proj - Sub-module: Attention layer: transformer.h.17.attn.out_proj - Sub-module: Attention layer: transformer.h.18.attn - Sub-module:  - Sub-module: attn_dropout - Sub-module: resid_dropout - Sub-module: qkv_proj - Sub-module: out_projAttention layer: transformer.h.18.attn.attn_dropout - Sub-module: Attention layer: transformer.h.18.attn.resid_dropout - Sub-module: Attention layer: transformer.h.18.attn.qkv_proj - Sub-module: Attention layer: transformer.h.18.attn.out_proj - Sub-module: Attention layer: transformer.h.19.attn - Sub-module:  - Sub-module: attn_dropout - Sub-module: resid_dropout - Sub-module: qkv_proj - Sub-module: out_projAttention layer: transformer.h.19.attn.attn_dropout - Sub-module: Attention layer: transformer.h.19.attn.resid_dropout - Sub-module: Attention layer: transformer.h.19.attn.qkv_proj - Sub-module: Attention layer: transformer.h.19.attn.out_proj - Sub-module: </code></pre><pre><code class="lang-python">peft_config = LoraConfig(    r=16,    lora_alpha=16,    # target_modules=[&#39;qkv_proj&#39;], # 这里和 sub_module 中的明明要对齐    lora_dropout=0.1,    bias=&quot;none&quot;,    task_type=&quot;CAUSAL_LM&quot;)</code></pre><h2 id="定义-prompt-instruction-模板"><a href="#定义-prompt-instruction-模板" class="headerlink" title="定义 prompt instruction 模板"></a>定义 prompt instruction 模板</h2><p>把输入训练样本改写成我们想要的 prompt template, 一种方式是直接对 dataset 处理, 另一种是给到 Trainer 作为参数处理, 这里我们将这个函数给到 Trainer</p><pre><code class="lang-python">def prompt_instruction_format(sample):    return f&quot;&quot;&quot;### Instruction:&#123;sample[&#39;instruction&#39;]&#125;### Input:&#123;sample[&#39;input&#39;]&#125;### Output:&#123;sample[&#39;output&#39;]&#125;&quot;&quot;&quot;</code></pre><pre><code class="lang-python"># 检查模板可用sample = dataset[0]formatted_text = prompt_instruction_format(sample)print(formatted_text)</code></pre><pre><code>### Instruction:Create a function to calculate the sum of a sequence of integers.### Input:[1, 2, 3, 4, 5]### Output:# Python codedef sum_sequence(sequence):  sum = 0  for num in sequence:    sum += num  return sum</code></pre><pre><code class="lang-python">import trlprint(trl.__version__)</code></pre><pre><code>0.17.0</code></pre><h2 id="定义-SFTTrainer"><a href="#定义-SFTTrainer" class="headerlink" title="定义 SFTTrainer"></a>定义 SFTTrainer</h2><p>实例化 trl 里面的 SFTTrainer, 替代了 transformers 里面的 Traniner<br>传入了个 prompt_instruction_format 控制输入 prompt 函数  </p><pre><code class="lang-python">LOCAL_PATH = &quot;/root/autodl-tmp&quot;FT_MODEL_NAME=TRAINER_EXP_NAMELOCAL_OUTPUT_PATH = f&quot;&#123;LOCAL_PATH&#125;/&#123;FT_MODEL_NAME&#125;&quot;print(f&quot;LOCAL_OUTPUT_PATH: &#123;LOCAL_OUTPUT_PATH&#125;&quot;)from trl import SFTTrainertraining_args = TrainingArguments(    output_dir=LOCAL_OUTPUT_PATH,    per_device_train_batch_size=8,    gradient_accumulation_steps=4,    learning_rate=2e-5,    logging_steps=0.05,    logging_strategy=&quot;steps&quot;,    fp16=True,    optim=&quot;paged_adamw_8bit&quot;,  # 改为8bit优化器    use_cpu=False,    save_strategy=&quot;epoch&quot;,    num_train_epochs=3,    logging_dir=LOCAL_OUTPUT_PATH+&#39;/log&#39;)trainer = SFTTrainer(    model=model,    train_dataset=dataset,    peft_config=peft_config,    formatting_func=prompt_instruction_format,    args=training_args)# 添加设备检查代码print(f&quot;Model device: &#123;model.device&#125;&quot;)print(f&quot;Current device: &#123;accelerator.device&#125;&quot;)</code></pre><pre><code>LOCAL_OUTPUT_PATH: /root/autodl-tmp/qlora_finetune_codegen350MNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can&#39;t be set automatically within `Trainer`. Note that empty label_names list will be used instead.Model device: cuda:0Current device: cuda</code></pre><pre><code class="lang-python">trainer.train()</code></pre><pre><code>[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter./root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.  warnings.warn(&lt;div&gt;  &lt;progress value=&#39;1743&#39; max=&#39;1743&#39; style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt;  [1743/1743 33:21, Epoch 2/3]&lt;/div&gt;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;</code></pre><p>  <thead> <tr style="text-align: left;">      <th>Step</th>      <th>Training Loss</th>    </tr>  </thead><br>  <tbody>    <tr>      <td>88</td>      <td>1.066100</td>    </tr>    <tr>      <td>176</td>      <td>0.948400</td>    </tr>    <tr>      <td>264</td>      <td>0.835600</td>    </tr>    <tr>      <td>352</td>      <td>0.809900</td>    </tr>    <tr>      <td>440</td>      <td>0.766200</td>    </tr>    <tr>      <td>528</td>      <td>0.768300</td>    </tr>    <tr>      <td>616</td>      <td>0.762000</td>    </tr>    <tr>      <td>704</td>      <td>0.765300</td>    </tr>    <tr>      <td>792</td>      <td>0.744000</td>    </tr>    <tr>      <td>880</td>      <td>0.768600</td>    </tr>    <tr>      <td>968</td>      <td>0.740400</td>    </tr>    <tr>      <td>1056</td>      <td>0.772900</td>    </tr>    <tr>      <td>1144</td>      <td>0.747100</td>    </tr>    <tr>      <td>1232</td>      <td>0.725800</td>    </tr>    <tr>      <td>1320</td>      <td>0.746100</td>    </tr>    <tr>      <td>1408</td>      <td>0.741000</td>    </tr>    <tr>      <td>1496</td>      <td>0.741400</td>    </tr>    <tr>      <td>1584</td>      <td>0.738400</td>    </tr>    <tr>      <td>1672</td>      <td>0.752900</td>    </tr>  </tbody><br>&lt;/table&gt;<p></p><pre><code>/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.  warnings.warn(/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.  warnings.warn(TrainOutput(global_step=1743, training_loss=0.7850063956474616, metrics=&#123;&#39;train_runtime&#39;: 2002.9908, &#39;train_samples_per_second&#39;: 27.876, &#39;train_steps_per_second&#39;: 0.87, &#39;total_flos&#39;: 4.473852407945626e+16, &#39;train_loss&#39;: 0.7850063956474616&#125;)</code></pre><pre><code class="lang-python">trained_model = AutoPeftModelForCausalLM.from_pretrained(    LOCAL_OUTPUT_PATH + &#39;/checkpoint-1743&#39;,    return_dict=True,    torch_dtype=torch.float16,    device_map=&quot;auto&quot;,)</code></pre><pre><code>Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: [&#39;transformer.h.0.attn.causal_mask&#39;, &#39;transformer.h.1.attn.causal_mask&#39;, &#39;transformer.h.10.attn.causal_mask&#39;, &#39;transformer.h.11.attn.causal_mask&#39;, &#39;transformer.h.12.attn.causal_mask&#39;, &#39;transformer.h.13.attn.causal_mask&#39;, &#39;transformer.h.14.attn.causal_mask&#39;, &#39;transformer.h.15.attn.causal_mask&#39;, &#39;transformer.h.16.attn.causal_mask&#39;, &#39;transformer.h.17.attn.causal_mask&#39;, &#39;transformer.h.18.attn.causal_mask&#39;, &#39;transformer.h.19.attn.causal_mask&#39;, &#39;transformer.h.2.attn.causal_mask&#39;, &#39;transformer.h.3.attn.causal_mask&#39;, &#39;transformer.h.4.attn.causal_mask&#39;, &#39;transformer.h.5.attn.causal_mask&#39;, &#39;transformer.h.6.attn.causal_mask&#39;, &#39;transformer.h.7.attn.causal_mask&#39;, &#39;transformer.h.8.attn.causal_mask&#39;, &#39;transformer.h.9.attn.causal_mask&#39;]- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).</code></pre><p>最后我们把 LoRA 和 base model 合并下，调用的是 merge_and_unload() 这个函数</p><pre><code class="lang-python">lora_merged_model = trained_model.merge_and_unload()</code></pre><pre><code class="lang-python">LOCAL_SAVED_MODEL_PATH = f&quot;&#123;LOCAL_OUTPUT_PATH&#125;/model&quot;UPLOAD_MODEL_NAME = f&quot;goldandrabbit/&#123;FT_MODEL_NAME&#125;&quot;lora_merged_model.save_pretrained(LOCAL_SAVED_MODEL_PATH, safe_serialization=True)tokenizer.save_pretrained(LOCAL_SAVED_MODEL_PATH)lora_merged_model.push_to_hub(UPLOAD_MODEL_NAME)tokenizer.push_to_hub(UPLOAD_MODEL_NAME)</code></pre><pre><code>model.safetensors:   0%|          | 0.00/713M [00:00&lt;?, ?B/s]No files have been modified since last commit. Skipping to prevent empty commit.CommitInfo(commit_url=&#39;https://huggingface.co/goldandrabbit/qlora_finetune_codegen350M/commit/c8ffec6ead8298a99b2a7b3239bab86752212327&#39;, commit_message=&#39;Upload tokenizer&#39;, commit_description=&#39;&#39;, oid=&#39;c8ffec6ead8298a99b2a7b3239bab86752212327&#39;, pr_url=None, repo_url=RepoUrl(&#39;https://huggingface.co/goldandrabbit/qlora_finetune_codegen350M&#39;, endpoint=&#39;https://huggingface.co&#39;, repo_type=&#39;model&#39;, repo_id=&#39;goldandrabbit/qlora_finetune_codegen350M&#39;), pr_revision=None, pr_num=None)</code></pre><h2 id="微调效果评估"><a href="#微调效果评估" class="headerlink" title="微调效果评估"></a>微调效果评估</h2><p>对比 base_model 和 lora_merged_model 生成代码的质量, 看下微调是否带来生成质量的提升  </p><pre><code class="lang-python">instruction=&quot;Collate a machine learning model in Python that distinguishes between cats and dogs&quot;input=&quot;A dataset of 800 images of cats and dogs&quot;prompt = f&quot;&quot;&quot;### Instruction:&#123;instruction&#125;### Input:&#123;input&#125;### Output:&quot;&quot;&quot;input_ids = tokenizer(prompt, return_tensors=&quot;pt&quot;, truncation=True).input_ids.cuda()print(f&quot;Before Training Response:&quot;)output_before = model.generate(input_ids=input_ids, max_new_tokens=300, do_sample=True, top_p=0.9, temperature=0.6, max_length=512)print(f&quot;&#123;tokenizer.decode(output_before[0], skip_special_tokens=True)&#125;&quot;)</code></pre><pre><code>The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input&#39;s `attention_mask` to obtain reliable results.Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.Both `max_new_tokens` (=300) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)Before Training Response:### Instruction:Collate a machine learning model in Python that distinguishes between cats and dogs### Input:A dataset of 800 images of cats and dogs### Output:import numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.metrics import confusion_matrix# Load the datasetdata = pd.read_csv(&quot;dataset.csv&quot;)# Split the data into training and testing dataX = data.iloc[:, :-1].valuesy = data.iloc[:, -1].valuesX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)# Standardize the datascaler = StandardScaler()X_train = scaler.fit_transform(X_train)X_test = scaler.transform(X_test)# Create the modelmodel = RandomForestClassifier(n_estimators=100)# Train the modelmodel.fit(X_train, y_train)# Predict the labelsy_pred = model.predict(X_test)# Confusion matrixcm = confusion_matrix(y_test, y_pred)# Print the confusion matrixprint(</code></pre><pre><code class="lang-python">print(f&quot;After Training Response:&quot;)outputs = lora_merged_model.generate(input_ids=input_ids, max_new_tokens=300, do_sample=True, top_p=0.9, temperature=0.6, max_length=512)print(f&quot;&#123;tokenizer.decode(outputs[0], skip_special_tokens=True)&#125;&quot;)</code></pre><pre><code>The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input&#39;s `attention_mask` to obtain reliable results.Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.Both `max_new_tokens` (=300) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)After Training Response:### Instruction:Collate a machine learning model in Python that distinguishes between cats and dogs### Input:A dataset of 800 images of cats and dogs### Output:import numpy as npimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import accuracy_score# Read the datadata = np.load(&#39;../data/cats_and_dogs.npz&#39;)X, y = data[&#39;arr_0&#39;], data[&#39;arr_1&#39;]# Split the dataX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)# Create the modelmodel = LogisticRegression()model.fit(X_train, y_train)# Evaluate the modely_pred = model.predict(X_test)acc = accuracy_score(y_test, y_pred)print(&#39;Accuracy:&#39;, acc)# Plot the training dataplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=&#39;viridis&#39;, edgecolors=&#39;black&#39;)plt.show()</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. <a href="https://medium.com/@abvijaykumar/fine-tuning-llm-parameter-efficient-fine-tuning-peft-lora-qlora-part-2-d8e23877ac6f">https://medium.com/@abvijaykumar/fine-tuning-llm-parameter-efficient-fine-tuning-peft-lora-qlora-part-2-d8e23877ac6f</a>   </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;qlora-finetune-codegen350M&quot;&gt;&lt;a href=&quot;#qlora-finetune-codegen350M&quot; class=&quot;headerlink&quot; title=&quot;qlora_finetune_codegen350M&quot;&gt;&lt;/a&gt;qlora_fi</summary>
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>Finetune GPT2 on wiki-text</title>
    <link href="http://example.com/2024/10/02/Finetune%20GPT2%20on%20wiki-text/"/>
    <id>http://example.com/2024/10/02/Finetune%20GPT2%20on%20wiki-text/</id>
    <published>2024-10-02T12:43:00.000Z</published>
    <updated>2025-05-05T11:45:21.581Z</updated>
    
    <content type="html"><![CDATA[<h2 id="finetune-GPT2-on-wiki-text"><a href="#finetune-GPT2-on-wiki-text" class="headerlink" title="finetune GPT2 on wiki-text"></a>finetune GPT2 on wiki-text</h2><p>1.使用 wiki 数据集微调 GPT2, 保存并 push 到 huggingface<br>2.本地 inference 微调后的模型推理<br>3.从 huggingface 拉模型推理  </p><h2 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h2><pre><code class="lang-python">%pip install transformers datasets huggingface_hub accelerate bitsandbytes trl peft loralib wandb%pip install ipywidgets widgetsnbextension</code></pre><pre><code>Looking in indexes: http://mirrors.aliyun.com/pypi/simpleRequirement already satisfied: transformers in /root/miniconda3/lib/python3.10/site-packages (4.51.3)Requirement already satisfied: datasets in /root/miniconda3/lib/python3.10/site-packages (3.5.1)Requirement already satisfied: huggingface_hub in /root/miniconda3/lib/python3.10/site-packages (0.30.2)Requirement already satisfied: accelerate in /root/miniconda3/lib/python3.10/site-packages (1.6.0)Requirement already satisfied: bitsandbytes in /root/miniconda3/lib/python3.10/site-packages (0.45.5)Requirement already satisfied: trl in /root/miniconda3/lib/python3.10/site-packages (0.17.0)Requirement already satisfied: peft in /root/miniconda3/lib/python3.10/site-packages (0.15.2)Requirement already satisfied: loralib in /root/miniconda3/lib/python3.10/site-packages (0.1.2)Requirement already satisfied: wandb in /root/miniconda3/lib/python3.10/site-packages (0.19.10)Requirement already satisfied: tokenizers&lt;0.22,&gt;=0.21 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (0.21.1)Requirement already satisfied: pyyaml&gt;=5.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (6.0.1)Requirement already satisfied: requests in /root/miniconda3/lib/python3.10/site-packages (from transformers) (2.32.3)Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from transformers) (3.14.0)Requirement already satisfied: packaging&gt;=20.0 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (24.1)Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (2024.11.6)Requirement already satisfied: safetensors&gt;=0.4.3 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (0.5.3)Requirement already satisfied: tqdm&gt;=4.27 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (4.67.1)Requirement already satisfied: numpy&gt;=1.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (1.26.4)Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (from datasets) (2.2.3)Requirement already satisfied: dill&lt;0.3.9,&gt;=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.3.8)Requirement already satisfied: multiprocess&lt;0.70.17 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.70.16)Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.11.18)Requirement already satisfied: fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (2024.6.0)Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.5.0)Requirement already satisfied: pyarrow&gt;=15.0.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (20.0.0)Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)Requirement already satisfied: torch&gt;=2.0.0 in /root/miniconda3/lib/python3.10/site-packages (from accelerate) (2.1.2+cu118)Requirement already satisfied: psutil in /root/miniconda3/lib/python3.10/site-packages (from accelerate) (5.9.8)Requirement already satisfied: rich in /root/miniconda3/lib/python3.10/site-packages (from trl) (14.0.0)Requirement already satisfied: click!=8.0.0,&gt;=7.1 in /root/miniconda3/lib/python3.10/site-packages (from wandb) (8.1.8)Requirement already satisfied: docker-pycreds&gt;=0.4.0 in /root/miniconda3/lib/python3.10/site-packages (from wandb) (0.4.0)Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,&lt;7,&gt;=3.19.0 in /root/miniconda3/lib/python3.10/site-packages (from wandb) (4.25.3)Requirement already satisfied: gitpython!=3.1.29,&gt;=1.0.0 in /root/miniconda3/lib/python3.10/site-packages (from wandb) (3.1.44)Requirement already satisfied: sentry-sdk&gt;=2.0.0 in /root/miniconda3/lib/python3.10/site-packages (from wandb) (2.27.0)Requirement already satisfied: pydantic&lt;3 in /root/miniconda3/lib/python3.10/site-packages (from wandb) (2.11.4)Requirement already satisfied: platformdirs in /root/miniconda3/lib/python3.10/site-packages (from wandb) (4.2.2)Requirement already satisfied: setuptools in /root/miniconda3/lib/python3.10/site-packages (from wandb) (65.5.0)Requirement already satisfied: setproctitle in /root/miniconda3/lib/python3.10/site-packages (from wandb) (1.3.6)Requirement already satisfied: six&gt;=1.4.0 in /root/miniconda3/lib/python3.10/site-packages (from docker-pycreds&gt;=0.4.0-&gt;wandb) (1.16.0)Requirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets) (2.6.1)Requirement already satisfied: propcache&gt;=0.2.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets) (0.3.1)Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets) (6.4.3)Requirement already satisfied: yarl&lt;2.0,&gt;=1.17.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets) (1.20.0)Requirement already satisfied: async-timeout&lt;6.0,&gt;=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets) (5.0.1)Requirement already satisfied: aiosignal&gt;=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets) (1.3.2)Requirement already satisfied: attrs&gt;=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets) (23.2.0)Requirement already satisfied: frozenlist&gt;=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets) (1.6.0)Requirement already satisfied: gitdb&lt;5,&gt;=4.0.1 in /root/miniconda3/lib/python3.10/site-packages (from gitpython!=3.1.29,&gt;=1.0.0-&gt;wandb) (4.0.12)Requirement already satisfied: annotated-types&gt;=0.6.0 in /root/miniconda3/lib/python3.10/site-packages (from pydantic&lt;3-&gt;wandb) (0.7.0)Requirement already satisfied: typing-inspection&gt;=0.4.0 in /root/miniconda3/lib/python3.10/site-packages (from pydantic&lt;3-&gt;wandb) (0.4.0)Requirement already satisfied: pydantic-core==2.33.2 in /root/miniconda3/lib/python3.10/site-packages (from pydantic&lt;3-&gt;wandb) (2.33.2)Requirement already satisfied: idna&lt;4,&gt;=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests-&gt;transformers) (3.4)Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests-&gt;transformers) (1.26.13)Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /root/miniconda3/lib/python3.10/site-packages (from requests-&gt;transformers) (2.0.4)Requirement already satisfied: certifi&gt;=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests-&gt;transformers) (2022.12.7)Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch&gt;=2.0.0-&gt;accelerate) (3.3)Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from torch&gt;=2.0.0-&gt;accelerate) (1.12.1)Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from torch&gt;=2.0.0-&gt;accelerate) (2.1.0)Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.10/site-packages (from torch&gt;=2.0.0-&gt;accelerate) (3.1.4)Requirement already satisfied: tzdata&gt;=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas-&gt;datasets) (2025.2)Requirement already satisfied: pytz&gt;=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas-&gt;datasets) (2025.2)Requirement already satisfied: python-dateutil&gt;=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas-&gt;datasets) (2.9.0.post0)Requirement already satisfied: markdown-it-py&gt;=2.2.0 in /root/miniconda3/lib/python3.10/site-packages (from rich-&gt;trl) (3.0.0)Requirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /root/miniconda3/lib/python3.10/site-packages (from rich-&gt;trl) (2.18.0)Requirement already satisfied: smmap&lt;6,&gt;=3.0.1 in /root/miniconda3/lib/python3.10/site-packages (from gitdb&lt;5,&gt;=4.0.1-&gt;gitpython!=3.1.29,&gt;=1.0.0-&gt;wandb) (5.0.2)Requirement already satisfied: mdurl~=0.1 in /root/miniconda3/lib/python3.10/site-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;trl) (0.1.2)Requirement already satisfied: MarkupSafe&gt;=2.0 in /root/miniconda3/lib/python3.10/site-packages (from jinja2-&gt;torch&gt;=2.0.0-&gt;accelerate) (2.1.5)Requirement already satisfied: mpmath&lt;1.4.0,&gt;=1.1.0 in /root/miniconda3/lib/python3.10/site-packages (from sympy-&gt;torch&gt;=2.0.0-&gt;accelerate) (1.3.0)[33mWARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m[33m[0mNote: you may need to restart the kernel to use updated packages.Looking in indexes: http://mirrors.aliyun.com/pypi/simpleRequirement already satisfied: ipywidgets in /root/miniconda3/lib/python3.10/site-packages (8.1.3)Requirement already satisfied: widgetsnbextension in /root/miniconda3/lib/python3.10/site-packages (4.0.11)Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /root/miniconda3/lib/python3.10/site-packages (from ipywidgets) (3.0.11)Requirement already satisfied: ipython&gt;=6.1.0 in /root/miniconda3/lib/python3.10/site-packages (from ipywidgets) (8.25.0)Requirement already satisfied: traitlets&gt;=4.3.1 in /root/miniconda3/lib/python3.10/site-packages (from ipywidgets) (5.14.3)Requirement already satisfied: comm&gt;=0.1.3 in /root/miniconda3/lib/python3.10/site-packages (from ipywidgets) (0.2.2)Requirement already satisfied: pygments&gt;=2.4.0 in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (2.18.0)Requirement already satisfied: jedi&gt;=0.16 in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (0.19.1)Requirement already satisfied: typing-extensions&gt;=4.6 in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (4.12.2)Requirement already satisfied: decorator in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (5.1.1)Requirement already satisfied: stack-data in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (0.6.3)Requirement already satisfied: exceptiongroup in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (1.2.1)Requirement already satisfied: pexpect&gt;4.3 in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (4.9.0)Requirement already satisfied: prompt-toolkit&lt;3.1.0,&gt;=3.0.41 in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (3.0.47)Requirement already satisfied: matplotlib-inline in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (0.1.7)Requirement already satisfied: parso&lt;0.9.0,&gt;=0.8.3 in /root/miniconda3/lib/python3.10/site-packages (from jedi&gt;=0.16-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (0.8.4)Requirement already satisfied: ptyprocess&gt;=0.5 in /root/miniconda3/lib/python3.10/site-packages (from pexpect&gt;4.3-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (0.7.0)Requirement already satisfied: wcwidth in /root/miniconda3/lib/python3.10/site-packages (from prompt-toolkit&lt;3.1.0,&gt;=3.0.41-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (0.2.13)Requirement already satisfied: pure-eval in /root/miniconda3/lib/python3.10/site-packages (from stack-data-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (0.2.2)Requirement already satisfied: asttokens&gt;=2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from stack-data-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (2.4.1)Requirement already satisfied: executing&gt;=1.2.0 in /root/miniconda3/lib/python3.10/site-packages (from stack-data-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (2.0.1)Requirement already satisfied: six&gt;=1.12.0 in /root/miniconda3/lib/python3.10/site-packages (from asttokens&gt;=2.1.0-&gt;stack-data-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (1.16.0)[33mWARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m[33m[0mNote: you may need to restart the kernel to use updated packages.</code></pre><pre><code class="lang-python">import subprocessimport osON_AUTODL_ENV = Trueif ON_AUTODL_ENV:    result = subprocess.run(&#39;bash -c &quot;source /etc/network_turbo &amp;&amp; env | grep proxy&quot;&#39;, shell=True, capture_output=True, text=True)    output = result.stdout    for line in output.splitlines():        if &#39;=&#39; in line:            var, value = line.split(&#39;=&#39;, 1)            os.environ[var] = value</code></pre><pre><code class="lang-python">from huggingface_hub import loginlogin(token=&quot;your keys&quot;)import wandbimport osos.environ[&quot;WANDB_NOTEBOOK_NAME&quot;] = &quot;finetune_gpt2_on_wiki.ipynb&quot;  # 替换为实际文件名wandb.init(project=&quot;finetune_gpt2_on_wiki&quot;)</code></pre><pre><code>[34m[1mwandb[0m: Currently logged in as: [33mgoldandrabbit[0m ([33mgoldandrabbit-g-r[0m) to [32mhttps://api.wandb.ai[0m. Use [1m`wandb login --relogin`[0m to force relogin</code></pre><p>Tracking run with wandb version 0.19.10</p><p>Run data is saved locally in <code>/root/z_notebooks/wandb/run-20250503_102611-0ryscs1u</code></p><p>Syncing run <strong><a href='https://wandb.ai/goldandrabbit-g-r/finetune_gpt2_on_wiki/runs/0ryscs1u' target="_blank">glad-feather-2</a></strong> to <a href='https://wandb.ai/goldandrabbit-g-r/finetune_gpt2_on_wiki' target="_blank">Weights &amp; Biases</a> (<a href='https://wandb.me/developer-guide' target="_blank">docs</a>)<br></p><p>View project at <a href='https://wandb.ai/goldandrabbit-g-r/finetune_gpt2_on_wiki' target="_blank">https://wandb.ai/goldandrabbit-g-r/finetune_gpt2_on_wiki</a></p><p>View run at <a href='https://wandb.ai/goldandrabbit-g-r/finetune_gpt2_on_wiki/runs/0ryscs1u' target="_blank">https://wandb.ai/goldandrabbit-g-r/finetune_gpt2_on_wiki/runs/0ryscs1u</a></p><p><button onClick="this.nextSibling.style.display='block';this.style.display='none';">Display W&amp;B run</button><iframe src='https://wandb.ai/goldandrabbit-g-r/finetune_gpt2_on_wiki/runs/0ryscs1u?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe></p><pre><code class="lang-python">import torchfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArgumentsfrom datasets import load_datasetif torch.backends.mps.is_available():    print(f&quot;torch.backeds.mps.is_available(): &#123;torch.backends.mps.is_available()&#125;&quot;)    device = torch.device(&quot;mps&quot; if torch.backends.mps.is_available() else &quot;cpu&quot;)elif torch.cuda.is_available():    print(f&quot;torch.cuda.is_available(): &#123;torch.cuda.is_available()&#125;&quot;)    device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)dataset = load_dataset(&#39;Self-GRIT/wikitext-2-raw-v1-preprocessed&#39;)</code></pre><pre><code>torch.cuda.is_available(): True</code></pre><h2 id="检查-wiki-数据"><a href="#检查-wiki-数据" class="headerlink" title="检查 wiki 数据"></a>检查 wiki 数据</h2><pre><code class="lang-python">dataset</code></pre><pre><code>DatasetDict(&#123;    test: Dataset(&#123;        features: [&#39;text&#39;],        num_rows: 1835    &#125;)    train: Dataset(&#123;        features: [&#39;text&#39;],        num_rows: 15313    &#125;)    validation: Dataset(&#123;        features: [&#39;text&#39;],        num_rows: 1649    &#125;)&#125;)</code></pre><pre><code class="lang-python">dataset[&#39;train&#39;][&#39;text&#39;][0:10]</code></pre><pre><code>[&#39; Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the &quot; Nameless &quot; , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit &quot; Calamaty Raven &quot; . \n&#39;, &quot; The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game &#39;s opening theme was sung by May &#39;n . \n&quot;, &quot; It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game &#39;s expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \n&quot;, &quot; As with previous Valkyira Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player &#39;s approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game &#39;s completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game &#39;s two main heroines , although they take a very minor role . \n&quot;, &#39; The game \&#39;s battle system , the BliTZ system , is carried over directly from Valkyira Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \&#39; turns . Each character has a field and distance of movement limited by their Action Gauge . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific &quot; Potentials &quot; , skills unique to each character . They are divided into &quot; Personal Potential &quot; , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and &quot; Battle Potentials &quot; , which are grown throughout the game and always grant boons to a character . To learn Battle Potentials , each character has a unique &quot; Masters Table &quot; , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special Abilities that grant them temporary boosts on the battlefield : Kurt can activate &quot; Direct Command &quot; and move around the battlefield without depleting his Action Point gauge , the character Reila can shift into her &quot; Valkyria Form &quot; and become invincible , while Imca can target multiple enemy units with her heavy weapon . \n&#39;, &quot; Troops are divided into five classes : Scouts , Shocktroopers , Engineers , Lancers and Armored Soldier . Troopers can switch classes by changing their assigned weapon . Changing class does not greatly affect the stats gained while in a previous class . With victory in battle , experience points are awarded to the squad , which are distributed into five different attributes shared by the entire squad , a feature differing from early games &#39; method of distributing to different unit types . \n&quot;, &#39; The game takes place during the Second Europan War . Gallian Army Squad 422 , also known as &quot; The Nameless &quot; , are a penal military unit composed of criminals , foreign deserters , and military offenders whose real names are erased from the records and thereon officially referred to by numbers . Ordered by the Gallian military to perform the most dangerous missions that the Regular Army and Militia will not do , they are nevertheless up to the task , exemplified by their motto , Altaha Abilia , meaning &quot; Always Ready . &quot; The three main characters are No.7 Kurt Irving , an army officer falsely accused of treason who wishes to redeem himself ; Ace No.1 Imca , a female Darcsen heavy weapons specialist who seeks revenge against the Valkyria who destroyed her home ; and No.13 Riela Marcellis , a seemingly jinxed young woman who is unknowingly a descendant of the Valkyria . Together with their fellow squad members , these three are tasked to fight against a mysterious Imperial unit known as Calamity Raven , consisting of mostly Darcsen soldiers . \n&#39;, &quot; As the Nameless officially do not exist , the upper echelons of the Gallian Army exploit the concept of plausible deniability in order to send them on missions that would otherwise make Gallia lose face in the war . While at times this works to their advantage , such as a successful incursion into Imperial territory , other orders cause certain members of the 422nd great distress . One such member , Gusurg , becomes so enraged that he abandons his post and defects into the ranks of Calamity Raven , attached to the ideal of Darcsen independence proposed by their leader , Dahau . At the same time , elements within Gallian Army Command move to erase the Nameless in order to protect their own interests . Hounded by both allies and enemies , and combined with the presence of a traitor within their ranks , the 422nd desperately move to keep themselves alive while at the same time fight to help the Gallian war effort . This continues until the Nameless &#39;s commanding officer , Ramsey Crowe , who had been kept under house arrest , is escorted to the capital city of Randgriz in order to present evidence exonerating the weary soldiers and expose the real traitor , the Gallian General that had accused Kurt of Treason . \n&quot;, &quot; Partly due to these events , and partly due to the major losses in manpower Gallia suffers towards the end of the war with the Empire , the Nameless are offered a formal position as a squad in the Gallian Army rather than serve as an anonymous shadow force . This is short @-@ lived , however , as following Maximilian &#39;s defeat , Dahau and Calamity Raven move to activate an ancient Valkyrian super weapon within the Empire , kept secret by their benefactor . Without the support of Maximilian or the chance to prove themselves in the war with Gallia , it is Dahau &#39;s last trump card in creating a new Darcsen nation . As an armed Gallian force invading the Empire just following the two nations &#39; cease @-@ fire would certainly wreck their newfound peace , Kurt decides to once again make his squad the Nameless , asking Crowe to list himself and all under his command as killed @-@ in @-@ action . Now owing allegiance to none other than themselves , the 422nd confronts Dahau and destroys the Valkyrian weapon . Each member then goes their separate ways in order to begin their lives anew . \n&quot;, &#39; Concept work for Valkyria Chronicles III began after development finished on Valkyria Chronicles II in early 2010 , with full development beginning shortly after this . The director of Valkyria Chronicles II , Takeshi Ozawa , returned to that role for Valkyria Chronicles III . Development work took approximately one year . After the release of Valkyria Chronicles II , the staff took a look at both the popular response for the game and what they wanted to do next for the series . Like its predecessor , Valkyria Chronicles III was developed for PlayStation Portable : this was due to the team wanting to refine the mechanics created for Valkyria Chronicles II , and they had not come up with the &quot; revolutionary &quot; idea that would warrant a new entry for the PlayStation 3 . Speaking in an interview , it was stated that the development team considered Valkyria Chronicles III to be the series \&#39; first true sequel : while Valkyria Chronicles II had required a large amount of trial and error during development due to the platform move , the third game gave them a chance to improve upon the best parts of Valkyria Chronicles II due to being on the same platform . In addition to Sega staff from the previous games , development work was also handled by Media.Vision. The original scenario was written Kazuki Yamanobe , while the script was written by Hiroyuki Fujii , Koichi Majima , Kishiko Miyagi , Seiki Nagakawa and Takayuki Shouji . Its story was darker and more somber than that of its predecessor . \n&#39;]</code></pre><pre><code class="lang-python">tokenizer = AutoTokenizer.from_pretrained(&#39;gpt2&#39;)model = AutoModelForCausalLM.from_pretrained(&#39;gpt2&#39;).to(device)tokenizer.pad_token = tokenizer.eos_tokendef tokenize_function(examples):    inputs = tokenizer(        examples[&#39;text&#39;],        truncation=True,        padding=&#39;max_length&#39;,        max_length=128    )    inputs[&#39;labels&#39;] = inputs[&#39;input_ids&#39;].copy()    return inputs</code></pre><pre><code class="lang-python">print(tokenizer)</code></pre><pre><code>GPT2TokenizerFast(name_or_path=&#39;gpt2&#39;, vocab_size=50257, model_max_length=1024, is_fast=True, padding_side=&#39;right&#39;, truncation_side=&#39;right&#39;, special_tokens=&#123;&#39;bos_token&#39;: &#39;&lt;|endoftext|&gt;&#39;, &#39;eos_token&#39;: &#39;&lt;|endoftext|&gt;&#39;, &#39;unk_token&#39;: &#39;&lt;|endoftext|&gt;&#39;, &#39;pad_token&#39;: &#39;&lt;|endoftext|&gt;&#39;&#125;, clean_up_tokenization_spaces=False, added_tokens_decoder=&#123;    50256: AddedToken(&quot;&lt;|endoftext|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),&#125;)</code></pre><pre><code class="lang-python">tokenizer.pad_token</code></pre><pre><code>&#39;&lt;|endoftext|&gt;&#39;</code></pre><h2 id="检查-pretrained-模型"><a href="#检查-pretrained-模型" class="headerlink" title="检查 pretrained 模型"></a>检查 pretrained 模型</h2><pre><code class="lang-python">model</code></pre><pre><code>GPT2LMHeadModel(  (transformer): GPT2Model(    (wte): Embedding(50257, 768)    (wpe): Embedding(1024, 768)    (drop): Dropout(p=0.1, inplace=False)    (h): ModuleList(      (0-11): 12 x GPT2Block(        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)        (attn): GPT2Attention(          (c_attn): Conv1D(nf=2304, nx=768)          (c_proj): Conv1D(nf=768, nx=768)          (attn_dropout): Dropout(p=0.1, inplace=False)          (resid_dropout): Dropout(p=0.1, inplace=False)        )        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)        (mlp): GPT2MLP(          (c_fc): Conv1D(nf=3072, nx=768)          (c_proj): Conv1D(nf=768, nx=3072)          (act): NewGELUActivation()          (dropout): Dropout(p=0.1, inplace=False)        )      )    )    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)  )  (lm_head): Linear(in_features=768, out_features=50257, bias=False))</code></pre><pre><code class="lang-python"># 计算总参数量total_params = sum(p.numel() for p in model.parameters())print(f&quot;总参数量：&#123;total_params / 1e6:.1f&#125;M&quot;)# 计算可训练参数量（若部分层被冻结）trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)print(f&quot;可训练参数量：&#123;trainable_params / 1e6:.1f&#125;M&quot;)</code></pre><pre><code>总参数量：124.4M可训练参数量：124.4M</code></pre><pre><code class="lang-python">tokenized_datasets = dataset.map(tokenize_function, batched=True)</code></pre><pre><code>Map:   0%|          | 0/15313 [00:00&lt;?, ? examples/s]</code></pre><pre><code class="lang-python">tokenized_datasets[&#39;train&#39;][0][&#39;text&#39;]</code></pre><pre><code>&#39; Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the &quot; Nameless &quot; , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit &quot; Calamaty Raven &quot; . \n&#39;</code></pre><pre><code class="lang-python">tokenized_datasets[&#39;train&#39;][0][&#39;input_ids&#39;]</code></pre><pre><code>[2311, 73, 13090, 645, 569, 18354, 7496, 513, 1058, 791, 47398, 17740, 357, 4960, 1058, 10545, 230, 99, 161, 254, 112, 5641, 44444, 9202, 25084, 24440, 12675, 11839, 18, 837, 6578, 764, 569, 18354, 7496, 286, 262, 30193, 513, 1267, 837, 8811, 6412, 284, 355, 569, 18354, 7496, 17740, 6711, 2354, 2869, 837, 318, 257, 16106, 2597, 2488, 12, 31, 2712, 2008, 983, 4166, 416, 29490, 290, 6343, 13, 44206, 329, 262, 14047, 44685, 764, 28728, 287, 3269, 2813, 287, 2869, 837, 340, 318, 262, 2368, 983, 287, 262, 569, 18354, 7496, 2168, 764, 12645, 278, 262, 976, 21748, 286, 16106, 290, 1103, 2488, 12, 31, 640, 11327, 355, 663, 27677, 837, 262, 1621, 4539, 10730, 284, 262, 717, 983, 290, 5679, 262, 366, 17871, 5321, 366, 837]</code></pre><h2 id="微调模型"><a href="#微调模型" class="headerlink" title="微调模型"></a>微调模型</h2><pre><code class="lang-python">exp_name = &#39;zxc_ft_gpt2_on_wikitext2&#39;if ON_AUTODL_ENV:    output_dir = f&quot;/root/autodl-tmp/&#123;exp_name&#125;&quot;else:    output_dir = f&quot;/Users/gold/repos/z_notebooks/tmp_file/&#123;exp_name&#125;&quot;print(f&quot;output_dir: &#123;output_dir&#125;&quot;)trainer = Trainer(    model=model,    args=TrainingArguments(        run_name=exp_name,        output_dir=output_dir,        eval_strategy=&#39;epoch&#39;,        num_train_epochs=1,        per_device_train_batch_size=8,        per_device_eval_batch_size=8,        warmup_steps=500,        weight_decay=0.01,        save_strategy=&quot;epoch&quot;,        logging_dir=output_dir+&#39;/log&#39;    ),    train_dataset=tokenized_datasets[&#39;train&#39;],    eval_dataset=tokenized_datasets[&#39;validation&#39;],)</code></pre><pre><code>output_dir: /root/autodl-tmp/zxc_ft_gpt2_on_wikitext2</code></pre><pre><code class="lang-python">trainer.train()</code></pre><pre><code>`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.&lt;div&gt;  &lt;progress value=&#39;1915&#39; max=&#39;1915&#39; style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt;  [1915/1915 01:40, Epoch 1/1]&lt;/div&gt;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;</code></pre><p>  <thead> <tr style="text-align: left;">      <th>Epoch</th>      <th>Training Loss</th>      <th>Validation Loss</th>    </tr>  </thead><br>  <tbody>    <tr>      <td>1</td>      <td>2.915400</td>      <td>2.805183</td>    </tr>  </tbody><br>&lt;/table&gt;<p></p><pre><code>TrainOutput(global_step=1915, training_loss=3.0091578232090406, metrics=&#123;&#39;train_runtime&#39;: 101.1733, &#39;train_samples_per_second&#39;: 151.354, &#39;train_steps_per_second&#39;: 18.928, &#39;total_flos&#39;: 1000291221504000.0, &#39;train_loss&#39;: 3.0091578232090406, &#39;epoch&#39;: 1.0&#125;)</code></pre><h2 id="保存和推送模型"><a href="#保存和推送模型" class="headerlink" title="保存和推送模型"></a>保存和推送模型</h2><pre><code class="lang-python">model_output_dir = output_dir + &#39;/model&#39;# 把模型保存到指定新目录print(f&#39;model_output_dir:&#39;, model_output_dir)</code></pre><pre><code>model_output_dir: /root/autodl-tmp/zxc_ft_gpt2_on_wikitext2/model</code></pre><pre><code class="lang-python">model.save_pretrained(model_output_dir)     # 存放 config.json generation_config.json 和 model.safetensorstokenizer.save_pretrained(model_output_dir) # 存放 merges.txt special_token_map.json tokenizer_config.json tokenizer.json vocab.json</code></pre><pre><code>(&#39;/root/autodl-tmp/zxc_ft_gpt2_on_wikitext2/model/tokenizer_config.json&#39;, &#39;/root/autodl-tmp/zxc_ft_gpt2_on_wikitext2/model/special_tokens_map.json&#39;, &#39;/root/autodl-tmp/zxc_ft_gpt2_on_wikitext2/model/vocab.json&#39;, &#39;/root/autodl-tmp/zxc_ft_gpt2_on_wikitext2/model/merges.txt&#39;, &#39;/root/autodl-tmp/zxc_ft_gpt2_on_wikitext2/model/added_tokens.json&#39;, &#39;/root/autodl-tmp/zxc_ft_gpt2_on_wikitext2/model/tokenizer.json&#39;)</code></pre><p>推送 model 和 tokenizer 到 hub, 参数指定 repo 的名字</p><pre><code class="lang-python">push_repo_id = exp_namemodel.push_to_hub(push_repo_id)tokenizer.push_to_hub(push_repo_id)</code></pre><pre><code>model.safetensors:   0%|          | 0.00/498M [00:00&lt;?, ?B/s]README.md:   0%|          | 0.00/5.17k [00:00&lt;?, ?B/s]CommitInfo(commit_url=&#39;https://huggingface.co/goldandrabbit/zxc_ft_gpt2_on_wikitext2/commit/6e5baf535a881c04c5a8d31f732e2d5264c2a3e6&#39;, commit_message=&#39;Upload tokenizer&#39;, commit_description=&#39;&#39;, oid=&#39;6e5baf535a881c04c5a8d31f732e2d5264c2a3e6&#39;, pr_url=None, repo_url=RepoUrl(&#39;https://huggingface.co/goldandrabbit/zxc_ft_gpt2_on_wikitext2&#39;, endpoint=&#39;https://huggingface.co&#39;, repo_type=&#39;model&#39;, repo_id=&#39;goldandrabbit/zxc_ft_gpt2_on_wikitext2&#39;), pr_revision=None, pr_num=None)</code></pre><p>测试本地 inference 流程, 先读取本地的模型</p><pre><code class="lang-python">local_model_path = model_output_dirtokenizer = AutoTokenizer.from_pretrained(local_model_path)model     = AutoModelForCausalLM.from_pretrained(local_model_path)</code></pre><pre><code class="lang-python">model</code></pre><pre><code>GPT2LMHeadModel(  (transformer): GPT2Model(    (wte): Embedding(50257, 768)    (wpe): Embedding(1024, 768)    (drop): Dropout(p=0.1, inplace=False)    (h): ModuleList(      (0-11): 12 x GPT2Block(        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)        (attn): GPT2Attention(          (c_attn): Conv1D(nf=2304, nx=768)          (c_proj): Conv1D(nf=768, nx=768)          (attn_dropout): Dropout(p=0.1, inplace=False)          (resid_dropout): Dropout(p=0.1, inplace=False)        )        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)        (mlp): GPT2MLP(          (c_fc): Conv1D(nf=3072, nx=768)          (c_proj): Conv1D(nf=768, nx=3072)          (act): NewGELUActivation()          (dropout): Dropout(p=0.1, inplace=False)        )      )    )    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)  )  (lm_head): Linear(in_features=768, out_features=50257, bias=False))</code></pre><pre><code class="lang-python">query_str = &quot;Can you tell me a story&quot;prompt = tokenizer(query_str, return_tensors=&#39;pt&#39;)</code></pre><pre><code class="lang-python">prompt</code></pre><pre><code>&#123;&#39;input_ids&#39;: tensor([[6090,  345, 1560,  502,  257, 1621]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1]])&#125;</code></pre><pre><code class="lang-python">outputs = model.generate(    **prompt,    max_length=200,    num_return_sequences=1,    pad_token_id=tokenizer.eos_token_id,    repetition_penalty=1.5,    # 对重复 token 施加惩罚（&gt;1 生效）    temperature=0.9,           # 降低概率分布尖锐度（0.7~1.0 平衡多样性与连贯性）    top_k=50,                  # 仅从前50个高概率 token 中采样    top_p=0.95,                # 从累积概率达95%的 token 集合中采样    do_sample=True             # 启用采样模式（禁用贪婪搜索）)</code></pre><pre><code class="lang-python">generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)</code></pre><pre><code class="lang-python">generated_text</code></pre><pre><code>&#39;Can you tell me a story that I did not already know ? \n&#39;</code></pre><h2 id="测试-load-from-hf-hub-的模型-inference"><a href="#测试-load-from-hf-hub-的模型-inference" class="headerlink" title="测试 load from hf_hub 的模型 inference"></a>测试 load from hf_hub 的模型 inference</h2><pre><code class="lang-python">hub_path = f&quot;goldandrabbit/&#123;exp_name&#125;&quot;tokenizer = AutoTokenizer.from_pretrained(hub_path)model = AutoModelForCausalLM.from_pretrained(hub_path)</code></pre><pre><code>tokenizer_config.json:   0%|          | 0.00/507 [00:00&lt;?, ?B/s]vocab.json:   0%|          | 0.00/798k [00:00&lt;?, ?B/s]merges.txt:   0%|          | 0.00/456k [00:00&lt;?, ?B/s]tokenizer.json:   0%|          | 0.00/3.56M [00:00&lt;?, ?B/s]special_tokens_map.json:   0%|          | 0.00/131 [00:00&lt;?, ?B/s]config.json:   0%|          | 0.00/880 [00:00&lt;?, ?B/s]model.safetensors:   0%|          | 0.00/498M [00:00&lt;?, ?B/s]generation_config.json:   0%|          | 0.00/119 [00:00&lt;?, ?B/s]</code></pre><pre><code class="lang-python">another_query_str = &quot;Can you tell me a story&quot;prompt = tokenizer(another_query_str, return_tensors=&#39;pt&#39;)</code></pre><pre><code class="lang-python">outputs = model.generate(    **prompt,    max_length=200,    num_return_sequences=1,    pad_token_id=tokenizer.eos_token_id,    repetition_penalty=1.5,    # 对重复 token 施加惩罚（&gt;1 生效）    temperature=0.9,           # 降低概率分布尖锐度（0.7~1.0 平衡多样性与连贯性）    top_k=50,                  # 仅从前50个高概率 token 中采样    top_p=0.95,                # 从累积概率达95%的 token 集合中采样    do_sample=True             # 启用采样模式（禁用贪婪搜索）)</code></pre><pre><code class="lang-python">generated_text = tokenizer.decode(    outputs[0],    skip_special_tokens=True)</code></pre><pre><code class="lang-python">print(generated_text)</code></pre><pre><code>Can you tell me a story about being kicked out of football by two people who were clearly in love for one another and there was no reason why they should be upset . &quot; </code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. <a href="https://medium.com/@prashanth.ramanathan/fine-tuning-a-pre-trained-gpt-2-model-and-performing-inference-a-hands-on-guide-57c097a3b810">https://medium.com/@prashanth.ramanathan/fine-tuning-a-pre-trained-gpt-2-model-and-performing-inference-a-hands-on-guide-57c097a3b810</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;finetune-GPT2-on-wiki-text&quot;&gt;&lt;a href=&quot;#finetune-GPT2-on-wiki-text&quot; class=&quot;headerlink&quot; title=&quot;finetune GPT2 on wiki-text&quot;&gt;&lt;/a&gt;finetune</summary>
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>Finetune bert using Trainer v.s. Pytorch train loop</title>
    <link href="http://example.com/2024/10/01/Finetune%20bert%20using%20Trainer%20v.s.%20Pytorch%20train%20loop/"/>
    <id>http://example.com/2024/10/01/Finetune%20bert%20using%20Trainer%20v.s.%20Pytorch%20train%20loop/</id>
    <published>2024-10-01T12:43:00.000Z</published>
    <updated>2025-05-05T11:45:09.730Z</updated>
    
    <content type="html"><![CDATA[<h2 id="finetune-bert-using-Trainer-v-s-Pytorch-Train-Loop"><a href="#finetune-bert-using-Trainer-v-s-Pytorch-Train-Loop" class="headerlink" title="finetune bert using Trainer v.s. Pytorch Train Loop"></a>finetune bert using Trainer v.s. Pytorch Train Loop</h2><p>1.采用 huggingface trainsformer 在文本多类别分类任务上微调 bert 模型<br>2.对比使用封装好的 Trainer v.s. 原生 Pytorch Training Loop 的两类实现方法  </p><h2 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h2><pre><code class="lang-python">%pip install transformers datasets huggingface_hub accelerate bitsandbytes%pip install trl peft loralib wandb evaluate%pip install ipywidgets widgetsnbextension</code></pre><pre><code>Looking in indexes: http://mirrors.aliyun.com/pypi/simpleRequirement already satisfied: transformers in /root/miniconda3/lib/python3.10/site-packages (4.51.3)Requirement already satisfied: datasets in /root/miniconda3/lib/python3.10/site-packages (3.5.1)Requirement already satisfied: huggingface_hub in /root/miniconda3/lib/python3.10/site-packages (0.30.2)Requirement already satisfied: accelerate in /root/miniconda3/lib/python3.10/site-packages (1.6.0)Requirement already satisfied: bitsandbytes in /root/miniconda3/lib/python3.10/site-packages (0.45.5)Requirement already satisfied: safetensors&gt;=0.4.3 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (0.5.3)Requirement already satisfied: numpy&gt;=1.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (1.26.4)Requirement already satisfied: requests in /root/miniconda3/lib/python3.10/site-packages (from transformers) (2.32.3)Requirement already satisfied: tqdm&gt;=4.27 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (4.67.1)Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from transformers) (3.14.0)Requirement already satisfied: pyyaml&gt;=5.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (6.0.1)Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (2024.11.6)Requirement already satisfied: packaging&gt;=20.0 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (24.1)Requirement already satisfied: tokenizers&lt;0.22,&gt;=0.21 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (0.21.1)Requirement already satisfied: multiprocess&lt;0.70.17 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.70.16)Requirement already satisfied: dill&lt;0.3.9,&gt;=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.3.8)Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.5.0)Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.11.18)Requirement already satisfied: pyarrow&gt;=15.0.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (20.0.0)Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (from datasets) (2.2.3)Requirement already satisfied: fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (2024.6.0)Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)Requirement already satisfied: torch&gt;=2.0.0 in /root/miniconda3/lib/python3.10/site-packages (from accelerate) (2.1.2+cu118)Requirement already satisfied: psutil in /root/miniconda3/lib/python3.10/site-packages (from accelerate) (5.9.8)Requirement already satisfied: propcache&gt;=0.2.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets) (0.3.1)Requirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets) (2.6.1)Requirement already satisfied: attrs&gt;=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets) (23.2.0)Requirement already satisfied: aiosignal&gt;=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets) (1.3.2)Requirement already satisfied: frozenlist&gt;=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets) (1.6.0)Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets) (6.4.3)Requirement already satisfied: yarl&lt;2.0,&gt;=1.17.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets) (1.20.0)Requirement already satisfied: async-timeout&lt;6.0,&gt;=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets) (5.0.1)Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests-&gt;transformers) (1.26.13)Requirement already satisfied: certifi&gt;=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests-&gt;transformers) (2022.12.7)Requirement already satisfied: idna&lt;4,&gt;=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests-&gt;transformers) (3.4)Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /root/miniconda3/lib/python3.10/site-packages (from requests-&gt;transformers) (2.0.4)Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from torch&gt;=2.0.0-&gt;accelerate) (2.1.0)Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from torch&gt;=2.0.0-&gt;accelerate) (1.12.1)Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch&gt;=2.0.0-&gt;accelerate) (3.3)Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.10/site-packages (from torch&gt;=2.0.0-&gt;accelerate) (3.1.4)Requirement already satisfied: pytz&gt;=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas-&gt;datasets) (2025.2)Requirement already satisfied: python-dateutil&gt;=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas-&gt;datasets) (2.9.0.post0)Requirement already satisfied: tzdata&gt;=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas-&gt;datasets) (2025.2)Requirement already satisfied: six&gt;=1.5 in /root/miniconda3/lib/python3.10/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;datasets) (1.16.0)Requirement already satisfied: MarkupSafe&gt;=2.0 in /root/miniconda3/lib/python3.10/site-packages (from jinja2-&gt;torch&gt;=2.0.0-&gt;accelerate) (2.1.5)Requirement already satisfied: mpmath&lt;1.4.0,&gt;=1.1.0 in /root/miniconda3/lib/python3.10/site-packages (from sympy-&gt;torch&gt;=2.0.0-&gt;accelerate) (1.3.0)[33mWARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m[33m[0mNote: you may need to restart the kernel to use updated packages.Looking in indexes: http://mirrors.aliyun.com/pypi/simpleRequirement already satisfied: trl in /root/miniconda3/lib/python3.10/site-packages (0.17.0)Requirement already satisfied: peft in /root/miniconda3/lib/python3.10/site-packages (0.15.2)Requirement already satisfied: loralib in /root/miniconda3/lib/python3.10/site-packages (0.1.2)Requirement already satisfied: wandb in /root/miniconda3/lib/python3.10/site-packages (0.19.10)Requirement already satisfied: evaluate in /root/miniconda3/lib/python3.10/site-packages (0.4.3)Requirement already satisfied: transformers&gt;=4.46.0 in /root/miniconda3/lib/python3.10/site-packages (from trl) (4.51.3)Requirement already satisfied: datasets&gt;=3.0.0 in /root/miniconda3/lib/python3.10/site-packages (from trl) (3.5.1)Requirement already satisfied: accelerate&gt;=0.34.0 in /root/miniconda3/lib/python3.10/site-packages (from trl) (1.6.0)Requirement already satisfied: rich in /root/miniconda3/lib/python3.10/site-packages (from trl) (14.0.0)Requirement already satisfied: huggingface_hub&gt;=0.25.0 in /root/miniconda3/lib/python3.10/site-packages (from peft) (0.30.2)Requirement already satisfied: packaging&gt;=20.0 in /root/miniconda3/lib/python3.10/site-packages (from peft) (24.1)Requirement already satisfied: numpy&gt;=1.17 in /root/miniconda3/lib/python3.10/site-packages (from peft) (1.26.4)Requirement already satisfied: torch&gt;=1.13.0 in /root/miniconda3/lib/python3.10/site-packages (from peft) (2.1.2+cu118)Requirement already satisfied: safetensors in /root/miniconda3/lib/python3.10/site-packages (from peft) (0.5.3)Requirement already satisfied: pyyaml in /root/miniconda3/lib/python3.10/site-packages (from peft) (6.0.1)Requirement already satisfied: psutil in /root/miniconda3/lib/python3.10/site-packages (from peft) (5.9.8)Requirement already satisfied: tqdm in /root/miniconda3/lib/python3.10/site-packages (from peft) (4.67.1)Requirement already satisfied: docker-pycreds&gt;=0.4.0 in /root/miniconda3/lib/python3.10/site-packages (from wandb) (0.4.0)Requirement already satisfied: pydantic&lt;3 in /root/miniconda3/lib/python3.10/site-packages (from wandb) (2.11.4)Requirement already satisfied: setproctitle in /root/miniconda3/lib/python3.10/site-packages (from wandb) (1.3.6)Requirement already satisfied: platformdirs in /root/miniconda3/lib/python3.10/site-packages (from wandb) (4.2.2)Requirement already satisfied: gitpython!=3.1.29,&gt;=1.0.0 in /root/miniconda3/lib/python3.10/site-packages (from wandb) (3.1.44)Requirement already satisfied: sentry-sdk&gt;=2.0.0 in /root/miniconda3/lib/python3.10/site-packages (from wandb) (2.27.0)Requirement already satisfied: setuptools in /root/miniconda3/lib/python3.10/site-packages (from wandb) (65.5.0)Requirement already satisfied: click!=8.0.0,&gt;=7.1 in /root/miniconda3/lib/python3.10/site-packages (from wandb) (8.1.8)Requirement already satisfied: requests&lt;3,&gt;=2.0.0 in /root/miniconda3/lib/python3.10/site-packages (from wandb) (2.32.3)Requirement already satisfied: typing-extensions&lt;5,&gt;=4.4 in /root/miniconda3/lib/python3.10/site-packages (from wandb) (4.12.2)Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,&lt;7,&gt;=3.19.0 in /root/miniconda3/lib/python3.10/site-packages (from wandb) (4.25.3)Requirement already satisfied: dill in /root/miniconda3/lib/python3.10/site-packages (from evaluate) (0.3.8)Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from evaluate) (0.70.16)Requirement already satisfied: fsspec[http]&gt;=2021.05.0 in /root/miniconda3/lib/python3.10/site-packages (from evaluate) (2024.6.0)Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from evaluate) (3.5.0)Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (from evaluate) (2.2.3)Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from datasets&gt;=3.0.0-&gt;trl) (3.14.0)Requirement already satisfied: pyarrow&gt;=15.0.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets&gt;=3.0.0-&gt;trl) (20.0.0)Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets&gt;=3.0.0-&gt;trl) (3.11.18)Requirement already satisfied: six&gt;=1.4.0 in /root/miniconda3/lib/python3.10/site-packages (from docker-pycreds&gt;=0.4.0-&gt;wandb) (1.16.0)Requirement already satisfied: gitdb&lt;5,&gt;=4.0.1 in /root/miniconda3/lib/python3.10/site-packages (from gitpython!=3.1.29,&gt;=1.0.0-&gt;wandb) (4.0.12)Requirement already satisfied: annotated-types&gt;=0.6.0 in /root/miniconda3/lib/python3.10/site-packages (from pydantic&lt;3-&gt;wandb) (0.7.0)Requirement already satisfied: typing-inspection&gt;=0.4.0 in /root/miniconda3/lib/python3.10/site-packages (from pydantic&lt;3-&gt;wandb) (0.4.0)Requirement already satisfied: pydantic-core==2.33.2 in /root/miniconda3/lib/python3.10/site-packages (from pydantic&lt;3-&gt;wandb) (2.33.2)Requirement already satisfied: idna&lt;4,&gt;=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (3.4)Requirement already satisfied: certifi&gt;=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (2022.12.7)Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (1.26.13)Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /root/miniconda3/lib/python3.10/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (2.0.4)Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from torch&gt;=1.13.0-&gt;peft) (1.12.1)Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from torch&gt;=1.13.0-&gt;peft) (2.1.0)Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.10/site-packages (from torch&gt;=1.13.0-&gt;peft) (3.1.4)Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch&gt;=1.13.0-&gt;peft) (3.3)Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers&gt;=4.46.0-&gt;trl) (2024.11.6)Requirement already satisfied: tokenizers&lt;0.22,&gt;=0.21 in /root/miniconda3/lib/python3.10/site-packages (from transformers&gt;=4.46.0-&gt;trl) (0.21.1)Requirement already satisfied: pytz&gt;=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas-&gt;evaluate) (2025.2)Requirement already satisfied: tzdata&gt;=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas-&gt;evaluate) (2025.2)Requirement already satisfied: python-dateutil&gt;=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas-&gt;evaluate) (2.9.0.post0)Requirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /root/miniconda3/lib/python3.10/site-packages (from rich-&gt;trl) (2.18.0)Requirement already satisfied: markdown-it-py&gt;=2.2.0 in /root/miniconda3/lib/python3.10/site-packages (from rich-&gt;trl) (3.0.0)Requirement already satisfied: frozenlist&gt;=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets&gt;=3.0.0-&gt;trl) (1.6.0)Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets&gt;=3.0.0-&gt;trl) (6.4.3)Requirement already satisfied: yarl&lt;2.0,&gt;=1.17.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets&gt;=3.0.0-&gt;trl) (1.20.0)Requirement already satisfied: propcache&gt;=0.2.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets&gt;=3.0.0-&gt;trl) (0.3.1)Requirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets&gt;=3.0.0-&gt;trl) (2.6.1)Requirement already satisfied: attrs&gt;=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets&gt;=3.0.0-&gt;trl) (23.2.0)Requirement already satisfied: aiosignal&gt;=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets&gt;=3.0.0-&gt;trl) (1.3.2)Requirement already satisfied: async-timeout&lt;6.0,&gt;=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp-&gt;datasets&gt;=3.0.0-&gt;trl) (5.0.1)Requirement already satisfied: smmap&lt;6,&gt;=3.0.1 in /root/miniconda3/lib/python3.10/site-packages (from gitdb&lt;5,&gt;=4.0.1-&gt;gitpython!=3.1.29,&gt;=1.0.0-&gt;wandb) (5.0.2)Requirement already satisfied: mdurl~=0.1 in /root/miniconda3/lib/python3.10/site-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;trl) (0.1.2)Requirement already satisfied: MarkupSafe&gt;=2.0 in /root/miniconda3/lib/python3.10/site-packages (from jinja2-&gt;torch&gt;=1.13.0-&gt;peft) (2.1.5)Requirement already satisfied: mpmath&lt;1.4.0,&gt;=1.1.0 in /root/miniconda3/lib/python3.10/site-packages (from sympy-&gt;torch&gt;=1.13.0-&gt;peft) (1.3.0)[33mWARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m[33m[0mNote: you may need to restart the kernel to use updated packages.Looking in indexes: http://mirrors.aliyun.com/pypi/simpleRequirement already satisfied: ipywidgets in /root/miniconda3/lib/python3.10/site-packages (8.1.3)Requirement already satisfied: widgetsnbextension in /root/miniconda3/lib/python3.10/site-packages (4.0.11)Requirement already satisfied: traitlets&gt;=4.3.1 in /root/miniconda3/lib/python3.10/site-packages (from ipywidgets) (5.14.3)Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /root/miniconda3/lib/python3.10/site-packages (from ipywidgets) (3.0.11)Requirement already satisfied: comm&gt;=0.1.3 in /root/miniconda3/lib/python3.10/site-packages (from ipywidgets) (0.2.2)Requirement already satisfied: ipython&gt;=6.1.0 in /root/miniconda3/lib/python3.10/site-packages (from ipywidgets) (8.25.0)Requirement already satisfied: decorator in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (5.1.1)Requirement already satisfied: pexpect&gt;4.3 in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (4.9.0)Requirement already satisfied: pygments&gt;=2.4.0 in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (2.18.0)Requirement already satisfied: typing-extensions&gt;=4.6 in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (4.12.2)Requirement already satisfied: prompt-toolkit&lt;3.1.0,&gt;=3.0.41 in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (3.0.47)Requirement already satisfied: matplotlib-inline in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (0.1.7)Requirement already satisfied: jedi&gt;=0.16 in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (0.19.1)Requirement already satisfied: stack-data in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (0.6.3)Requirement already satisfied: exceptiongroup in /root/miniconda3/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (1.2.1)Requirement already satisfied: parso&lt;0.9.0,&gt;=0.8.3 in /root/miniconda3/lib/python3.10/site-packages (from jedi&gt;=0.16-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (0.8.4)Requirement already satisfied: ptyprocess&gt;=0.5 in /root/miniconda3/lib/python3.10/site-packages (from pexpect&gt;4.3-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (0.7.0)Requirement already satisfied: wcwidth in /root/miniconda3/lib/python3.10/site-packages (from prompt-toolkit&lt;3.1.0,&gt;=3.0.41-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (0.2.13)Requirement already satisfied: executing&gt;=1.2.0 in /root/miniconda3/lib/python3.10/site-packages (from stack-data-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (2.0.1)Requirement already satisfied: pure-eval in /root/miniconda3/lib/python3.10/site-packages (from stack-data-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (0.2.2)Requirement already satisfied: asttokens&gt;=2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from stack-data-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (2.4.1)Requirement already satisfied: six&gt;=1.12.0 in /root/miniconda3/lib/python3.10/site-packages (from asttokens&gt;=2.1.0-&gt;stack-data-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (1.16.0)[33mWARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m[33m[0mNote: you may need to restart the kernel to use updated packages.</code></pre><pre><code class="lang-python">import subprocessimport wandbimport osON_AUTODL_ENV = Trueif ON_AUTODL_ENV:    result = subprocess.run(&#39;bash -c &quot;source /etc/network_turbo &amp;&amp; env | grep proxy&quot;&#39;, shell=True, capture_output=True, text=True)    output = result.stdout    for line in output.splitlines():        if &#39;=&#39; in line:            var, value = line.split(&#39;=&#39;, 1)            os.environ[var] = valueprint(f&quot;ON_AUTODL_ENV: &#123;ON_AUTODL_ENV&#125;&quot;)TRAINER_EXP_NAME = &quot;finetune_bert_using_Trainer_vs_pytorch_train_loop&quot;RUN_NOTEBOOK_NAME = &quot;finetune_bert_using_Trainer_vs_pytorch_train_loop.ipynb&quot;print(f&#39;TRAINER_EXP_NAME: &#123;TRAINER_EXP_NAME&#125;&#39;)print(f&#39;RUN_NOTEBOOK_NAME: &#123;RUN_NOTEBOOK_NAME&#125;&#39;)from huggingface_hub import loginimport timeint_timestamp = str(int(time.time()))print(f&#39;int_timestamp: &#123;int_timestamp&#125;&#39;)login(token=&quot;your keys&quot;)os.environ[&quot;WANDB_NOTEBOOK_NAME&quot;] = f&quot;&#123;RUN_NOTEBOOK_NAME&#125;_&#123;int_timestamp&#125;.ipynb&quot;  # 替换为实际文件名wandb.init(project=f&quot;&#123;RUN_NOTEBOOK_NAME&#125;&quot;)</code></pre><pre><code>ON_AUTODL_ENV: TrueTRAINER_EXP_NAME: finetune_bert_using_Trainer_vs_pytorch_train_loopRUN_NOTEBOOK_NAME: finetune_bert_using_Trainer_vs_pytorch_train_loop.ipynbint_timestamp: 1746370141[34m[1mwandb[0m: [33mWARNING[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn&#39;t find finetune_bert_using_Trainer_vs_pytorch_train_loop.ipynb_1746370141.ipynb.[34m[1mwandb[0m: Currently logged in as: [33mgoldandrabbit[0m ([33mgoldandrabbit-g-r[0m) to [32mhttps://api.wandb.ai[0m. Use [1m`wandb login --relogin`[0m to force relogin</code></pre><p>Tracking run with wandb version 0.19.10</p><p>Run data is saved locally in <code>/root/z_notebooks/wandb/run-20250504_224903-ig9gqolf</code></p><p>Syncing run <strong><a href='https://wandb.ai/goldandrabbit-g-r/finetune_bert_using_Trainer_vs_pytorch_train_loop.ipynb/runs/ig9gqolf' target="_blank">stellar-tauntaun-7</a></strong> to <a href='https://wandb.ai/goldandrabbit-g-r/finetune_bert_using_Trainer_vs_pytorch_train_loop.ipynb' target="_blank">Weights &amp; Biases</a> (<a href='https://wandb.me/developer-guide' target="_blank">docs</a>)<br></p><p>View project at <a href='https://wandb.ai/goldandrabbit-g-r/finetune_bert_using_Trainer_vs_pytorch_train_loop.ipynb' target="_blank">https://wandb.ai/goldandrabbit-g-r/finetune_bert_using_Trainer_vs_pytorch_train_loop.ipynb</a></p><p>View run at <a href='https://wandb.ai/goldandrabbit-g-r/finetune_bert_using_Trainer_vs_pytorch_train_loop.ipynb/runs/ig9gqolf' target="_blank">https://wandb.ai/goldandrabbit-g-r/finetune_bert_using_Trainer_vs_pytorch_train_loop.ipynb/runs/ig9gqolf</a></p><p><button onClick="this.nextSibling.style.display='block';this.style.display='none';">Display W&amp;B run</button><iframe src='https://wandb.ai/goldandrabbit-g-r/finetune_bert_using_Trainer_vs_pytorch_train_loop.ipynb/runs/ig9gqolf?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe></p><pre><code class="lang-python">from datasets import load_datasetdataset = load_dataset(&quot;yelp_review_full&quot;, cache_dir=&quot;/root/autodl-tmp/datasets&quot;)print(dataset)</code></pre><pre><code>DatasetDict(&#123;    train: Dataset(&#123;        features: [&#39;label&#39;, &#39;text&#39;],        num_rows: 650000    &#125;)    test: Dataset(&#123;        features: [&#39;label&#39;, &#39;text&#39;],        num_rows: 50000    &#125;)&#125;)</code></pre><pre><code class="lang-python">dataset[&quot;train&quot;][1]</code></pre><pre><code>&#123;&#39;label&#39;: 1, &#39;text&#39;: &quot;Unfortunately, the frustration of being Dr. Goldberg&#39;s patient is a repeat of the experience I&#39;ve had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don&#39;t get it.  You have office workers, you have patients with medical needs, why isn&#39;t anyone answering the phone?  It&#39;s incomprehensible and not work the aggravation.  It&#39;s with regret that I feel that I have to give Dr. Goldberg 2 stars.&quot;&#125;</code></pre><pre><code class="lang-python">from transformers import AutoTokenizertokenizer = AutoTokenizer.from_pretrained(&quot;google-bert/bert-base-cased&quot;)def tokenize_function(examples):    return tokenizer(examples[&quot;text&quot;], padding=&quot;max_length&quot;, truncation=True)tokenized_datasets = dataset.map(tokenize_function, batched=True)</code></pre><pre><code class="lang-python"># 简化流程采样少量数据small_train_dataset = tokenized_datasets[&quot;train&quot;].shuffle(seed=42).select(range(5000))small_eval_dataset  = tokenized_datasets[&quot;test&quot;].shuffle(seed=42).select(range(1000))print(small_train_dataset)print(small_eval_dataset)print(small_train_dataset[0])</code></pre><pre><code>Dataset(&#123;    features: [&#39;label&#39;, &#39;text&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;],    num_rows: 5000&#125;)Dataset(&#123;    features: [&#39;label&#39;, &#39;text&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;],    num_rows: 1000&#125;)&#123;&#39;label&#39;: 4, &#39;text&#39;: &quot;I stalk this truck.  I&#39;ve been to industrial parks where I pretend to be a tech worker standing in line, strip mall parking lots, and of course the farmer&#39;s market.  The bowls are so so absolutely divine.  The owner is super friendly and he makes each bowl by hand with an incredible amount of pride.  You gotta eat here guys!!!&quot;, &#39;input_ids&#39;: [101, 146, 27438, 1142, 4202, 119, 146, 112, 1396, 1151, 1106, 3924, 8412, 1187, 146, 9981, 1106, 1129, 170, 13395, 7589, 2288, 1107, 1413, 117, 6322, 8796, 5030, 7424, 117, 1105, 1104, 1736, 1103, 9230, 112, 188, 2319, 119, 1109, 20400, 1132, 1177, 1177, 7284, 10455, 119, 1109, 3172, 1110, 7688, 4931, 1105, 1119, 2228, 1296, 7329, 1118, 1289, 1114, 1126, 10965, 2971, 1104, 8188, 119, 1192, 13224, 3940, 1303, 3713, 106, 106, 106, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], &#39;token_type_ids&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]&#125;</code></pre><pre><code class="lang-python">from transformers import AutoModelForSequenceClassificationmodel = AutoModelForSequenceClassification.from_pretrained(    &quot;google-bert/bert-base-cased&quot;,    num_labels=5)</code></pre><pre><code>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: [&#39;classifier.bias&#39;, &#39;classifier.weight&#39;]You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</code></pre><pre><code class="lang-python">import numpy as npimport evaluatemetric = evaluate.load(&quot;accuracy&quot;)def compute_metrics(eval_pred):    logits, labels = eval_pred    predictions = np.argmax(logits, axis=-1)    return metric.compute(predictions=predictions, references=labels)</code></pre><pre><code class="lang-python">from transformers import Trainer, TrainingArgumentsif ON_AUTODL_ENV:    output_dir = f&quot;/root/autodl-tmp/&#123;TRAINER_EXP_NAME&#125;&quot;else:    output_dir = f&quot;/Users/gold/repos/z_notebooks/tmp_file/&#123;TRAINER_EXP_NAME&#125;&quot;print(f&quot;output_dir: &#123;output_dir&#125;&quot;)trainer = Trainer(    model=model,    args=TrainingArguments(        run_name=TRAINER_EXP_NAME,        output_dir=output_dir,        per_device_train_batch_size=16,        per_device_eval_batch_size=8,        learning_rate=5e-5,        num_train_epochs=3,        logging_strategy=&quot;steps&quot;,        logging_steps=0.05,        logging_dir=os.path.join(output_dir, &quot;logs&quot;),    ),    train_dataset=small_train_dataset,    eval_dataset=small_eval_dataset,    compute_metrics=compute_metrics,)</code></pre><pre><code>output_dir: /root/autodl-tmp/finetune_bert_using_Trainer_vs_pytorch_train_loop</code></pre><pre><code class="lang-python">trainer.train()</code></pre><pre><code>&lt;div&gt;  &lt;progress value=&#39;939&#39; max=&#39;939&#39; style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt;  [939/939 03:10, Epoch 3/3]&lt;/div&gt;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;</code></pre><p>  <thead> <tr style="text-align: left;">      <th>Step</th>      <th>Training Loss</th>    </tr>  </thead><br>  <tbody>    <tr>      <td>47</td>      <td>1.488200</td>    </tr>    <tr>      <td>94</td>      <td>1.131200</td>    </tr>    <tr>      <td>141</td>      <td>1.130800</td>    </tr>    <tr>      <td>188</td>      <td>1.078700</td>    </tr>    <tr>      <td>235</td>      <td>1.061300</td>    </tr>    <tr>      <td>282</td>      <td>1.024400</td>    </tr>    <tr>      <td>329</td>      <td>0.951700</td>    </tr>    <tr>      <td>376</td>      <td>0.800300</td>    </tr>    <tr>      <td>423</td>      <td>0.789600</td>    </tr>    <tr>      <td>470</td>      <td>0.790900</td>    </tr>    <tr>      <td>517</td>      <td>0.764000</td>    </tr>    <tr>      <td>564</td>      <td>0.701000</td>    </tr>    <tr>      <td>611</td>      <td>0.773500</td>    </tr>    <tr>      <td>658</td>      <td>0.545700</td>    </tr>    <tr>      <td>705</td>      <td>0.525800</td>    </tr>    <tr>      <td>752</td>      <td>0.527300</td>    </tr>    <tr>      <td>799</td>      <td>0.401600</td>    </tr>    <tr>      <td>846</td>      <td>0.437600</td>    </tr>    <tr>      <td>893</td>      <td>0.408700</td>    </tr>  </tbody><br>&lt;/table&gt;<p></p><pre><code>TrainOutput(global_step=939, training_loss=0.7916636461901843, metrics=&#123;&#39;train_runtime&#39;: 190.9139, &#39;train_samples_per_second&#39;: 78.569, &#39;train_steps_per_second&#39;: 4.918, &#39;total_flos&#39;: 3946772136960000.0, &#39;train_loss&#39;: 0.7916636461901843, &#39;epoch&#39;: 3.0&#125;)</code></pre><pre><code class="lang-python">trainer.evaluate(small_eval_dataset)</code></pre><div>  <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>  [125/125 00:04]</div><pre><code>&#123;&#39;eval_loss&#39;: 1.0120339393615723, &#39;eval_accuracy&#39;: 0.635, &#39;eval_runtime&#39;: 4.6131, &#39;eval_samples_per_second&#39;: 216.775, &#39;eval_steps_per_second&#39;: 27.097, &#39;epoch&#39;: 3.0&#125;</code></pre><pre><code class="lang-python">model_output_dir = output_dir + &#39;/model&#39;print(f&#39;model_output_dir:&#39;, model_output_dir)model.save_pretrained(model_output_dir)tokenizer.save_pretrained(model_output_dir)push_repo_id = TRAINER_EXP_NAMEprint(f&#39;push_repo_id:&#39;, push_repo_id)model.push_to_hub(push_repo_id)tokenizer.push_to_hub(push_repo_id)</code></pre><pre><code>model_output_dir: /root/autodl-tmp/finetune_bert_using_Trainer_vs_pytorch_train_loop/modelpush_repo_id: finetune_bert_using_Trainer_vs_pytorch_train_loopmodel.safetensors:   0%|          | 0.00/433M [00:00&lt;?, ?B/s]No files have been modified since last commit. Skipping to prevent empty commit.CommitInfo(commit_url=&#39;https://huggingface.co/goldandrabbit/finetune_bert_using_Trainer_vs_pytorch_train_loop/commit/797b90ad9ab50d08544eb738fadbfd997b0ab492&#39;, commit_message=&#39;Upload tokenizer&#39;, commit_description=&#39;&#39;, oid=&#39;797b90ad9ab50d08544eb738fadbfd997b0ab492&#39;, pr_url=None, repo_url=RepoUrl(&#39;https://huggingface.co/goldandrabbit/finetune_bert_using_Trainer_vs_pytorch_train_loop&#39;, endpoint=&#39;https://huggingface.co&#39;, repo_type=&#39;model&#39;, repo_id=&#39;goldandrabbit/finetune_bert_using_Trainer_vs_pytorch_train_loop&#39;), pr_revision=None, pr_num=None)</code></pre><h2 id="本地加载模型测试"><a href="#本地加载模型测试" class="headerlink" title="本地加载模型测试"></a>本地加载模型测试</h2><p>1.测试单条样本的预测结果: 直接将 tokenize 之后的数据作为参数传给模型<br>2.测试数据集的预测结果: 需要实例化一个新的 trainer, 这里我们写成 eval_trainer, 然后再调用 eval_trainer.evaluate()</p><pre><code class="lang-python">from transformers import AutoTokenizer, AutoModelForSequenceClassificationlocal_model_path = model_output_dirtokenizer = AutoTokenizer.from_pretrained(local_model_path)model     = AutoModelForSequenceClassification.from_pretrained(local_model_path)</code></pre><pre><code class="lang-python">import torchimport torch.nn.functional as F</code></pre><pre><code class="lang-python">s = &quot;The was awesome and I loved it&quot;tt = tokenizer(s, return_tensors=&quot;pt&quot;, padding=True, truncation=True)</code></pre><pre><code class="lang-python">model.eval()with torch.no_grad():    outputs=model(**tt)</code></pre><pre><code class="lang-python">print(outputs)logits = outputs.logitsprint(&quot;Logits:&quot;, logits)# 输出概率probabilities = F.softmax(logits, dim=-1)print(&quot;Probabilities:&quot;, probabilities)# 输出对应的类别predicted_class = torch.argmax(probabilities, dim=-1)print(&quot;Predicted Class:&quot;, predicted_class.item())</code></pre><pre><code>SequenceClassifierOutput(loss=None, logits=tensor([[-2.4610, -2.6981, -1.1480,  3.0334,  3.9818]]), hidden_states=None, attentions=None)Logits: tensor([[-2.4610, -2.6981, -1.1480,  3.0334,  3.9818]])Probabilities: tensor([[0.0011, 0.0009, 0.0042, 0.2775, 0.7163]])Predicted Class: 4</code></pre><h2 id="分类问题的评估模板"><a href="#分类问题的评估模板" class="headerlink" title="分类问题的评估模板"></a>分类问题的评估模板</h2><p>1.使用的是 evaluate 类, 分别 load 多种指标: acc, precision, recall, f1<br>2.写一个 compute_metrics() 传给 trainer 类 compute_metrics 参数<br>3.实例化 trainer (和训练过程一样)<br>4.调用 eval_trainer.evaluate() 函数</p><pre><code class="lang-python">accuracy_metric  = evaluate.load(&quot;accuracy&quot;)precision_metric = evaluate.load(&quot;precision&quot;)recall_metric    = evaluate.load(&quot;recall&quot;)f1_metric        = evaluate.load(&quot;f1&quot;)def compute_classification_metrics(eval_pred):    logits, labels = eval_pred    predictions = np.argmax(logits, axis=-1)    # 计算多分类指标（需指定 average 参数）    results = &#123;        &quot;accuracy&quot;: accuracy_metric.compute(predictions=predictions, references=labels)[&quot;accuracy&quot;],        &quot;precision&quot;: precision_metric.compute(predictions=predictions, references=labels, average=&quot;macro&quot;)[&quot;precision&quot;],        &quot;recall&quot;: recall_metric.compute(predictions=predictions, references=labels, average=&quot;macro&quot;)[&quot;recall&quot;],        &quot;f1&quot;: f1_metric.compute(predictions=predictions, references=labels, average=&quot;macro&quot;)[&quot;f1&quot;]    &#125;    return results</code></pre><pre><code class="lang-python">eval_trainer = Trainer(    model=model,    args=TrainingArguments(        run_name=&quot;local_eval_dataset&quot;,        output_dir=output_dir + &quot;./local_eval_results&quot;,        per_device_eval_batch_size=16    ),    eval_dataset=small_eval_dataset,    compute_metrics=compute_classification_metrics)</code></pre><pre><code class="lang-python">results = eval_trainer.evaluate()</code></pre><div>  <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>  [63/63 00:04]</div><pre><code class="lang-python">from pprint import pprintpprint(results)</code></pre><pre><code>&#123;&#39;eval_accuracy&#39;: 0.635, &#39;eval_f1&#39;: 0.6364144170295373, &#39;eval_loss&#39;: 1.0120338201522827, &#39;eval_model_preparation_time&#39;: 0.0068, &#39;eval_precision&#39;: 0.6412483673175235, &#39;eval_recall&#39;: 0.6334288150192273, &#39;eval_runtime&#39;: 4.434, &#39;eval_samples_per_second&#39;: 225.53, &#39;eval_steps_per_second&#39;: 14.208&#125;</code></pre><h2 id="使用原生-pytorch-微调"><a href="#使用原生-pytorch-微调" class="headerlink" title="使用原生 pytorch 微调"></a>使用原生 pytorch 微调</h2><p>1.加载预训练模型和使用 trainer 无任何区别, 这里我们继续复用已经 tokenized 之后的 tokenized_datasets<br>2.加载数据采用 pytorch DataLoader, DataLoader 控制 batchsize 和 shuffle<br>3.训练过程需要手动写 training loop, 标准的教程在 <a href="https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html">https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html</a></p><pre><code class="lang-python">def train_loop(dataloader, model, loss_fn, optimizer):    size = len(dataloader.dataset)    # 打开训练模式, 对于 batch_norm 或者 dropout 是有用的    model.train()    for batch, (X, y) in enumerate(dataloader):        # 计算预估结果        pred = model(X)        # 计算 loss        loss = loss_fn(pred, y)        # 计算反向传播        loss.backward()        # 梯度更新        optimizer.step()        # 防止梯度累积​        optimizer.zero_grad()        if batch % 100 == 0:            loss, current = loss.item(), batch * batch_size + len(X)            print(f&quot;loss: &#123;loss:&gt;7f&#125;  [&#123;current:&gt;5d&#125;/&#123;size:&gt;5d&#125;]&quot;)def test_loop(dataloader, model, loss_fn):    # 打开 eval 模式     model.eval()    size = len(dataloader.dataset)    num_batches = len(dataloader)    test_loss, correct = 0, 0    # eval 保证无梯度更新, 开启 torch.no_grad()    with torch.no_grad():        for X, y in dataloader:            pred = model(X)            test_loss += loss_fn(pred, y).item()            correct += (pred.argmax(1) == y).type(torch.float).sum().item()    test_loss /= num_batches    correct /= size    print(f&quot;Test Error: \n Accuracy: &#123;(100*correct):&gt;0.1f&#125;%, Avg loss: &#123;test_loss:&gt;8f&#125; \n&quot;)</code></pre><pre><code class="lang-python">tokenized_datasets = tokenized_datasets.remove_columns([&quot;text&quot;])tokenized_datasets = tokenized_datasets.rename_column(&quot;label&quot;, &quot;labels&quot;)tokenized_datasets.set_format(&quot;torch&quot;)</code></pre><pre><code class="lang-python">tokenized_datasets</code></pre><pre><code>DatasetDict(&#123;    train: Dataset(&#123;        features: [&#39;labels&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;],        num_rows: 650000    &#125;)    test: Dataset(&#123;        features: [&#39;labels&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;],        num_rows: 50000    &#125;)&#125;)</code></pre><pre><code class="lang-python">small_train_dataset = tokenized_datasets[&quot;train&quot;].shuffle(seed=42).select(range(5000))small_eval_dataset  = tokenized_datasets[&quot;test&quot;].shuffle(seed=42).select(range(1000))</code></pre><pre><code class="lang-python">import torchfrom torch.utils.data import DataLoadertraindataloader=DataLoader(small_train_dataset, batch_size=16, shuffle=True)testdataloader=DataLoader(small_eval_dataset, batch_size=8)</code></pre><pre><code class="lang-python">from transformers import AutoModelForSequenceClassificationmodel = AutoModelForSequenceClassification.from_pretrained(&quot;google-bert/bert-base-cased&quot;, num_labels=5)device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)model.to(device)</code></pre><pre><code>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: [&#39;classifier.bias&#39;, &#39;classifier.weight&#39;]You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.BertForSequenceClassification(  (bert): BertModel(    (embeddings): BertEmbeddings(      (word_embeddings): Embedding(28996, 768, padding_idx=0)      (position_embeddings): Embedding(512, 768)      (token_type_embeddings): Embedding(2, 768)      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)      (dropout): Dropout(p=0.1, inplace=False)    )    (encoder): BertEncoder(      (layer): ModuleList(        (0-11): 12 x BertLayer(          (attention): BertAttention(            (self): BertSdpaSelfAttention(              (query): Linear(in_features=768, out_features=768, bias=True)              (key): Linear(in_features=768, out_features=768, bias=True)              (value): Linear(in_features=768, out_features=768, bias=True)              (dropout): Dropout(p=0.1, inplace=False)            )            (output): BertSelfOutput(              (dense): Linear(in_features=768, out_features=768, bias=True)              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)              (dropout): Dropout(p=0.1, inplace=False)            )          )          (intermediate): BertIntermediate(            (dense): Linear(in_features=768, out_features=3072, bias=True)            (intermediate_act_fn): GELUActivation()          )          (output): BertOutput(            (dense): Linear(in_features=3072, out_features=768, bias=True)            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)            (dropout): Dropout(p=0.1, inplace=False)          )        )      )    )    (pooler): BertPooler(      (dense): Linear(in_features=768, out_features=768, bias=True)      (activation): Tanh()    )  )  (dropout): Dropout(p=0.1, inplace=False)  (classifier): Linear(in_features=768, out_features=5, bias=True))</code></pre><pre><code class="lang-python">from transformers import get_schedulerfrom torch.optim import AdamW, SGDoptimizer = AdamW(model.parameters(), lr=5e-5)num_epochs = 3num_training_steps = num_epochs * len(traindataloader)lr_scheduler = get_scheduler(    name=&quot;linear&quot;,    optimizer=optimizer,    num_warmup_steps=0,    num_training_steps=num_training_steps)</code></pre><pre><code class="lang-python">from tqdm.auto import tqdmprogress_bar = tqdm(range(num_training_steps))# 打开训练模式model.train()for epoch in range(num_epochs):    total_loss = 0    for batch in traindataloader:        batch = &#123;k: v.to(device) for k, v in batch.items()&#125;        outputs = model(**batch)        loss = outputs.loss        loss.backward()        optimizer.step()        lr_scheduler.step()        optimizer.zero_grad()        total_loss += loss.item()        progress_bar.update(1)    avg_train_loss = total_loss / len(traindataloader)    print(f&quot;Epoch &#123;epoch+1&#125; | Avg Loss: &#123;avg_train_loss:.6f&#125;&quot;)</code></pre><pre><code>  0%|          | 0/939 [00:00&lt;?, ?it/s]Epoch 1 | Avg Loss: 1.149697Epoch 2 | Avg Loss: 0.798091Epoch 3 | Avg Loss: 0.504651</code></pre><pre><code class="lang-python">import evaluatemetric = evaluate.load(&quot;accuracy&quot;)# 打开 eval 模式model.eval()for batch in testdataloader:    b = &#123;k: v.to(device) for k, v in batch.items()&#125;    with torch.no_grad():        outputs = model(**b)    logits = outputs.logits    predictions = torch.argmax(logits, dim=-1)    metric.add_batch(predictions=predictions, references=batch[&quot;labels&quot;])metric.compute()</code></pre><pre><code>&#123;&#39;accuracy&#39;: 0.611&#125;</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. <a href="https://medium.com/codex/fine-tune-bert-for-text-classification-cef7a1d6cdf1">https://medium.com/codex/fine-tune-bert-for-text-classification-cef7a1d6cdf1</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;finetune-bert-using-Trainer-v-s-Pytorch-Train-Loop&quot;&gt;&lt;a href=&quot;#finetune-bert-using-Trainer-v-s-Pytorch-Train-Loop&quot; class=&quot;headerlink&quot;</summary>
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>LLaMA</title>
    <link href="http://example.com/2024/09/05/LLaMA/"/>
    <id>http://example.com/2024/09/05/LLaMA/</id>
    <published>2024-09-05T12:00:00.000Z</published>
    <updated>2025-04-28T11:04:15.181Z</updated>
    
    <content type="html"><![CDATA[<div align="center"><img src="/imgs/LLaMA/llama.png" width="80%"/></div><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>1.LLaMA<br>2.LLaMA2 和 LLaMA-2-Chat<br>3.LLaMA3  </p><h2 id="LLaMA"><a href="#LLaMA" class="headerlink" title="LLaMA"></a>LLaMA</h2><p>参数量: 7B/13B/33B/65B<br>上下文长度: 2K<br>数据集大小: 1.4 Trillion token  </p><h3 id="LLaMA-预训练数据"><a href="#LLaMA-预训练数据" class="headerlink" title="LLaMA 预训练数据"></a>LLaMA 预训练数据</h3><div align="center"><img src="/imgs/LLaMA/0.png" width="60%"/></div><p>exhuasively,<br>1.English CommonCrawl: 英文爬虫数据, 占比高达 67%, 绝对的大头数据, 训练了 1.10 个 epoch<br>2.C4: 探索实验的时候, 发现使用不同预处理的方法后的 CommonCrawl 可以提高性能.<br>3.Github: 基于 Google BigQuery 得到的 Github 数据, 只用了 Apache/BSD/MIT licenses 的数据. 采用两种规则过滤: 利用每行的长度和数字字母的比例干掉有杂质的数据, 用正则表达式过滤掉 header 之类的信息. 最后再做文件级别的去重操作.<br>4.Wikipedia: 维基百科, 高质量的数据, 占比 4.5%, 训练了 2.45 个 epoch. 覆盖 20 种语言. 删掉了各种链接, 评论和其他的 boilerplate<br>5.Books: 包括两个数据集: Gutenberg 和 Book3<br>6.ArXiv: 学术论文, 高质量数据, 占比 4.5%, 训练 1 个 epoch<br>7.StackExchange: 高质量的问答数据  </p><h3 id="LLaMA-Tokenizer"><a href="#LLaMA-Tokenizer" class="headerlink" title="LLaMA Tokenizer"></a>LLaMA Tokenizer</h3><p>1.Tokenizer 采用 byte-pair encoding algorithm (BPE) 采用 Sentence-Piece 实现<br>2.所有数字 split 成 individual digits<br>3.未知的 UTF-8 字符用 byte 表示<br>4.词表大小 32K</p><h3 id="LLaMA-Arichitecture"><a href="#LLaMA-Arichitecture" class="headerlink" title="LLaMA Arichitecture"></a>LLaMA Arichitecture</h3><p>基于 transformer decoder-only 主架构上, 上做了如下的改进</p><h3 id="Pre-normalization-RMSNorm-normalizing"><a href="#Pre-normalization-RMSNorm-normalizing" class="headerlink" title="Pre-normalization: RMSNorm normalizing"></a>Pre-normalization: RMSNorm normalizing</h3><p>1.相比原始的 transformer 架构, 采用了 pre-normalization 的方式: 也就是说在 input 之后先过 layer-norm 再 attention, 而不是传统方法上的先 attention 再 layer-norm  </p><p>2.将 pre-normalization 的方式从原始的 layer-normalization 转化成了 RMSNorm</p><div align="center"><img src="/imgs/LLaMA/1.png" width="60%"/></div><p>RMSNorm 是一种简化版本的的 LayerNorm, 相比原始的 LayerNorm, RMS 稳定性和泛化性都更好, 效率提升了 10-50%  (有这么强吗?) </p><p>RMSNorm 是 Root Mean Square Normalization Layer 均方根正则化层, 计算公式如下</p><script type="math/tex; mode=display">\overline a_i=\frac{a_i}{RMS}=\frac{a_i}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}a_i^2}}</script><h3 id="SwiGLU-activation-function"><a href="#SwiGLU-activation-function" class="headerlink" title="SwiGLU activation function"></a>SwiGLU activation function</h3><p>SwiGLU 是一个激活函数, 用来取代标准的 ReLU</p><h3 id="RoPE-旋转位置编码"><a href="#RoPE-旋转位置编码" class="headerlink" title="RoPE 旋转位置编码"></a>RoPE 旋转位置编码</h3><h3 id="训练参数"><a href="#训练参数" class="headerlink" title="训练参数"></a>训练参数</h3><p>Opitimizer<br>AadamW<br>beta_1 = 0.9<br>beta_2 = 0.95<br>weight decay = 0.1<br>gradient clip = 1.0<br>warmup_step = 2000</p><h2 id="LLaMA2"><a href="#LLaMA2" class="headerlink" title="LLaMA2"></a>LLaMA2</h2><p>LLaMA2 是 LLaMA 升级版本, 同时 LLaMA2 基础上 finetune 了一个 LLaMA2-Chat 用于对话场景.<br>参数量: 7B/13B/70B, 还有个 34B 版本未公布<br>上下文长度: 4K (+100%)<br>数据集大小: 2.0 Trillion token (+40%)</p><p>LLaMA1 v.s. LLaMA2 宏观对比</p><div align="center"><img src="/imgs/LLaMA/2.png" width="60%"/></div><h3 id="LLaMA2-预训练数据"><a href="#LLaMA2-预训练数据" class="headerlink" title="LLaMA2 预训练数据"></a>LLaMA2 预训练数据</h3><blockquote><p>LLaMA-2 adopts a new mixture of pre-training data (i.e., sources that are known to be high-quality and factual are sampled more heavily) and increases the size of the pre-training dataset by 40%.</p></blockquote><p>intuitively,<br>1.采用了新的混合数据, 数据质量是显著很高的, 且事实类的数据被更大量的采样, 相比 LLaMA1 数据增加了 40%, 不得不说 效果都靠堆砌数据啊   </p><h3 id="LLaMA2-Tokenizer"><a href="#LLaMA2-Tokenizer" class="headerlink" title="LLaMA2 Tokenizer"></a>LLaMA2 Tokenizer</h3><p>1.Tokenizer 采用 byte-pair encoding algorithm (BPE) 采用 Sentence-Piece 实现<br>2.所有数字 split 成 individual digits<br>3.未知的 UTF-8 字符用 byte 表示<br>4.词表大小 32K</p><h3 id="LLaMA2-Architecture"><a href="#LLaMA2-Architecture" class="headerlink" title="LLaMA2 Architecture"></a>LLaMA2 Architecture</h3><p>LLaMA2 核心改动是采用了 Grouped-Query Attention (GQA)</p><h3 id="Grouped-Query-Attention-GQA"><a href="#Grouped-Query-Attention-GQA" class="headerlink" title="Grouped-Query Attention (GQA)"></a>Grouped-Query Attention (GQA)</h3><p>其实是原始的 multi-head self attention 和 multi-query attention 的一个折中选择   </p><div align="center"><img src="/imgs/LLaMA/3.png" width="60%"/></div><p>将总共 N 个 self-attention head 分成了几个组, 每个组里面用一个</p><p>思考: 这个 Group_num 怎么调优呢?  </p><h2 id="LLaMA-2-Chat"><a href="#LLaMA-2-Chat" class="headerlink" title="LLaMA-2-Chat"></a>LLaMA-2-Chat</h2><p>有了 LLaMA2 之后, SFT + RLHF 搞出来 LLaMA-2-Chat</p><h2 id="LLaMA3"><a href="#LLaMA3" class="headerlink" title="LLaMA3"></a>LLaMA3</h2><p>参数量: 8B/70B/405B<br>上下文长度: 128K (显著增大)<br>数据集大小: 15 Trillion tokens (显著增大)</p><h3 id="LLaMA3-预训练数据"><a href="#LLaMA3-预训练数据" class="headerlink" title="LLaMA3 预训练数据"></a>LLaMA3 预训练数据</h3><p>Meta 自己构建一个数据集, 并做了大量的洗数据操作:  </p><h4 id="PII-and-safety-filtering-personally-identifiable-information"><a href="#PII-and-safety-filtering-personally-identifiable-information" class="headerlink" title="PII and safety filtering personally identifiable information"></a>PII and safety filtering personally identifiable information</h4><p>过滤掉个人身份信息</p><h4 id="Text-extraction-and-cleaning"><a href="#Text-extraction-and-cleaning" class="headerlink" title="Text extraction and cleaning"></a>Text extraction and cleaning</h4><p>原生 HTML 数据清理</p><h4 id="De-duplication-去重操作的-3-个层级"><a href="#De-duplication-去重操作的-3-个层级" class="headerlink" title="De-duplication 去重操作的 3 个层级"></a>De-duplication 去重操作的 3 个层级</h4><p>1.URL-level de-duplication<br>2.Document-level de-duplication: 采用的是 MinHash算法<br>3.Line-level de-duplication: 行级别去重, 采用 ccNet, 在每 3000 万 doc 中移除出现超过 6 次的行 </p><h4 id="Heuristic-filtering"><a href="#Heuristic-filtering" class="headerlink" title="Heuristic filtering"></a>Heuristic filtering</h4><p>启发式过滤低质量文档或者重复太多的文档   </p><p>1.计算重复 n-gram 覆盖率比例, 来干掉日志记录或者错误消息组成的行, 这种就是很长很独特但没有意义的东西<br>2.使用 “dirty word” 计数这种启发式规则过滤成人网站 </p><h4 id="基于模型的质量过滤"><a href="#基于模型的质量过滤" class="headerlink" title="基于模型的质量过滤"></a>基于模型的质量过滤</h4><p>1.用 fasttext 训练一个是否被维基百科引用的分类器<br>2.基于 LLaMa2 预训练计算一个 Roberta 分类器</p><h4 id="多语言数据"><a href="#多语言数据" class="headerlink" title="多语言数据"></a>多语言数据</h4><p>1.使用 fasttext 将文档分成 176 中语言</p><h4 id="确定数据比例"><a href="#确定数据比例" class="headerlink" title="确定数据比例"></a>确定数据比例</h4><p>为了构建高质量的数据集, 必须确定预训练数据中不同数据源的比例<br>1.首先训练一个知识类别分类器, 对 web 上出现太多的类别进行采样减少<br>2.怎么确定最优的比例 ? 在一个数据比例上训练几个小模型, 并使用这些数据比例来预测大模型的性能. 然后搞几个不同的比例, 对比不同数据比例上的效果, 然后重复这个过程, 确定比例候选, 然后再这个比例上选一个更大的模型, 评估该模型在几个关键基准上的表现<br>3.数据比例总结: 50% 常识 token, 25% 数学和推理 token, 17% 的代码 token, 8% 的多语言 token</p><h3 id="LLaMa3-Architecture"><a href="#LLaMa3-Architecture" class="headerlink" title="LLaMa3 Architecture"></a>LLaMa3 Architecture</h3><p>LLaMa3 在架构上有 4 个核心优化<br>1.Group query attention: 延续 GQA, 采用 8 个 k-v heads 来提升 inference 效率<br>2.Attention Mask: 采用了一种 attention mask 方式, 目的是为了避免出现这样一种情况: 对同一个序列, 在不同 doc 之间的 self-attention 是不同的;  发现在标准预训练的过程中, 这个操作没什么影响, 但是在对超长序列预训练的时候, 这个很重要<br>3.采用了个 128K token 的词表, 其中有 100K 来自于 tiktoken tokenizer, 28K 来自于非英语语种<br>4.调整了 RoPE 的一个超参数: base frequency hyperparameter 到 50K, 目的是能支持更长的上下文, 有文献曾经说明最长支持 32768 的长度</p><h2 id="Now-My-Perspective"><a href="#Now-My-Perspective" class="headerlink" title="Now My Perspective"></a>Now My Perspective</h2><p>1.相比 OpenAI 那种 Closed AI, 这是真 OpenAI</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. LLaMA: Open and Efficient Foundation Language Models.<br>[2]. LLaMA-1 技术详解. <a href="https://zhuanlan.zhihu.com/p/648774481">https://zhuanlan.zhihu.com/p/648774481</a><br>[3]. LLaMA 2: Open Foundation and Fine-Tuned Chat Models.<br>[4]. LLaMA 超详细解读（paper &amp; code）. <a href="https://zhuanlan.zhihu.com/p/632102048">https://zhuanlan.zhihu.com/p/632102048</a>.<br>[5]. LLaMA-2 from the Ground Up. <a href="https://cameronrwolfe.substack.com/p/LLaMA-2-from-the-ground-up">https://cameronrwolfe.substack.com/p/LLaMA-2-from-the-ground-up</a><br>[6]. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.<br>[7]. Root Mean Square Layer Normalization.<br>[8]. The Llama 3 Herd of Models. <a href="https://arxiv.org/pdf/2407.21783">https://arxiv.org/pdf/2407.21783</a><br>[9]. Llama 3.1技术报告（精华版）. <a href="https://zhuanlan.zhihu.com/p/712251536">https://zhuanlan.zhihu.com/p/712251536</a>  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/LLaMA/llama.png&quot; width=&quot;80%&quot;/&gt;
&lt;/div&gt;

&lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;</summary>
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>Large Language Model</title>
    <link href="http://example.com/2024/06/30/Large%20Language%20Model/"/>
    <id>http://example.com/2024/06/30/Large%20Language%20Model/</id>
    <published>2024-06-30T12:00:00.000Z</published>
    <updated>2025-04-28T11:04:15.168Z</updated>
    
    <content type="html"><![CDATA[<div align="center"><img src="/imgs/Large Language Model/llm.png" width="80%"/></div><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>1.Large Language Model == Next Token Prediction 大语言模型就是单字接龙模型<br>2.LLM 怎么 train?<br>3.LLM 怎么 inference?<br>4.Emergent abilities 涌现能力<br>5.Why LLM work in Everywhere? 为啥 LLM 是真的能 work?<br>6.LLM: A Way to AGI LLM 是走向通用人工智能一条路径</p><h2 id="Large-Language-Model-Next-Token-Prediction-大语言模型就是单字接龙模型"><a href="#Large-Language-Model-Next-Token-Prediction-大语言模型就是单字接龙模型" class="headerlink" title="Large Language Model == Next Token Prediction 大语言模型就是单字接龙模型"></a>Large Language Model == Next Token Prediction 大语言模型就是单字接龙模型</h2><p>涉及到自然语言的任务, (几乎) 所有的任务都可以被转化成 Next Token Prediction 问题; 所以 2024.11.01 目前来看 LLM 其实就是在做 Next Token Prediction, 还是遵循的 GPT 原始的思想   </p><p>比如 QA Task:<br>Where is Tsinghua hua University ? Answer:<br>=&gt; Beijing</p><p>比如机器翻译<br>Machine Translation:<br>Tom chases Jerry<br>汤姆追杰瑞</p><p>仍然可以理解为 Next Sentence Prediction<br>Tom chases Jerry =&gt; 汤<br>Tom chases Jerry 汤 =&gt; 姆<br>Tom chases Jerry 汤姆 =&gt; 追<br>Tom chases Jerry 汤姆追 =&gt; 杰<br>Tom chases Jerry 汤姆追杰 =&gt; 瑞</p><h2 id="LLM-怎么-train"><a href="#LLM-怎么-train" class="headerlink" title="LLM 怎么 train ?"></a>LLM 怎么 train ?</h2><p>前置条件: 需要收集海量的高质量的数据, 并进行有效的预处理; 虽然我们谈到模型总会提到各种模型结构, 炼丹技巧, 或者推理性能等等, 但是最核心的还是原始高质量数据的获得</p><div align="center"><img src="/imgs/Large Language Model/0.png" width="100%"/></div><p>intuitively,<br>1.从互联网找到一切可用的高质量数据, 然后进行清洗工作, 具体来说就是 filtering &amp; selection: 过滤掉各种噪声数据和 dirty word, 过滤的方法其实是启发式+模型的结合, 完全取决于对数据的一种理解; 接下来就是去重, 去重需要做到各种级别: sentence-level, doc-level 或者 url-level 等等; 去重之后就要进行隐私保护, 可以理解为一种特殊的过滤<br>2.语料质量搞干净之后, 就需要 token 化, 这里涉及到一些算法<br>3.在具体训练的时候还会遇到很多问题, 比如数据混合的比例和 scaling law 的实验测试  </p><p>当上述数据 ready 之后, 就可投入训练了, 训练过程分为三个阶段  </p><blockquote><p>Self-Supervised Pre-Training<br>=&gt; Supervised Fine-Tuning<br>=&gt; Reinforcement Learning from Human Feedback</p></blockquote><p>1.Self-Supervised Pre-Training (自监督预训练): 收集大量的互联网数据, 利用收集到的无标注数据 training, training 就是在训练样本上做逐个 token 预测, 然后计算 loss 和 gradients, 然后做 optimization<br>2.Supervised Fine-Tuning (SFT, 有监督微调):<br>如果只有第一个阶段无监督的训练, 模型会异想天开扯一堆乱七八糟的东西, 不能拿来为人类所用 (我们先假设我们的目标是为我们所用); 因此 SFT 直接有监督学习我们要的那些个答案; SFT 的过程是必须的, 目前最强的 GPT4 花了 8 个月时间去做 SFT; 需要大量的人工标注的数据, 其实在这给人类已经注入了人工先验; 但是只要涉及到人工的答案, 很多问题没有标准答案, 很多问题有很多种表达形式比如说   </p><p>Q: When was Tsinghua University founded ?<br>模型预测: In 1911, Tsinghua University was founded in Beijing.<br>Label: Tsinghua University was founded in 1911.  </p><p>3.Reinforcement Learning from Human Feedback (RLHF)<br>让模型学习某些 (人类) 偏好 preference, 给多个输出给个打分, 给定人类的偏好, 更好给到模型的灵活性  </p><h2 id="LLM-怎么-inference"><a href="#LLM-怎么-inference" class="headerlink" title="LLM 怎么 inference?"></a>LLM 怎么 inference?</h2><p>LLM Model 输出在所有 token 上的概率分布, 输出过程其实就从分布中采样  </p><h2 id="Emergent-Abilities-涌现能力"><a href="#Emergent-Abilities-涌现能力" class="headerlink" title="Emergent Abilities 涌现能力"></a>Emergent Abilities 涌现能力</h2><p>当 LLM 的参数量超过某个特定的阈值, LLM 就能展现出新的能力, 这种新的能力在小模型完全不具备<br>emergent 这个词, 本质上不是大模型里面提出的, 来自于统计物理学和复杂系统领域: 一个一个的个体行为很简单, 但是个体组成的系统, 会呈现出来超越个体的更强的, 更复杂的能力. 自然界很多这种现象, 水分子会凝结成非常复杂的雪花的形状, 单个水分子行为非常简单, 但是整体形成了非常复杂的规则. 地球的大气和人类的大脑都是非常复杂的系统, 理论上也能学习到相关能力  </p><p>涌现出来什么能力呢 ? 几个例子:<br>1.In-Context Learning: 仅仅给 LLM 几个示例 (sample), 作为一种上下文 (context), 就能类比似的计算出来<br>2.Instruction Following: 我们给 LLM 一个非常复杂的指令, 我们的输入可以非常长, 要求非常多, 逻辑都非常复杂, LLM 能按照你一堆复杂的指令给出来<br>3.Chain-of-Thought: LLM 不仅能输出结果, 还能输出原来任务分解成多个子任务或者中间结果, 一步一步的推理出来最终的结果   </p><h2 id="Why-LLM-work-in-Everywhere-为啥-LLM-是真的能-work"><a href="#Why-LLM-work-in-Everywhere-为啥-LLM-是真的能-work" class="headerlink" title="Why LLM work in Everywhere? 为啥 LLM 是真的能 work?"></a>Why LLM work in Everywhere? 为啥 LLM 是真的能 work?</h2><p>有两个假设<br>1.Everything can be tokenized.<br>2.Every token can be learned.  </p><p>intuitively,<br>1.LLM 能 work 或者说能通用的一个前提假设是: 一切数据都可 token 化, 且这些 token 的表达都可以被学习  </p><h2 id="LLM-A-Way-to-AGI-LLM-是走向通用人工智能一条路径"><a href="#LLM-A-Way-to-AGI-LLM-是走向通用人工智能一条路径" class="headerlink" title="LLM: A Way to AGI LLM 是走向通用人工智能一条路径"></a>LLM: A Way to AGI LLM 是走向通用人工智能一条路径</h2><p>我们从另一个技术发展的角度去思考, 如何走向一个通用的人工智能? 以终为始, 假设我们真要走向一个通用的人工智能, 具体点就是有个很强的通用的模型, 我们幻想下这个模型应该是一个什么的状态? 总接下来这个 [通用] 实现了三个 [统一]<br>1.Unified architecture for various domains. 统一架构, 现在 Transformer 架构一统天下<br>2.Unified model for various tasks. 真正的多任务处理<br>3.Unified model for various modalities. 真正的多模态  </p><p>这么强大的模型, 放在几年前是不敢想象的, 但是 LLM 的兴起是符合上面的三个规律的, 因此让人感觉语言模型可能真的是一条走向 AGI 的路经, LLM 的严谨规律和上面的三个理念能有高度的契合, 令人充满想象空间  </p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. A Survey of Large Language Models.<br>[2]. Large Language Models: A Survey.  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/Large Language Model/llm.png&quot; width=&quot;80%&quot;/&gt;
&lt;/div&gt;

&lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;header</summary>
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>Decoder-only transformer</title>
    <link href="http://example.com/2024/05/30/Decoder-only%20transformer/"/>
    <id>http://example.com/2024/05/30/Decoder-only%20transformer/</id>
    <published>2024-05-30T12:00:00.000Z</published>
    <updated>2025-04-28T11:04:15.181Z</updated>
    
    <content type="html"><![CDATA[<div align="center"><img src="/imgs/Decoder-only transformer/dec.png" width="80%"/></div><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>1.Recap Transformer<br>2.decoder-only 架构<br>3.为什么要用 decoder-only 的架构 ?</p><h2 id="Recap-Transformer"><a href="#Recap-Transformer" class="headerlink" title="Recap Transformer"></a>Recap Transformer</h2><p>采用 encoder-decoder 结构如下图</p><div align="center"><img src="/imgs/Decoder-only transformer/0.png" width="60%"/></div><h2 id="decoder-only-架构"><a href="#decoder-only-架构" class="headerlink" title="decoder-only 架构"></a>decoder-only 架构</h2><p>相比最原始的 encoder-decoder Transformer 架构, decoder-only 把 encoder 这边全部删除 (连带着 encoder-decoder self attention 的连线模块), 可以理解为是多个 decoder block 堆叠组成的, 其中最基础的 decoder-block 的组成就是 masked self-attention 上面跟一个 FFN 层</p><div align="center"><img src="/imgs/Decoder-only transformer/1.png" width="60%"/></div><h2 id="为什么要用-decoder-only-的架构"><a href="#为什么要用-decoder-only-的架构" class="headerlink" title="为什么要用 decoder-only 的架构 ?"></a>为什么要用 decoder-only 的架构 ?</h2><p>理解这个问题有几个理解层次<br>1.首先思考用的为什么是 decoder 结构, 而不是 encoder 结构? 换句话说 encoder-only 不行吗 ?  </p><blockquote><p>why the decoder? The choice of using the decoder architecture (as opposed to the encoder) for LMs is not arbitrary. The masked self-attention layers within the decoder ensure that the model cannot look forward in a sequence when crafting a token’s representation. In contrast, bidirectional self-attention (as used in the encoder) allows each token’s representation to be adapted based on all other tokens within a sequence.</p><p>Masked self-attention is required for language modeling because we should not be able to look forward in the sentence while predicting the next token. Using masked self-attention yields an autoregressive architecture (i.e., meaning that the model’s output at time t is used as input at time t+1) that can continually predict the next token in a sequence.</p></blockquote><p>intuitively,<br>1.解码器中的 mask self-attention 确保模型在学习 token 的表示时不能向前偷看序列, 因为我们采用的是 autoregressive 的架构, 即 t 时刻的输出用在了 t +1 时刻的输入里; 如果要用 encoder 里面的双向的 self-attention 的结构, 就是那就允许 token 学习的时候看到前后序列所有的信息  </p><div align="center"><img src="/imgs/Decoder-only transformer/2.png" width="80%"/></div><p>2.这里有个值得额外思考的地方是, 对于某些特定下游任务, 比如我们只做句子分类, 这种任务没有必要进行 mask attention, 使用双向的 self-attention 对于学习来说其实是有益的  </p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. <a href="https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2?open=false#%C2%A7decoder-only-transformers">https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2?open=false#%C2%A7decoder-only-transformers</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/Decoder-only transformer/dec.png&quot; width=&quot;80%&quot;/&gt;
&lt;/div&gt;

&lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;he</summary>
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>Ubuntu 22.04 Upgrade Log</title>
    <link href="http://example.com/2024/03/22/Ubuntu%2022.04%20Upgrade%20Log/"/>
    <id>http://example.com/2024/03/22/Ubuntu%2022.04%20Upgrade%20Log/</id>
    <published>2024-03-22T02:11:00.000Z</published>
    <updated>2025-05-05T12:03:39.283Z</updated>
    
    <content type="html"><![CDATA[<div align="center"><img src="/imgs/Ubuntu 22.04 Upgrade Log/0.jpg" width="80%"/></div><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>1.安装 Ubuntu 22.04<br>2.git 配置<br>3.terminal: oh my zsh + plugins<br>4.ide: vim + vscode + sublime<br>5.Firefox 设置默认开启视频声音<br>6.微信<br>7.适配 4K 显示的 175% display<br>8.Keyboard input method system 选择 Fcitx 4<br>9.浏览器选择 Google Chrome  </p><h2 id="安装-Ubuntu-22-04"><a href="#安装-Ubuntu-22-04" class="headerlink" title="安装 Ubuntu 22.04"></a>安装 Ubuntu 22.04</h2><p>1.下载 Ubuntu 22.04 LTS: <a href="https://ubuntu.com/download/desktop">https://ubuntu.com/download/desktop</a><br>2.使用 Rufus 制作 Ubuntu 启动盘: <a href="https://zhuanlan.zhihu.com/p/498100251">https://zhuanlan.zhihu.com/p/498100251</a><br>3.安装的时候参考这个界面, 和实际的界面略有不同, 但也可参考: <a href="https://ubuntu.com/tutorials/install-ubuntu-desktop#1-overview">https://ubuntu.com/tutorials/install-ubuntu-desktop#1-overview</a></p><h2 id="git-配置"><a href="#git-配置" class="headerlink" title="git 配置"></a>git 配置</h2><pre><code class="lang-bash">sudo apt-get install -y gitgit config --global user.name goldandrabbitgit config --global user.email goldandrabbit@foxmail.comgit config --listssh-keygen -t rsa -C &quot;goldandrabbit@foxmail.com&quot; gedit ~/.ssh/id_rsa.pub</code></pre><p>然后 github 登录账号新建公钥  </p><p>最后在 ~/ 目录下 .gitconfig 配置 git 的 alias</p><pre><code class="lang-bash">[user]  name = goldandrabbit  email = goldandrabbit@foxmail.com[alias]  co = checkout  ci = commit  br = branch  st = status</code></pre><h2 id="terminal-oh-my-zsh-plugins"><a href="#terminal-oh-my-zsh-plugins" class="headerlink" title="terminal: oh my zsh + plugins"></a>terminal: oh my zsh + plugins</h2><p>通过 curl 请求 oh my zsh</p><pre><code class="lang-bash">sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot;</code></pre><p>查看所有的 shell, 最下面应该新增了 zsh 的 2 个 shell</p><pre><code class="lang-bash">cat /etc/shells</code></pre><p>设置 zsh 为默认的 shell</p><pre><code class="lang-bash">sudo chsh /bin/zsh</code></pre><p>检查默认的 shell</p><pre><code class="lang-bash">echo $SHELL</code></pre><p>增加字体显示工具</p><pre><code class="lang-bash">sudo apt-get install fonts-powerline</code></pre><p>.zshrc 更换主题为 agnoster</p><pre><code class="lang-bash"># ZSH_THEME=&quot;robbyrussell&quot;ZSH_THEME=&quot;agnoster&quot;</code></pre><p>然后在 terminal 设置里面设置成 Solarized-dark/Solarized 主题  </p><p>增加自动补全和语法高亮插件, zsh-autosuggestions 和 zsh-syntax-highlighting  </p><pre><code class="lang-bash">git clone https://github.com/zsh-users/zsh-autosuggestions $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-autosuggestionsgit clone https://github.com/zsh-users/zsh-syntax-highlighting.git</code></pre><p>在 .zshrc 里面启用上面 2 个插件</p><pre><code class="lang-bash">plugins=(  git  zsh-autosuggestions  zsh-syntax-highlighting)</code></pre><h2 id="ide-vim-vscode-sublime"><a href="#ide-vim-vscode-sublime" class="headerlink" title="ide: vim + vscode + sublime"></a>ide: vim + vscode + sublime</h2><p>先来安装 vim</p><pre><code class="lang-bash">sudo apt-get install vim</code></pre><p>在 .zshrc 里面增加 vim 的 alias </p><pre><code class="lang-bash">alias v=&quot;vim&quot;</code></pre><p>在 ~/ 目录下新建 .vimrc 配置</p><pre><code class="lang-bash">set number</code></pre><p>不使用软件商店里面的 code, 因为是一个阉割版本, 直接上官网 <a href="https://code.visualstudio.com/docs/setup/linux">https://code.visualstudio.com/docs/setup/linux</a>  </p><pre><code class="lang-bash">sudo dpkg -i path_to_deb_file</code></pre><p>sublime 安装参考 <a href="https://www.sublimetext.com/docs/linux_repositories.html">https://www.sublimetext.com/docs/linux_repositories.html</a></p><pre><code class="lang-bash">wget -qO - https://download.sublimetext.com/sublimehq-pub.gpg | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/sublimehq-archive.gpg &gt; /dev/nullecho &quot;deb https://download.sublimetext.com/ apt/stable/&quot; | sudo tee /etc/apt/sources.list.d/sublime-text.listsudo apt-get updatesudo apt-get install sublime-text</code></pre><p>在 .zshrc 里面增加 subl 的 alias </p><pre><code class="lang-bash">alias s=&quot;subl&quot;</code></pre><h2 id="Firefox-设置默认开启视频声音"><a href="#Firefox-设置默认开启视频声音" class="headerlink" title="Firefox 设置默认开启视频声音"></a>Firefox 设置默认开启视频声音</h2><p>如果不设定, b站等网页每次打开视频都需要反复开启声音, 需要开启 Autoplay<br>settings =&gt; Privacy &amp; Security =&gt; Autoplay 打开</p><h2 id="微信"><a href="#微信" class="headerlink" title="微信"></a>微信</h2><p>截止 2024.03.30 微信 for linux 正式版本在官网还没有发布, 下面这个版本可以使用<br><a href="https://github.com/lovechoudoufu/wechat_for_linux">https://github.com/lovechoudoufu/wechat_for_linux</a></p><h2 id="适配-4K-显示的-175-display"><a href="#适配-4K-显示的-175-display" class="headerlink" title="适配 4K 显示的 175% display"></a>适配 4K 显示的 175% display</h2><p>1.默认浏览器和 vscode 对于 4K 显示不友好, 需要去 setting =&gt; display 里面设置 175%</p><h2 id="Keyboard-input-method-system-选择-Fcitx-4"><a href="#Keyboard-input-method-system-选择-Fcitx-4" class="headerlink" title="Keyboard input method system 选择 Fcitx 4"></a>Keyboard input method system 选择 Fcitx 4</h2><p>1.Settings =&gt; Region &amp; Language =&gt; 如果没有就添加简体中文汉语, 添加完最下面应该有灰色的 汉语 (中国) =&gt; 最下面 Keyboard input method system 选择 Fcitx 4, 也就是替换了 iBus =&gt; 点击 Apply System Wide, 然后重启系统<br>2.右上角白色的小键盘里面 Configure 选择第一个输入法是 pinyin 并设置字体大小 18, Global Config 选择 ctrl + Space 切换输入法 </p><h2 id="浏览器选择-Google-Chrome"><a href="#浏览器选择-Google-Chrome" class="headerlink" title="浏览器选择 Google Chrome"></a>浏览器选择 Google Chrome</h2><p>1.为什么不选择 Firefox ? 经常莫名的不能打字, 让我一度认为是系统的问题或者键盘的问题<br>2.为什么不选择 Opera ? 界面可能突然打不开, 重启也无法修复<br>因此看起来没得选择, 只能用 Chrome  </p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. <a href="https://maxim-danilov.github.io/make-linux-terminal-great-again/">https://maxim-danilov.github.io/make-linux-terminal-great-again/</a>  Make Linux terminal great again (Terminator + Oh My ZSH + autosuggestions + highlighting + Agnoster theme + powerline fonts + solarized colors) .<br>[2]. <a href="https://www.sublimetext.com/docs/linux_repositories.html">https://www.sublimetext.com/docs/linux_repositories.html</a> </p>]]></content>
    
    
      
      
    <summary type="html">&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/Ubuntu 22.04 Upgrade Log/0.jpg&quot; width=&quot;80%&quot;/&gt;
&lt;/div&gt;

&lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;head</summary>
      
    
    
    
    <category term="Coding" scheme="http://example.com/categories/Coding/"/>
    
    
  </entry>
  
  <entry>
    <title>2023 Annual Summary</title>
    <link href="http://example.com/2024/02/20/2023%20Annual%20Summary/"/>
    <id>http://example.com/2024/02/20/2023%20Annual%20Summary/</id>
    <published>2024-02-20T12:00:00.000Z</published>
    <updated>2025-04-28T09:20:54.393Z</updated>
    
    <content type="html"><![CDATA[<h2 id="年度关键词"><a href="#年度关键词" class="headerlink" title="年度关键词"></a>年度关键词</h2><center><font color="#1E90FF" size="5"><strong>Deep, and more deep</strong></font></center><h2 id="工作内容-打造工业界最-solid-的-ROAS-预估模型方案"><a href="#工作内容-打造工业界最-solid-的-ROAS-预估模型方案" class="headerlink" title="工作内容: 打造工业界最 solid 的 ROAS 预估模型方案"></a>工作内容: 打造工业界最 solid 的 ROAS 预估模型方案</h2><p>今年核心工作目标很 plain &amp; simple, 如何打造工业界最 solid 的精排 ROAS 预估模型? 4 个视角去牵引做功的方向  </p><p>1.<strong>外循环兼容多行业业务差异的统一预估模型</strong><br>外循环广告业务显著特点之一是”不同行业广告投放具备显著的业务差异性”, 差异性反映在 LTV 预估任务之上表现为: 不同行业回传辅助反馈事件及其影响程度的差异性/不同行业有效特征 (分布) 空间的差异性/不同行业深转样本稀疏程度差异性, 主要有以下 4 个迭代方向;<br>(i). 行业回传事件的高效辅助任务建模. 初期迭代中聚焦于基础 task relationship 建模的有效性, 目标识别并建模不同任务在用户行为动线之间的关系; 中后期迭代, 致力于构建更通用的knowledge transfer &amp; specific task 兼备的模型结构<br>(ii). 行业自适应的特征筛选模型. 针对单次模型迭代对不同行业效果提升幅度存在差异问题, 建设面向行业自适应特征的特征筛选模型, 抽象出面向行业自适应特征筛选模块, 将行业特征和行业特点的表示学习直接反馈到模型结构之上; 从关键的用户行为序列特征的角度建模的方法论上, 面向行业自适应的 retrieve-modeling 长序列建模, 提升模型效果上限<br>(iii). 面向行业特性的长期特征工程专项. 梳理并集成不同行业的特征大图, 配合产运打造统一的特征工程模板 pipeline, 并集成高效特征选择工具, 构建一体化的的数据价值验证框架<br>(iv). 面向全行业的中台模型基座. 将建模视角从单一的样本组织转移到 industry-based task 任务组织, 采用 curriculum learning 等框架建模 task 之间的 difficulty ranking, 集成task 之后建立起 Training Scheduler, 能够更灵活地帮助不同的行业同学进行短平快的模型优化  </p><p>2.<strong>序关系和预估准度兼容建模性质下的预估最优性保证</strong><br>面向 ROAS 类广告出价产品, 不同于前链路预估相关出价产品（唤端/激活/表单）, 客户维度 ROAS 属于最末端链路面向效果的出价诉求; 聚焦投中表现看, 因竞价环境的高频变化以及突发性发生的付费行为, 后验 ROAS 以及成本达成存在更显著的波动性. 因此对 LTV 预估任务对预估准度具备更高的精度要求和稳定性要求; 前期大量工作聚焦于 LTV 预估的序关系建模, 后续会重点加强准度和序关系建模兼容的性质, 深入探索准度建模和序关系建模两个关键问题的兼容性质的最优性保证<br>(i). 针对预估分布/预估准度优化的 Muti-view LTV优化. 当前模型引入了回归损失作为优化目标, 但针对 LTV 的的预估值分布的特殊性缺少针对性的建模, 引入更多 view 下的回归目标, 例如通过核密度估计等方法, 有效利用分布信息做值准度建模; 充分采用 contrastive learning 等方法对 大R 和 小R 进行中间结果建模, 充分缓解深度转化中存在的稀疏性问题<br>(ii). Ord2Seq 连续值预估建模范式. 目标打破原有 hard layer transfer 建模带来效果天花板, 采用 ord2seq 框架统筹建模多阶段独立分类框架  </p><p>3.<strong>带有噪声标签的 ROAS 模型学习</strong><br>短视频外循环广告业务显著特点之一是 “广告主对深度转化行为存在显著的 mis-report”, 由于隐私原因等部分广告主存在一定的错误事件上报, 对于模型学习带来了显著的挑战, 使得深度网络模型可能会学到 corrupt representaion; 因此如何进行有效的 learning with noisy label, 是外循环 ROAS 模型优化的另一大关键挑战<br>针对上述问题主要梳理以下核心优化方向:<br>(i). 如何构建更优质纯净的数据集做模型训练. 针对 LTV 样本存在 label noise 问题,<br>a.探索如何移除 noise label 对模型训练的影响, 构建优质/纯净的训练集, 或者采用无监督预训练之后利用特征空间的相似性做样本 label 的 correction, 致力于提升更高质量表示学习过程;<br>b.探索如何弱化 noise label 对模型训练的影响, 采用置信度建模 &amp; selective reweight 的方法, 弱化 noise 信息对模型学习的影响, 提升模型的鲁棒性;<br>(ii). 如何更充分地利用有限且稀疏的信号反馈提升效果. LTV 样本在无法避免的不置信的情况下, 如何提升模型的效果, 采用 contrastive learning/transfer learning 的方式, 充分利用场景内样本以及更宽泛的回传事件样本挖掘转化信息;  </p><p>4.<strong>multi-view 多视角混合下的学习任务</strong><br>ROAS 预估模型在模型结构上, 既不是单纯的回归任务, 也不是单纯的分类问题, 而是一个融合了多个视角下的混合建模任务, 这也是 ROAS 模型的魅力所在. 年终总结暂不铺开  </p><h2 id="工作方法论"><a href="#工作方法论" class="headerlink" title="工作方法论"></a>工作方法论</h2><p>1.算法工程师这份工作赋予了我什么关键能力与关键习惯. 自 2019.06 年, 从事广告算法工程师已经 4.7 年, 不妨在这个时间节点系统地回顾一下, [算法工程师] 这个工作或者这个 title, 赋予了我哪些比较本质的能力和思考问题的方式 ? 即使有一天不做算法工程师了, 哪些做事的方法会给下一个阶段职业生涯提供更强的项目推进的模式  </p><p>(i). <strong>可控试错成本下的快速迭代</strong><br>快速迭代始终是任何产品, 或者任何算法, 在成长期间存在的唯一状态; 一定要在短时间内高度聚焦自己的注意力, 充分验证自己的固有的认知, 猜测的认知, 以及更宽范围内猜测/拿不准的认知; 迭代的更快, 才能更快地接近效率更高的本质的那一种方法论; 在这个过程中只需要满足试错的成本是可控的, 带来的风险不足以摧毁整个产品的生命, 那么就是要大胆的投入在更密集迭代里面; 在算法工程师的世界里面, 我们称之为 [实验]; 我们的实验如果能做的更快一点, 我们就能积累一点点更快的优势, done is better than perfect.  </p><p>(ii). <strong>认可对抗不确定性的价值</strong>: 长期与不确定性的近身肉搏成就了最终的产出价值.<br>任何有价值的工作, 看起来都是存在大量的不确定性的; 比如买一碗面, 为什么一碗放了辣椒和豌豆的面, 就有来自天南海北不同的顾客长期喜欢呢? 售卖一杯奶茶, 为什么只是放了几块新鲜的水果和大量的芝士就能把公司做到上市 ? 这些工作的共性都存在很强的不确定性, 只不过从有些人从这些不确定性里面找到了某些确定性的因素; 从第一天做算法工程师, 我们都在和一个又一个 [不确定性] 在近身肉搏, 只不过在一次一次不确定性中的抉择, 把我们分成了不同的样子; 不确定性会带来压力, 但是价值也确定性的存在在不确定性中; 这可能是我为什么认可或者喜欢算法工程师这个工作的原因, 过程中压力一直存在, 克服了一个又一个困难之后也确定性地存在一些短暂的成就感, 然后向着更大的不确定性去探索    </p><p>(iii). <strong>巧立名目</strong><br>巧立名目这个词, 在中文的语境下是指 [指用欺骗的手段设立各种名目以达到不正当目的] , 但这里我要强行将这个词赋予一个新的褒义的, 正向的, 新时代的含义; 因为对任何事物的刷新认知的过程, 都是从某个不同于常态思维的, 创新的角度, 或者说一个偏门的角度进行开荒, 然后通过长期的迭代和修正, 最终变成一种长久的价值; 具备永久的价值在初期均是 [巧立名目]; 有些时候的离经叛道的开荒路径, 往往是通向事物更本质, 更具备高效生产力的, 打开天花板的, 探索更长久价值的入口所在; 如果一个目标在一开始就是什么条件都是成熟的, 可满足的, 理由明确的, 那么这个事情能带来收益空间是很值得怀疑的  </p><p>(iv). <strong>convincing matters most</strong><br>即使是长期专注于技术研究/迭代的工程师, 也需要相当高水准的 convincing 的能力, 面向不同受众的 convincing 是工作中非常关键的部分; 当我们在 convincing 的时候, 要注重给受众建立一些符合认知的 intuition, 这个 intuition 可能在受众在接受我们的 idea 之前可能是没有的, 缺失的, 或者模糊的, 甚至是有偏的; 在广告算法的工作范围内, 数据是很强的 impact 工具, 任何情况下要把来龙去脉, 前因后果 connect 起来, 并且拉到 bigger picture 下去 带着 context 讨论问题, 是持久要做的事情. 我在这里总结把 convincing 能力分成两个部分, 一部分取决于对事物的本质的认知水平, 另一部分是日常生活中的自我呈现, 且认为后者的占比在很多情况下影响更大  </p><p>(vi). <strong>训练场不带手机</strong><br>近几年工作的广告核心算法团队里面, 不乏有一些优秀的同学. 这些 “优秀的同学”, 都具备一些比较 solid 的素质, 但抽丝剥茧一下, 我理解这些优秀同学最共性的, 最显著的核心竞争力, 仍然还是对于做事的高度专注状态, 能够较长地保持; 这一点我是 buy in 的, 仅从 2023 自我总结来看, 我和心目中很专注的同学相比, 能做到 neck and neck, 但也清楚自己在专注度上的提升空间, 还是能加大药劲, 进一步提升的.   </p><p>2.<strong>more deeper ?</strong><br>广告系统, 或者广告算法系统是一个非常复杂的动态系统, 相比一些职级更高, 影响力更大的老板或者同事, for some case 上他们更具备相对 deeper 的思考.<br>(i). 复杂系统下的耦合与解耦. 就以精排模型优化来说, 多任务建模/延迟转化/序关系建模/准度建模/序列建模/竞争环境建模可能是现在大家广泛讨论的几个问题, 长期来看结合生成模型/因果推断等开放性问题也有较大讨论空间, 因为思考角度的复杂性, 导致模型的损失函数还在以较大的密度去增加, 因此至少有一个可以回头思考的点是, 当前的思考的角度下, 对已有的其他几个优化的角度会产生怎样的影响 ? 是强冲突的影响, 还是弱冲突的影响<br>(ii). 找寻下一个关键的 motivation. 技术的迭代, 说穿了其实就是两件事, 1.认定某个 make sense 的 motivation 2.基于认定的 motivation 找对合适的方法/途径/手段; 时间迭代长了, 越来越来认为找到合理的 motivation 是最正确的线路, 有点类似于 “道 v.s. 术” 类似的概念, 训练自己的大脑能够 capture 下一个关键的 motivation 是未来训练自己成长的关键.  </p><h2 id="价值观升级"><a href="#价值观升级" class="headerlink" title="价值观升级"></a>价值观升级</h2><p>1.<strong>反着理解一切, 加速认知效率</strong><br>(i). 今年建立了个比较关键的价值观, 就是很多事情最先反着理解的人往往是最早吃到螃蟹的, 反着理解很多事, 其实就是最快加强的认知的方法, 坚持某些 [奇奇怪怪] 的理念或者想法, 坚持到有收益, 或者验证出来有收益, 那就是正确的道路; 加速认知迭代的过程, 本质上就是更快的理解那些之前认为不可能的, 大家都不这么想的, 这时候一定要 “我就要这么想, 我没问题, 错的是大家”. 这里有个最基础的假设是, 如果所有的人都能看懂看明白的东西, 那就根本没价值; 如果所有人都认为是有前途的机会, 那就根本不是机会; 如果所有人都看好某个东西, 那就压根没办法从这个东西上获利; 有些话一定要可以告诉自己, 这个话一定得反着听; 嗯你说的没错, 反着来就对了.<br>(ii). 主动认知迭代: 多告诉自己迭代下认知. 另外有些事情, 其实很难一步到位反着理解到底, 往往会处于一种模糊的, 曲折的, 学不懂, 跟不上, 看不明白的状态. 想持有某些特例独行的想法, 也得一定程度上说服自己, 说服自己很难一步到位. 所以一个执行层面的方法是, 我一定要迭代下对 xx 的认知, 好像记录日记本一样, 今天观测到它的数据, 认知更新一下, 明天观测到它的数据, 认知再来一下, 后天观测到新的数据, 认知再来一下; 说服自己加大认知的频率, 可能是一种有效方法.   </p><p>2.<strong>学习拼多多的执行力/组织能力</strong><br>作为在阿里搞过 4 年电商业务的同学, 今年 pdd 市值首次超越阿里, 关于 pdd 一直有着比较多的思考, 就好比足球上我们想学巴西/学德国/学意大利/学日本一样, 其实很难学到什么东西. 但是有一个关键的输入 from 某个老板, 什么都不用学 (其实你也学不会), 就学一点就行了: 学习 pdd 的执行力; pdd 将一家公司分成了三类人:<br>(i). 想注意的人. 这批人对市场理解深刻, 我是十分佩服, 很少的几苗人. 但短期内我能成为这类人的可能性不大.<br>(ii). 做执行的人.<br>(iii). 做监工的人.<br>在这种机制下, 这三类人各自做各自的事情, 尤其是第 2 类人和第 3 类人, 他们主要的优势就是专注的执行力, 机制保证了他们不去考虑第 (i) 类人的问题, 然后把执行力拉满. 于此回想 2018 年在阿里实习时候, 似乎期望哪怕招聘一个实习生, 都想搞成第 (i) 类人的要求并培养成第 (i) 类人的思维模式.  </p><p>3.<strong>人生的目标是做有趣的事情, 因为我们肯定是最无趣的一代</strong><br>2023 年我突然意识到一件事情, 就是我们这一代人在后人眼里也是非常无趣的一代: 每天上班到很晚, 一年到头很努力地工作, 但是也没有发财; 做出来的事情/成果, 后代肯定觉得: 这东西还值得天天加班吗? 毫无创造力, 毫无影响力呀; 真是土到不行土到掉渣还活在自己的回忆里面的一代人. 所以, 既然是这样一个结局, 我考虑尽可能让自己过得更有趣一点, 不管是工作还是生活, 将趣味性还是拉到很高的优先级, 搞点有意思的事情 (就是写年终总结, 我也是尽可能当更趣的一个) 等到20年后再回头看, 某种程度上已经赢了.  </p><p>4.<strong>我幻想 v.s. 我推测</strong><br>[幻想] 这个词通常很少量地出现在特定的场景, 比如小孩子爱幻想 xx 之类的. 但事实上, 成人一般用 [我想] [我认为] [我觉得] 这些词来不经意之间表述事实意义上 [幻想] 含义; 所以在今后的表达中, 为了追求表达的准确性, 我把 [幻想] 这个词扩展到更多的场景里面, 与之相对的是 [推测], 比如有一些较强的事实依据或者相关数据支持的推断里面.  </p><p>5.<strong>轻的打败重的, 除非重不可</strong><br>2023 年写作的量越来越大, 有一个较多的体感就是 vscode 这工具真的太好用了, 忽然有一天打开其他的 ide 简直难受, 必须要夸赞一下这么强大的 ide. 给我的感触最深的是, 这种轻量级的产品, 可以把重的产品降维打击到没有任何还手之力. 越来越喜欢这种产品设计上的轻盈感或者说工具设计上的简约感带来的做事的丝滑感.  </p><p>6.<strong>拿下捷豹, 才能拿下雪佛兰</strong><br>上世纪 4A 广告公司打入汽车市场, 是这些公司发展的一个重要的里程碑. 这里有一个比较基础的规律, 一个重要的里程碑的获得, 一定有之前的一个重要的里程碑. 我不要聚焦追求幻想中的某一个/欲望中最后一个重要的里程碑, 而是追求之前的某一个必经到的里程碑. 然后让自己再尝试进入下一个 level. </p><p>7.<strong>The magic you are looking for is in the work you’re avoiding</strong><br>这句话无需建立一种理解, 每次在脑子里多默读回荡几遍, 就一定能感受到它说什么.  </p><p>8.<strong>找寻 “后劲很大” 的东西</strong><br>2023 经历了几个 “后劲很大” 的东西: 最后的生还者2, 风骚律师, 一天深潜 2 次搞到流鼻血, 被海胆在手上扎了一堆恐怖的刺 (都没看清楚海胆藏在哪) , 关了灯还会突然倒着开的过山车, 口感巨细腻的鹅肝, 空调开到非常变态的候机室… 都 “后劲很大”, 获得的乐趣感和沉浸感是前所未有的. 与之相比, 大城市买房/投资这些事我是感觉没什么意思, 多做点 “后劲很大” 的事情.  </p><h2 id="高效学习方法论"><a href="#高效学习方法论" class="headerlink" title="高效学习方法论"></a>高效学习方法论</h2><p>1.<strong>把一个东西学明白, 首先得欣赏它</strong><br>这一点其实在 2022 年终总结有初步提到, 但是还想再更深一层说明持续高效的学习是一个自然的认知加强过程; 我们通常说 [欣赏] 一个东西, 比如欣赏一幅画, 欣赏一个话剧, 欣赏一个音乐, 仿佛是要 [具备了不低的专业水平/基础认知] 之后, 才能 [欣赏]; 这里我要推翻一下这种理念, 之所以能学明白一个东西, 还是因为某个在模糊认知瞬间下的突然觉醒, 突然醒悟, 之后带来的满足感和回味感, 自然而然变成一种认同感; 然后基于这种 [默认的认同感], 开始密集的, 高强度的学习, 然后继续找到 [更多的乐趣] 或者 [更深层次的认同感] 或者 [更深层次的认同感]</p><p>2.<strong>Intuitively and Exhaustively Explained</strong><br>[直觉上的认知] 和 [详尽的认知], 是一枚硬币的两面. 2023 通过又一年的直觉认知训练, 显著提高了学习的效率, 做的不错. 这俩一个是哥哥, 一个是弟弟, 哥哥主输出, 弟弟打辅助, 就没有学不会的东西.  </p><p>3.<strong>问问自己没有跳过什么必经的阶段 ?</strong><br>一种复杂的能力或者说不容易轻易模仿的能力, 比如 acm 金牌, 数学竞赛冠军. 除了这些人具备的天赋以外, 学习过程中往往是存在一种分阶梯的线性提升思维: 一层扎实, 一层扎实, 一层扎实地递进; 因此想锻炼一种独有的 solid 能力, 能应付各种不同的问题, 多问问自己没有跳过什么必经的阶段? 过程中有没有缺失某种更符合自然直觉的必经的阶段?  </p><h2 id="日常生活中的自我呈现"><a href="#日常生活中的自我呈现" class="headerlink" title="日常生活中的自我呈现"></a>日常生活中的自我呈现</h2><p>1.<strong>我想成为什么样的人和人们对我有何种期待矛盾</strong><br>人的情绪都来自于上面这一个关键的矛盾, 有时候我们会说服自己, 做一个”我想成为的自己”, 有时候我们会说服自己 “成为xx期待中的自己”, 二者总会有存在某个矛盾的瞬间, 使人感到焦虑和迷茫. 对于这一点如何平衡, 暂时没有形成属于我自己的自洽的准则或者理解. 但是区分清楚人生中是存在这两者的不同的, 是走向更好的自己的基础一步.  </p><p>2.<strong>Emotional Value is all I provide</strong><br>情绪价值, 也是今年思考比较多的一个词, 例如像我这种爱写笔记爱总结的, 给我自己提供了大量的价值满足自己, 推进了 “我想成为什么样的人”; but sometimes, 一个人在群体中能向其他个体的提供的核心价值, 其实主要还是情绪价值, 或者说情绪价值 is all we need.  这又属于 “人们对我有何种期待” 的范畴; 最简单的, 美国人用 bro 或者 body 这两个词称呼别人, 中国人喊你叫 “x哥/大佬/x总”, 其实都存在情绪价值上的巨大的差别, 而且这种情绪价值是独立于种族和文化的. 所以, 我 (如果愿意) 能给别人提供的价值, 还是一种情绪价值. 既然是情绪价值, 那做产品, 做技术, 还是做公司, 其实从这个角度去思考就走到了某种正确的道路上.  </p><p>3.<strong>精准营销是是我们这种人创造出来的概念</strong><br>广告行业有个基础理解, 广告存在一种最基本的机制, 类似于创造 [幸福] [爱] [关怀] 这种 concept, 并主动地赋予它一定的意义, 最终的目的, 是把与之相关的想要的东西更高效地卖出去; 在现在人工智能全力推进的今天, 我们又在产品/服务上赋予了 [智能] 这样的标签. 这种标签唯一的意义, 也是让本来也都平庸的服务, 更高效的卖出去. 精准营销, 本来是有一部分人提出的理想的 concept, 但是通过我们一线工程师的手把它实现交付成可以执行的产品, 本质上变成了我们创造出来的概念.  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;年度关键词&quot;&gt;&lt;a href=&quot;#年度关键词&quot; class=&quot;headerlink&quot; title=&quot;年度关键词&quot;&gt;&lt;/a&gt;年度关键词&lt;/h2&gt;&lt;center&gt;
&lt;font color=&quot;#1E90FF&quot; size=&quot;5&quot;&gt;
&lt;strong&gt;Deep, and mo</summary>
      
    
    
    
    <category term="Rethinking" scheme="http://example.com/categories/Rethinking/"/>
    
    
  </entry>
  
  <entry>
    <title>CREAD A Classification-Restoration Framework with Error Adaptive Discretization for Watch Time Prediction in Video Recommender Systems</title>
    <link href="http://example.com/2024/01/20/CREAD%20A%20Classification-Restoration%20Framework%20with%20Error%20Adaptive%20Discretization%20for%20Watch%20Time%20Prediction%20in%20Video%20Recommender%20Systems/"/>
    <id>http://example.com/2024/01/20/CREAD%20A%20Classification-Restoration%20Framework%20with%20Error%20Adaptive%20Discretization%20for%20Watch%20Time%20Prediction%20in%20Video%20Recommender%20Systems/</id>
    <published>2024-01-20T12:20:00.000Z</published>
    <updated>2025-04-28T09:20:54.399Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Motivation-amp-Contribution"><a href="#Motivation-amp-Contribution" class="headerlink" title="Motivation &amp; Contribution"></a>Motivation &amp; Contribution</h2><p>1.给出一种时长预估任务上较为完备的 framework formulation: CREAD<br>2.分析了离散化分桶过程中的误差来源, 认为两种误差不能同时减小<br>3.提出一个观看时长预估分类任务中的自适应分桶的框架  </p><h2 id="Challenges-of-Discretization-分桶离散化的关键挑战"><a href="#Challenges-of-Discretization-分桶离散化的关键挑战" class="headerlink" title="Challenges of Discretization 分桶离散化的关键挑战"></a>Challenges of Discretization 分桶离散化的关键挑战</h2><div align="center"><img src="/imgs/CREAD A Classification-Restoration Framework with Error Adaptive Discretization for Watch Time Prediction in Video Recommender Systems/0.png" width="40%"/></div><p>1.离散化的操作引入了两种误差:<br>(i). Learning Error 学习误差. 什么是”学习误差”? 因采用更多的分桶导致的样本数量有限导致的单个分桶效果的天花板; 如果我们增加分桶数 $M$ , 会导致每个分桶内的样本就会减少, 限制了分类性能<br>(ii). Restoration Error 恢复误差: 这种恢复方式省略了每个桶的详细的概率密度, 会引入误差  </p><div align="center"><img src="/imgs/CREAD A Classification-Restoration Framework with Error Adaptive Discretization for Watch Time Prediction in Video Recommender Systems/1.png" width="50%"/></div><p>值得思考的是, 上述两种误差不能同时减小</p><h2 id="EAD"><a href="#EAD" class="headerlink" title="EAD"></a>EAD</h2><p>等距分桶</p><script type="math/tex; mode=display">t_m=\frac{m}{M} T_{\max}</script><p>等频分桶</p><script type="math/tex; mode=display">t_m-\Psi^{-1}(\frac{m}{M})</script><p>合并为</p><script type="math/tex; mode=display">t_m=\Psi^{-1}[\gamma(\frac{m}{M})]</script><p>2.CREAD 的缩写是 Classification-Restoration framework with Error-Adaptive-Discretization</p><div align="center"><img src="/imgs/CREAD A Classification-Restoration Framework with Error Adaptive Discretization for Watch Time Prediction in Video Recommender Systems/2.png" width="45%"/></div><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. CREAD: A Classification-Restoration Framework with Error Adaptive Discretization for Watch Time Prediction in Video Recommender Systems.<br>[2]. <a href="https://zhuanlan.zhihu.com/p/675844142">https://zhuanlan.zhihu.com/p/675844142</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Motivation-amp-Contribution&quot;&gt;&lt;a href=&quot;#Motivation-amp-Contribution&quot; class=&quot;headerlink&quot; title=&quot;Motivation &amp;amp; Contribution&quot;&gt;&lt;/a&gt;Mot</summary>
      
    
    
    
    <category term="Ads_RecSys" scheme="http://example.com/categories/Ads-RecSys/"/>
    
    
  </entry>
  
  <entry>
    <title>从找事理论到 TRIZ</title>
    <link href="http://example.com/2024/01/11/%E4%BB%8E%E6%89%BE%E4%BA%8B%E7%90%86%E8%AE%BA%E5%88%B0%20TRIZ/"/>
    <id>http://example.com/2024/01/11/%E4%BB%8E%E6%89%BE%E4%BA%8B%E7%90%86%E8%AE%BA%E5%88%B0%20TRIZ/</id>
    <published>2024-01-11T12:11:00.000Z</published>
    <updated>2025-05-05T11:43:52.449Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>1.找工作 v.s. 找事做<br>2.找到红缨枪的问题, 红布才有价值<br>3.What is TRIZ? 什么是 TRIZ 理论 ?<br>4.TRIZ 处理问题的 2 个原则<br>5.Generalizing Problems and Solutions 泛化问题, 泛化方案<br>6.Don’t accept compromises, eliminating contradictions 不接受折中, 消除矛盾  </p><h2 id="找工作-v-s-找事做"><a href="#找工作-v-s-找事做" class="headerlink" title="找工作 v.s. 找事做"></a>找工作 v.s. 找事做</h2><p>1.对于年轻人来说, 最重要的不是”找工作”, 而是”找事”做; 不是找工作, 而是”找事”. 区别在哪?<br>2.”找工作”, 意味着接受一份公司给我的任务, 做完了, 拿到属于我的报酬; 我和社会之间, 始终搁着公司这个组织, 这个媒介, 这个中间人, 这个社会供需关系的代理, 这个agent; 在”找工作”的设定下, 我不用判断我做的事情有没有社会价值, 因为公司已经帮你判断了是有价值的; 我不用判断自己的天赋和能力是否满足这个要做的事情, 因为公司帮我判断了适不适合这个<br>3.”找事做”, 意味着视线要穿透公司这一层, 直接地去思考, 我做的事情, 为社会解决了什么问题, 贡献了什么价值. 那个价值是什么?<br>4.职业是一个生态系统, 有很多物种, 很多物种之间相互依赖. 这就意味着只要有一个能让别人依赖的技能, 就能找到清晰的职业路径. 比如”娱乐圈”是个生态系统, 当不了导演没关系, 导演不是需要制片主任吗? 可以先从这个工作干起, 如果制片主任当不上, 难度太大了, 制片主任总需要场记, 可以先想办法当场记  </p><p>intuitively,<br>1.”找事做”模型, 一方面用于求职过程的这个事情, 同时适合用于在公司工作中的”找事做”模型, 或者说工作中的”找问题”模型<br>2.”找事做”, 这个事情说起来简单, 但是没有经过”长期找事/找问题训练”的工作思维, 往往做事的时候不能够在有限的时间里面把握”问题”主线并拿到令老板结果; 比如在广告系统的算法模型组, 可能有10+个同学都在负责各个出价产品模块的模型, 大家有个共同的基础的价值: 为公司提升预期收入; 没错, 这确实是这件事的价值, 但是为什么有的同学晋升快, 有的同学晋升慢呢? 这里面核心关键点就在于”有没有快速找到那个当务之急最重要的问题, 然后把这个问题的解决方案推进过程中使自己成为 “1号位”, 并交付令所有人难以挑战的结果; 比如广告模型相比推荐模型的一个点在于需要密切关心”预估准度”这件事情, 因为在业务上预估准度过高/过低/不稳定都会带来各种投放上的问题, 这时候”我”的校准解决方案就是最好的”抓手” (阿里人说法, 个人理解来自于单词 “handler”), 有了这个”抓手”; 我们模型组作为商业化算法就具备了可用的一类问题的关键能力, 就好比厨师每天处理复杂的大型排骨有了个专用的拆骨刀 (而不是其他的完全没法用的刀)<br>3.在工作中, 如果没有找到这样的”问题”就下手做, 往往是吃力不讨好; 如果问题找的不够核心/不够关键/不够引起老板的重视, 那么就需要持续锻炼自己的”找问题”的能力, 最基础的方案就是快速换一个问题来做, 不管前面的问题有没有解决好, 因为问题没找对, 就好比足球射门只有力度发力够了, 但是射门角度打偏了/打飞了/打高了, 索性我就不浪费力气射门了<br>4.这里我想到一个例子:  穆里尼奥就是典型的基层干起的足球教练, 在称为欧洲冠军教练之前是一个俱乐部的翻译. 我们知道, 足球教练和足球运动员之间的沟通是非常关键的. 他从翻译入手, 培养了建立 足球运动员 和 教练之间沟通的关键能力. 从翻译这样的基础工作做起, 从教练身上学习到如何做临时决策, 比如临时换人等. 也能直接学习到应该以怎样的态度和时机和球员表达指令. 为后来成为冠军教练奠定了很强的基础.  </p><div align="center"><img src="/imgs/从找事理论到 TRIZ/0.png" width="60%"/></div><h2 id="找到红缨枪的问题-红布才有价值"><a href="#找到红缨枪的问题-红布才有价值" class="headerlink" title="找到红缨枪的问题, 红布才有价值"></a>找到红缨枪的问题, 红布才有价值</h2><p>1.我的每个技能点, 都一定是某个问题的解决方案, 关键在于, 找到那个问题.<br>2.找工作, 关键在于有技能, 并持续提升自己技能. 但是, 技能这个东西, 哪有个尽头? 学习这个东西, 哪有个尽头? 我可以无止境地考证, 无止境地学习, 学到下辈子也学不完. 内卷不就是这么来的吗? 读书无用不就是这么来的吗?<br>3.在提升技能的同时, 必须分出一点精力, 实际上我们要分出 [占比不低的一部分精力/时间], 去找找, 我的技能所对应的问题<br>4.任何看到的现存的事务, 最初都是某个问题的解决方案, 比如红缨枪上的那块红布, 就是士兵手滑的解决方案<br>5.只有当问题和方案一起出现时, 我才会意识到这个解决方案的价值, 我们把红缨枪上的红缨扯下来, 不告诉你这是干啥的, 我们不可能理解这块红布有啥作用或者有啥奇妙之处, 当红布和红缨枪组合在一起的时候, 我们立马感觉到了红缨枪上的红缨的价值是这么有用<br>6.很多时候, 光有答案, 一点不值钱 (孤立地看那块红布); 只有答案和问题结合在一起才值钱  </p><div align="center"><img src="/imgs/从找事理论到 TRIZ/1.png" width="20%"/></div><h2 id="What-is-TRIZ-什么是-TRIZ-理论"><a href="#What-is-TRIZ-什么是-TRIZ-理论" class="headerlink" title="What is TRIZ? 什么是 TRIZ 理论 ?"></a>What is TRIZ? 什么是 TRIZ 理论 ?</h2><p>上面谈到的理论, 说明了人应该”找问题=&gt;匹配方案”这种思想; 将这个思想放到更大的背景下, 这种思想其实有个更成熟的理论体系, 在该理论下更系统的提出了这种”找问题-匹配方案”的普适的哲学思想: TRIZ 理论.  </p><blockquote><p>TRIZ is the Russian acronym for the “Theory of Inventive Problem Solving,” an international system of creativity developed in the U.S.S.R. between 1946 and 1985, by engineer and scientist Genrich S. Altshuller and his colleagues.</p><p>Following Altshuller’s insight, the theory developed on a foundation of extensive research covering hundreds of thousands of inventions across many different fields to produce an approach that defines generalizable patterns like inventive solutions and the distinguishing characteristics of the problems these inventions have overcome.  </p><p>In other words, whatever problem you’re facing, somebody, somewhere, has already solved it (or one very like it). Creative problem solving involves finding that solution and adapting it to your problem.</p></blockquote><p>intuitively,<br>1.TRIZ 是一个俄罗斯人: Altshuller, 提出的一种理论, TRIZ 是 “Theory of Inventive Problem Solving” 的一种缩写, 翻译过来就是 “发明式的问题解决理论”, 这个概念不好翻译, 第一次听不好感知到在说什么; 一个中文翻译为”萃思”，但这种翻译仍然不够直白, 听完还是不知道是要表达什么; 我们先看下 TRIZ 理论提出背景: Altshuller 这个人思考一个核心问题: 我如果想要解决一个问题, 或者创造性的发明一个东西, 能通过学习的方式解决或者创造发明出来吗 ?<br>2.为了得到这个问题的答案, Altshuller 研究了几十万个解决方案 (来自于专利), 这些发明来自于各行各业, 然后期望能概括出来一些共有的模式; 换句话说, 他想总结一些关于 [解决问题] 或者 [创造] 的普适的原则和模式; 注意, 这里提到的 [解决问题] 或者 [创造] 其实指的是一回事: 都是解决问题; 比如一个问题已经有很成熟的解决方案了, 那么这个问题已经解决了, 比如如何定位我们在地球的位置, GPS 就是一个完美的解决方案; 另一个问题: 如何实现一个完美的自动驾驶, 这个问题目前暂时没有完美的解决方案, 我们需要 “创造” 出一个解决方案<br>3.以上的思考过程, 已将 TRIZ 思想的提出背景交代完毕, 那我们具体如何利用 TRIZ 原则去解决问题 ?</p><h2 id="TRIZ-处理问题的-2-个原则"><a href="#TRIZ-处理问题的-2-个原则" class="headerlink" title="TRIZ 处理问题的 2 个原则"></a>TRIZ 处理问题的 2 个原则</h2><p>1.TRIZ 具体理论内容是提出了一堆表格式的问题/思考方案/相应的矛盾总结, 但感觉太机械和理想了 (可以感受下下面的一个表格), 有点普适哲学的意思, 归根到底还得实践出真知; 但这里不妨从 TRIZ 原则上去体会一下 TRIZ 解决问题的哲学思想  </p><div align="center"><img src="/imgs/从找事理论到 TRIZ/2.png" width="60%"/></div>2.如何使用 TRIZ 原则去解决问题, 总共分两个核心的概念:  (i). Generalizing Problems and Solutions 泛化问题, 泛化方案  (ii). Don't accept compromises, eliminating contradictions 不接受折中, 消除矛盾  ## Generalizing Problems and Solutions 泛化问题, 泛化方案> (i). Problems and solutions are repeated across industries and sciences. By representing a problem as a "contradiction", you can predict creative solutions to that problem.> (ii). Patterns of technical evolution tend to repeat themselves across industries and sciences.> (iii). Creative innovations often use scientific effects outside the field where they were developed.<div align="center"><img src="/imgs/从找事理论到 TRIZ/3.png" width="60%"/></div><p>intuitively,<br>1.不同问题在跨越各个行业 (各类科学) 中, 都是重复出现的; 不同的解决方案, 在跨越各个行业 (各类科学) 中, 也是重复出现的; 例如: A 行业遇到的问题在 B 行业也有, B 行业的解决方法在 C 行业也存在. 这里我们把问题统统定义为”矛盾”<br>2.各个行业的技术都在发生演变, 但是各个行业中的演变也存在相似的规律. 所谓的创新, 都往利用了其开发领域之外的某些科学效应<br>3.他这里得出一个关键的结论: 无论在遇到什么时刻, 无论遇到什么人, 无论遇到怎样的问题. 其实这个 [遇到的问题] 总是有人做过相似的问题了. 无非就是找到这个问题和对应的问题答案, 然后再你的问题上面适配一下, 所谓 creativity (创造/创造力), 其实就是做最后一步的适配问题的过程; 其实仔细想想, 这种找问题-找方案-适配方案的完整过程, 就是一个人在解决任何问题的过程中, 从上帝视角看到的样子  </p><h2 id="Don’t-accept-compromises-eliminating-contradictions-不接受折中-消除矛盾"><a href="#Don’t-accept-compromises-eliminating-contradictions-不接受折中-消除矛盾" class="headerlink" title="Don’t accept compromises, eliminating contradictions 不接受折中, 消除矛盾"></a>Don’t accept compromises, eliminating contradictions 不接受折中, 消除矛盾</h2><p>矛盾, 可以分成两个类型:<br><strong>Technical contradictions</strong>. These are classical engineering “trade-offs” where you can’t reach the desired state because something else in the system prevents it. In other words, when something gets better, something else automatically gets worse. For example:</p><blockquote><p>(i). The product gets stronger (good), but the weight increases (bad).<br>(ii). Service is customized to each customer (good), but the service delivery system gets complicated (bad).<br>(iii). Training is comprehensive (good), but it keeps employees away from their assignments (bad).</p></blockquote><p><strong>Physical (or “inherent”) contradictions</strong>. These are situations in which an object or system suffers contradictory, opposite requirements. Everyday examples include:</p><blockquote><p>(i). Software should be complex (to have many features), but simple (to be easy to learn).<br>(ii). Coffee should be hot (to be enjoyed), but cool (to avoid burning the drinker).<br>(iii). An umbrella should be large (to keep the rain off), but small (to be maneuverable in a crowd).</p></blockquote><p>intuitively,<br>1.矛盾分为了两类矛盾<br>(i). 技术上的矛盾. 其实就是工程上的 “trade-off” 这种概念, 这里比较偏向于归因为某种系统性的问题, 我们有时候永远无法达到某种确定的理想的状态, 有时候总是一个变好, 一个变差. 比如一个产品我们想弄得坚固耐用点, 但是太坚固有时候就变重了; 我们想给每个用户提供 “个性化” 的服务, 有点类似于当老师搞 1对1 教学, 但是如果都 1对1 人多了就干不成了; 给员工搞培训是能提高某些员工的能力的, 但是培训太多就导致实际产出反而少了.<br>(ii). 物理上 (内在的) 的矛盾. 有些场景上, 其实需求存在对立的, 相反的需求.  软件系统通常是复杂的, 但是太复杂其他人上手就麻烦了; 咖啡通常是热的, 但是太烫有时候容易烫伤了; 雨伞应该搞得大一点就能防止被淋到, 但是太大在人群里面用着不方便.<br>2.从我的理解, 这两类矛盾其实也可以合并一下, 都是识别关键问题并作出决策的过程.<br>3.这里明确提出了: 不要妥协, 不要接受折中, 消除矛盾就是要做的, 你选定的方案就是最终的最好的方案. 我理解这里还是比较强调其实人的主观能动性的意思, 矛盾是永恒存在的, 从来没有消除过, 我们所能做的就是我们力所能及能做的消除矛盾, 给出属于自己的答案.  </p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. 得到头条. 176期 | 找工作就是找麻烦.<br>[2]. <a href="https://en.wikipedia.org/wiki/TRIZ#Basic_principles">https://en.wikipedia.org/wiki/TRIZ#Basic_principles</a>.  TRIZ wiki 介绍.<br>[3]. <a href="https://www.mindtools.com/amtcc5f/triz">https://www.mindtools.com/amtcc5f/triz</a>. TRIZ A Powerful Methodology for Creative Problem Solving.<br>[4]. <a href="https://www.slideshare.net/ybaronov/triz-overview-yaroslav">https://www.slideshare.net/ybaronov/triz-overview-yaroslav</a>. TRIZ overview and examples.  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h2&gt;&lt;p&gt;1.找工作 v.s. 找事做&lt;br&gt;2.找到红缨枪的问题, 红布才有价值&lt;br&gt;3.What</summary>
      
    
    
    
    <category term="Reading" scheme="http://example.com/categories/Reading/"/>
    
    
  </entry>
  
  <entry>
    <title>Joint Optimization of Ranking and Calibration with Contextualized Hybrid Model</title>
    <link href="http://example.com/2023/12/30/Joint%20Optimization%20of%20Ranking%20and%20Calibration%20with%20Contextualized%20Hybrid%20Model/"/>
    <id>http://example.com/2023/12/30/Joint%20Optimization%20of%20Ranking%20and%20Calibration%20with%20Contextualized%20Hybrid%20Model/</id>
    <published>2023-12-30T12:43:00.000Z</published>
    <updated>2025-04-28T09:20:54.406Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>1.提出一种 CTR 模型中解耦校准能力和排序能力的损失: 将原本 1 个自由度的 logit 改成 2 个自由度的 logit 作差的形式<br>2.提出同时优化校准和排序的损失函数 (注意这里的校准和业务通常 CTR 校准不是一个概念, 目标和通常意义下的二分类排序损失 + LTR 损失联合建模更接近)  </p><h2 id="Two-types-of-capabilities-that-CTR-CVR-needs-to-have-点击率-转化率模型需要的两类能力"><a href="#Two-types-of-capabilities-that-CTR-CVR-needs-to-have-点击率-转化率模型需要的两类能力" class="headerlink" title="Two types of capabilities that CTR/CVR needs to have 点击率/转化率模型需要的两类能力"></a>Two types of capabilities that CTR/CVR needs to have 点击率/转化率模型需要的两类能力</h2><p>1.CTR 模型需要兼备两种能力, 1 个是校准能力 (calibration ability) 另 1 个是 (ranking ability), 其中校准能力可以写成下面的式子<br>2.注意这里的校准不是业务指标通常意义上的校准, 业务通常意义上的校准能力是 pcoc/calN/ECE 上的保证, 这里的校准指的仍然是做二分类排序能力; 为什么这里叫做校准? 因为本文要引出 LTR 的排序能力对模型效果的收益, LTR 的排序能力在这里用了 ranking ability, 相对本文研究的 ranking ability, 原始的二分类排序的能力称之为校准能力  </p><script type="math/tex; mode=display">\mathcal l_{\text{calib}}=-\sum_{x,y}\log\ \hat p(y|x)=-\sum_{x,y}\log\frac{\exp(f_\theta(x))}{1+\exp(f_{\theta}(x))}=-\sum_{x,y}\log\frac{1}{1+\exp(-f_{\theta}(x))}</script><p>如果这 2 种能力我们同时想要应该怎么做 ? 一种直接的思路就是融合这 2 个 loss , 但是直觉上这 2 种 loss 直接相加是不合理的  </p><h2 id="Joint-Optimization-of-Ranking-and-Calibration-排序和校准同时优化"><a href="#Joint-Optimization-of-Ranking-and-Calibration-排序和校准同时优化" class="headerlink" title="Joint Optimization of Ranking and Calibration 排序和校准同时优化"></a>Joint Optimization of Ranking and Calibration 排序和校准同时优化</h2><div align="center"><img src="/imgs/Joint Optimization of Ranking and Calibration with Contextualized Hybrid Model/0.png" width="80%"/></div><blockquote><p>The intuition of introducing an additional degree of freedom is to alleviate the conflict between the optimization of ranking and calibration.</p><p>To avoid representing different meanings with the same logit, the proposed JRC extends the output logit from 1 dimension to 2 dimensions, $f:\mathbb R^{D}\rightarrow \mathbb R^{2}$</p></blockquote><p>intuitively,<br>1.作者认为 CTR 模型不能兼顾训练准度和校准的原因是我们有两个想要的目标, 但在默认的设定下只有一个 logit; 假如模型的输出有两个 logit, (也就是对 logit 增加一个自由度), 就能把校准和排序的目标分解开, 怎么拆开呢?<br>2.怎么把一个 logit 拆成 2 个? 把原本模型的输出从 1 个 logit 强行拆分成两个 logit , 一个是点击状态 logit, 一个是不点击状态 logit; 我们约定模型参数为 $\theta$ , 用 $f<em>{\theta}(x)[0]$ 表示非点击状态下的 logit, 且用 $f</em>{\theta}(x)[1]$ 表示点击状态下的 logit  </p><script type="math/tex; mode=display">\hat p(y|x)=\frac{1}{1+\exp(-\underbrace{(f_{\theta}(x)[1]-f_{\theta}(x)[0])}_{两个\text{logit}的差作为预估点击概率, 取代原本1个自由度下的f_{\theta}(x)})}</script><p>我们有了上述新的2个自由度的建模方式之后, 相应的 pointwise 的校准 loss 如下所示, 这个式子其实就是等价于两个 logits 采用了 softmax 的计算  </p><script type="math/tex; mode=display">\begin{aligned}\mathcal{l}_{\text{calib}}&=-\sum_{x,y}{\log}\ \hat p(y|x)\\&=-\sum_{x,y}\log \frac{\overbrace{\exp(f_{\theta}(x)[y])}^{正样本用f_\theta(x)[1], 负样本用f_\theta(x)[0]}}{\exp(f_{\theta}(x)[1])+\exp(f_{\theta}(x)[0])} (原文用这个式子表达) \\&=-\sum_{x,y} [\ y \cdot \log \hat p(y=1|x)-(1-y) \log (1-\hat p(y=1|x))] \ (等价于CE损失)\end{aligned}</script><p>3.这里我们不妨倒着想一下校准这个公式是怎么想出来的 ?  这里是要还原一下 softmax 的本质, softmax 这个函数其实就是 sigmoid 在多分类下的推广; softmax 和 sigmoid 在二分类任务上有区别吗? 完全没有, 只是有个数学上等价推导; 这里猜测作者在思考解耦 ranking 和 calibration 的时候, 还是不想放弃属于 Listwise 的建模的 naive 思想: 经典Listwise 的建模方式我们刚才提到了本质上是建模的是元素排在 Top1 的概率, 模型计算 loss 执行的是 softmax 的计算过程, 然后再这样的 softmax 和 二分类的 sigmoid 的关系中找到一种可联系二分类的的方法  </p><script type="math/tex; mode=display">\begin{align}output(x_1)&=\frac{1}{1+e^{-x_1}}=\text{sigmoid}(x_1) \\&=\frac{e^{x_1}}{e^{x_1}+e^{x_2}}=\frac{1}{1+e^{-\underbrace{(x_1-x_2)}_{写成z_1}}}=\text{softmax}(x_1-x_2)  \\output(z_1)&=\frac{1}{1+e^{-z_1}}\Leftrightarrow output(x_1) \\\end{align}</script><p>4.有了校准损失的解耦公式, 那么 listwise 的 ranking 损失应该怎么表达? 选择一个 session , 目标是对点击 logit 和 未点击 logit 都增加一个 listwise 的 loss 提升排序建模, 其中让正样本的点击 logit 大于 session 内其他样本的点击 logit , 让负样本的非点击 logit 大于同 session 内样本的非点击的 logit; 具体怎么做?<br>(i). 对于正样本, 对比的是点击logits 和其他样本的点击logits, 也就是 $f<em>\theta(x)[1]$<br>(ii). 对于负样本, 对比的是非点击logits 和其他样本的非点击logits, 也就是 $f</em>\theta(x)[0]$  </p><script type="math/tex; mode=display">l_{rank}=-\log \frac{\exp(f_\theta (x)[y])}{\sum_{x^{\prime}\in{\text{session}_x}}\exp(f_\theta({x^{\prime})}[y])}  , y\in\{0,1\}</script><p>其实我们对比下 softmax 函数, 本质上是没有区别的  </p><script type="math/tex; mode=display">\text{softmax}(x)=\frac{\exp(x_i)}{\sum_{j=1}^n\exp(x_j)}</script><p>综合两个 loss , 得到最终的 JRC loss 如下  </p><script type="math/tex; mode=display">l_{\text{final}}=\alpha \mathcal l_{\text{calib}}+(1-\alpha) \mathcal l_{\text{rank}}</script><h2 id="JRC-pseudo-code-JRC代码实现"><a href="#JRC-pseudo-code-JRC代码实现" class="headerlink" title="JRC pseudo code JRC代码实现"></a>JRC pseudo code JRC代码实现</h2><p>代码如下</p><pre><code class="lang-python"># B: batch size, label: [B, 2], context_index: [B, 1]# Feed forward computation to get the 2-dimensional logits # and compute LogLoss -log p(y|x, z)logits = feed_forward(inputs)ce_loss = mean(CrossEntropyLoss(logits, label))# Mask: shape [B, B], mask[i,j]=1 indicates the i-th sample # and j-th sample are in the same contextmask = equal(context_index, transpose(context_index))# 铺开 logits 和 label [B,2] =&gt; [B, B, 2]logits = tile(expand_dims(logits, 1), [1, B, 1])y = tile(expand_dims(label, 1), [1, B, 1])# Set logits that are not in the same context to -infy = y * expand_dims(mask, 2)logits = logits + (1-expand_dims(mask, 2))*-1e9 y_neg, y_pos = y[:,:,0], y[:,:,1]l_neg, l_pos = logits[:,:,0], logits[:,:,1]# Compute listwise generative loss -log p(x|y, z)loss_pos = -sum(y_pos * log(softmax(l_pos, axis=0)), axis=0) loss_neg = -sum(y_neg * log(softmax(l_neg, axis=0)), axis=0) ge_loss = mean((loss_pos + loss_neg) / sum(mask, axis=0))# 合并JRC的 lossloss = alpha * ce_loss + (1 - alpha) * ge_loss</code></pre><h2 id="Listwise-softmax-loss-v-s-JRC-loss"><a href="#Listwise-softmax-loss-v-s-JRC-loss" class="headerlink" title="Listwise softmax loss v.s. JRC loss"></a>Listwise softmax loss v.s. JRC loss</h2><p>对比两种损失函数</p><div align="center"><img src="/imgs/Joint Optimization of Ranking and Calibration with Contextualized Hybrid Model/1.png" width="80%"/></div><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. Joint Optimization of Ranking and Calibration with Contextualized Hybrid Model.<br>[2]. KDD’23 | 排序和准度联合优化：一种基于混合生成/判别式建模的方案. 阿里妈妈技术. <a href="https://zhuanlan.zhihu.com/p/639569672">https://zhuanlan.zhihu.com/p/639569672</a>.<br>[3]. KDD’23 | 阿里, 排序和校准联合建模: 让listwise模型也能用于CTR预估. 蘑菇先生. <a href="https://zhuanlan.zhihu.com/p/638414676">https://zhuanlan.zhihu.com/p/638414676</a>.  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h2&gt;&lt;p&gt;1.提出一种 CTR 模型中解耦校准能力和排序能力的损失: 将原本 1 个自</summary>
      
    
    
    
    <category term="Ads_RecSys" scheme="http://example.com/categories/Ads-RecSys/"/>
    
    
  </entry>
  
  <entry>
    <title>Multi-Scenario Ranking with Adaptive Feature Learning</title>
    <link href="http://example.com/2023/11/20/Multi-Scenario%20Ranking%20with%20Adaptive%20Feature%20Learning/"/>
    <id>http://example.com/2023/11/20/Multi-Scenario%20Ranking%20with%20Adaptive%20Feature%20Learning/</id>
    <published>2023-11-20T12:08:00.000Z</published>
    <updated>2025-04-28T09:20:54.408Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>1.梳理四种多场景建模的范式: 辅助任务范式, 分塔建模范式, 专家网络范式以及本文提出的 MARIA 范式, 并且认为传统的多场景模型的设计 (前三种范式) 均忽略了在特征这个层面上的自适应, 限制模型效果的上限<br>2.针对多场景自适应模型, 提出一种在特征层面做自适应场景建模的模型结构 MARIA, 将场景自适应建模下沉到特征层面, 能够针对不同的场景刻画不同特征的影响  </p><div align="center"><img src="/imgs/Multi-Scenario Ranking with Adaptive Feature Learning/0.png" width="60%"/></div><h2 id="Four-Paradigm-of-Multi-Scenario-Ranking-多场景排序模型的4种范式"><a href="#Four-Paradigm-of-Multi-Scenario-Ranking-多场景排序模型的4种范式" class="headerlink" title="Four Paradigm of Multi-Scenario Ranking 多场景排序模型的4种范式"></a>Four Paradigm of Multi-Scenario Ranking 多场景排序模型的4种范式</h2><div align="center"><img src="/imgs/Multi-Scenario Ranking with Adaptive Feature Learning/1.png" width="100%"/></div><p>intuitively,<br>1.辅助任务范式: 将场景特征单独拆出来建辅助任务塔, 然后将辅助塔的结果和主塔做融合, 学习目标是是残差<br>2.分塔建模范式: shared_bottom + 分塔结构, 把所有的共享信息都集中在底座上, 场景信息建模汇聚在不同的塔上<br>3.专家网络范式: shared_bootom 保持不变, 将分塔结构替换成了 针对型设计的 gate 模块去做领域自适应<br>4.Maria范式: 在特征层面设计更复杂的场景自适应结构, 将场景自适应建模下沉到特征层面, 并通过逐级地模块增强场景自适应  </p><h2 id="Formulation"><a href="#Formulation" class="headerlink" title="Formulation"></a>Formulation</h2><p>1.$h_b$ 表示行为序列 (已通过 Transformer 完成序列特征的提取) 特征<br>2.$u$ 表示用户相关的特征, $e_u$ 是其中的 user embedding, 另外还有 $L$ 个用户特征 field<br>3.$t$ 表示 trigger 相关的特征, 当且仅当有 trigger 的场景才有这个特征, 另外还有 $O$ 个特征<br>4.$c$ 表示上下文特征, 上下文总共有 $N_c$ 个特征<br>5.$x$ 表示 target 商品/广告, 其中 $e_x$ 表示商品的 embedding, 另外还有 $P$ 个商品特征<br>6.$e_s$ 表示场景的 embedding<br>7.$N_s$ 总共有 $N_s$ 个场景  </p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>1.MARIA 模型分为 3 个部分, 从下往上 a / b / c 这三个部分各自有一定程度上的场景刻画建模操作, 堆叠起来构造成一个完整的结构; 在 a =&gt; b =&gt; c 这三个部分  </p><div align="center"><img src="/imgs/Multi-Scenario Ranking with Adaptive Feature Learning/2.png" width="100%"/></div><h2 id="Embedding-amp-Encoder-Layer"><a href="#Embedding-amp-Encoder-Layer" class="headerlink" title="Embedding &amp; Encoder Layer"></a>Embedding &amp; Encoder Layer</h2><p>1.商品id/用户id/trigger/context 和它们对应的的基础属性 concate 得到商品/用户/trigger/context embedding, 以商品和用户为例  </p><script type="math/tex; mode=display">x=[e_x||a_x^1||\cdots||a_x^P] \\u=[e_u||a_u^1||\cdots||a_u^L] \\</script><p>2.行为序列过 Transformer 得到一个表征 $h_b$<br>3.各种 embedding 和行为序列表征 concat, 得到基础特征输入 $Q$ , 为了表述容易理解, 把下面的 concat 起来的每个特征组, 称为 [特征分组] , 总共有 5 [特征分组]  </p><script type="math/tex; mode=display">Q=[h_b||\bf u||x_i||t||c]</script><h2 id="Feature-Scaling-特征缩放"><a href="#Feature-Scaling-特征缩放" class="headerlink" title="Feature Scaling 特征缩放"></a>Feature Scaling 特征缩放</h2><p>特征缩放是想在 [原子特征] 上做缩放操作, 这里的 [原子特征] 定义为某个具体 sparse_id / dense 特征, 用 $N_Q=L+P+O+N_c+4$ 来表示基础特征 $Q$ 有 $N_Q$ 个[原子特征], 然后用一个向量  $\alpha \in \mathbb R^{N_Q}$, 来对这 $N_Q$ 个分组生成对应的标量缩放系数 $\alpha_i$ </p><script type="math/tex; mode=display">\begin{aligned}Q_S&=[\alpha_ 1 Q_1||\alpha_ 2 Q_2||\ldots||\alpha _{N_Q}Q_{NQ}]\\&=[\alpha_ 1 h_b||\alpha_ 2 e_u ||\ldots||\alpha_ {N_Q} c_u^{N_c}]\end{aligned}</script><p>这个 $\alpha \in \mathbb R^{N_Q}$ 怎么学出来的呢? 用一层神经网络 sigmoid 激活后的值 乘以外加的一个 $\lambda$ 系数</p><script type="math/tex; mode=display">\alpha=\lambda \cdot \text{sigmoid}(\text{FCN}([freeze(Q)||e_u||e_{x_i}||e_s]))</script><p>intuitively,<br>1.我们看下这个式子, $freeze()$ 函数表示 stop_gradient op, 把原始 $Q$ 固定住参数不参与梯度更新, 不更新的原因是避免过拟合和梯度冲突, 并且和 user embedding 和 item embedding 加上场景 embedding 拼起来做出来一个 attention weight<br>2.这里有个疑问是 $Q$ 里面是包括原来的 $e_u$ 和 $e_x$ embedding 的, 所以如果按照公式里面这么写, 岂不是 $Q$ 的局部还是会修改? 文章没有没明确解释这个操作如何处理<br>3.我们将输出的结果, 一个和 $Q$ 完全相同的表征取出来记录为 $Q_S$ , 这个新表征吸收了在不同特征分组下的 $\alpha$ 参数, 可以理解为完成了一个非常基础的特征分组的场景自适应, 记录一个新的表示为  </p><script type="math/tex; mode=display">Q_S=[\hat h_b||\hat u||\hat x_i||\hat t||\hat c]</script><h2 id="Feature-Refinement-特征精细化"><a href="#Feature-Refinement-特征精细化" class="headerlink" title="Feature Refinement 特征精细化"></a>Feature Refinement 特征精细化</h2><p>1.特征精细化希望进一步提取每个 [特征分组] 的语义信息, 具体来讲, 对每个 [特征分组] 搞若干个属于这个分组的 feature refiner, 这个 refiner 是一个1层神经网络<br>2.这里抽象的层次相比之前的 Feature Scaling 的语义维度高了一层, 我们总共有5个特征分组, 我们就对这5个分组生成对应的 refiner, 每个 refiner 对应有对应的维度<br>3.比如对于 [用户行为序列这个分组] 我们搞 3个 refiner, 相当于把行为序列的特征长度 concat 到原来的 3 倍  </p><p>下面 formulation 写的是 $\beta$ 的生成过程, 并且以 $\hat h_b$ 为代表说明了怎么加权</p><script type="math/tex; mode=display">\begin{aligned}\beta&=\text{Gumbel Softmax}(sigmoid(\text{FCN}([\hat h_b||e_s]))), \beta \in \mathbb R^{N_b} \\\tilde{h}_b&=[\beta_ 1 FC_1(\hat h_b)||\ldots||\beta_ {N_b}FC_{N_b}(\hat h_b)]\end{aligned}</script><p>4.到这为止, 我们生成 (输出) 的表达为  </p><script type="math/tex; mode=display">Q_R=[\tilde{h}_b||\tilde{u}||\tilde{x}_i||\tilde{t}||\tilde{c}]</script><p>5.什么是 Gumbel softmax?  </p><blockquote><p>Gumbel softmax 允许模型中有从离散的分布（比如类别分布categorical distribution）中采样的这个过程变得可微，从而允许反向传播时可以用梯度更新模型参数</p></blockquote><h2 id="Feature-Correlation-Modeling-特征相关性建模"><a href="#Feature-Correlation-Modeling-特征相关性建模" class="headerlink" title="Feature Correlation Modeling 特征相关性建模"></a>Feature Correlation Modeling 特征相关性建模</h2><p>1.我们对现在的 5 个[特征分组], 还想学一个 [特征分组之间] 相关关系, 核心原理是对每两两分组 pair 学一个内积表示 [特征分组] 和 [特征分组] 之间的关系; 为了想用内积, 要把每个分组的表达搞到一个相同的维度 $d_r$ 才能去计算向量内积<br>2.因此第1步我们先对每个分组过一层网络拿到相同维度的表达, 分别是 $\hat h_b,\hat u,\hat x_i,\hat t,\hat c$ 它们的维度都是 $d_r$, 然后生成一个两两内积层  </p><script type="math/tex; mode=display">Q_C=[\hat h_b\cdot \hat u||\hat h_b\cdot \hat x_i||\cdots||\hat t\cdot \hat c]</script><p>3.内积层只是为了捕获特征相关性信息, 我们还是要带上原来的特征  </p><script type="math/tex; mode=display">Q_{f}=[Q_{R}||Q_C]</script><p>4.到此为止, 我们已经把特征层面的事情已经都做完了, 实现了逐层增加的特征对场景的自适应效果, 并且也建模了特征之间复杂的跨域关系  </p><h2 id="MMoE-Layer-多专家混合网络层"><a href="#MMoE-Layer-多专家混合网络层" class="headerlink" title="MMoE Layer 多专家混合网络层"></a>MMoE Layer 多专家混合网络层</h2><p>1.MMoE 这一层想在网络层面对场景进行一次自适应, 总共设置了 $N_e$ 个 experts, 对于然后构造了一个 MMoE 中间层, 其中每个 expert 是独立的全连接层网络<br>2.为什么说是对场景自适应, gate 网络的 softmax 的输入来自于 $W_ge_s$</p><script type="math/tex; mode=display">h_N=\sum_{j=1}^{N_e}gf_j(Q_f)=\sum_{j=1}^{N_e}softmax(W_ge_s)</script><h2 id="Prediction-and-Model-Optimization-分塔融合预测和损失函数"><a href="#Prediction-and-Model-Optimization-分塔融合预测和损失函数" class="headerlink" title="Prediction and Model Optimization 分塔融合预测和损失函数"></a>Prediction and Model Optimization 分塔融合预测和损失函数</h2><p>最终的多任务输出, 采用了一个分塔融合的策略: 用场景单独隐藏层 + $\alpha$ 倍的场景共享层去做融合, 融合的结果最终过一层全连接层输出最终的结果 $\hat y_{ui}$  </p><script type="math/tex; mode=display">\begin{aligned}&h_f=h_{specific}^{s}+\alpha _s h_{shared} \\&\hat y_{ui}=FCN(h_f) \\&h_{specific}=FCN^{s}_{sp}(h_N) \\&h_{shared}=FCN_{sp}(h_N)\end{aligned}</script><p>融合系数 $\alpha_ s$ 是如何产出的? 充分利用场景的 embedding $e_s$ 计算场景的相似性分数, 具体来说, 针对每个场景的 embedding 和其他所有的场景的 embedding 做内积再求和然后归一化  </p><p>intuitively,<br>1.采用这种分塔融合的方式, 类似于直接预估模型 ensemble 融合的方式, 这里的系数 $\alpha_s$ 起到较强的融合作用; 我理解这个融合参数的给出不一定需要一个端到端学习的过程, 因为会给模型学习带来更多的复杂度, 如果采用离线计算的结果直接作用, 可能会降低模型复杂度  </p><script type="math/tex; mode=display">\alpha _s=\frac{1}{N_s-1}\sum_{j=1,s_j\ne s}^{N_s}e_s\cdot e_{s_j}</script><h2 id="From-My-Perspective"><a href="#From-My-Perspective" class="headerlink" title="From My Perspective"></a>From My Perspective</h2><p>1.该模型从特征层面在多场景自适应上的探索更优的效果值得尝试<br>2.该模型采用了较多模块去耦合, 端到端建模的难度非常大, 直觉上耦合的系数过多; 在利用的时候考虑可以组件成独立的模块, 然后再在模型上去做逐层 (a/b/c) 的实验  </p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. Multi-Scenario Ranking with Adaptive Feature Learning.<br>[2]. SIGIR’23 | 基于特征自适应的多场景预估建模. <a href="https://zhuanlan.zhihu.com/p/641895931">https://zhuanlan.zhihu.com/p/641895931</a>.  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h2&gt;&lt;p&gt;1.梳理四种多场景建模的范式: 辅助任务范式, 分塔建模范式, 专家网络范式</summary>
      
    
    
    
    <category term="Ads_RecSys" scheme="http://example.com/categories/Ads-RecSys/"/>
    
    
  </entry>
  
  <entry>
    <title>Towards Deeper, Lighter and Interpretable Cross Network for CTR Prediction</title>
    <link href="http://example.com/2023/11/20/Towards%20Deeper,%20Lighter%20and%20Interpretable%20Cross%20Network%20for%20CTR%20Prediction/"/>
    <id>http://example.com/2023/11/20/Towards%20Deeper,%20Lighter%20and%20Interpretable%20Cross%20Network%20for%20CTR%20Prediction/</id>
    <published>2023-11-20T12:08:00.000Z</published>
    <updated>2025-04-28T09:20:54.412Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>1.提出一个 <a href="https://goldandrabbit.github.io/2023/07/13/DCN%20V2%20Improved%20Deep%20&amp;%20Cross%20Network%20and%20Practical%20Lessons%20for%20Web-scale%20Learning%20to%20Rank%20Systems/">DCN V2</a> 的升级版本: gdcn, 引入了信息门控组件，自适应地学习上一层阶交叉结果的重要性</p><h2 id="GDCN"><a href="#GDCN" class="headerlink" title="GDCN"></a>GDCN</h2><p>1.GDCN 核心实现 </p><pre><code class="lang-python">x_&#123;l+1&#125;=x_0 * (w_l+b_l) * sigmoid(w_g * x_l) + x_l</code></pre><p>intuitively,<br>1.GDCN 结构在 DCN 结构上引入了信息门控组件，自适应地学习上一层阶交叉结果的重要性: 我们期望该过程可以放大更重要特征，减轻不重要特征的影响; 随着交叉层数量的增加，每个交叉层的信息门过滤下一阶交叉特征，并有效地控制信息流</p><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><p>自己实现的 tf 版本</p><pre><code class="lang-python">&quot;&quot;&quot;tensorlow online version&quot;&quot;&quot;def dcn_stack_net(self, input_layer, cross_layer_num, output_size):  &quot;&quot;&quot;  x_l = x_0 \odot (W \cdot x_l + b) + x_l  \odot 代表 element-wise 的tensor 乘法, 通常 tf.multiply() 实现  &quot;&quot;&quot;  name = &#39;dcn&#39;  act_fun = tf.nn.tanh  with tf.variable_scope(&quot;dcn_stack_net&quot;):    x_0 = input_layer    x_l = x_0    input_size = input_layer.shape1.value    for i in range(cross_layer_num):      w      = tf.get_variable(shape=[input_size, input_size], name=f&quot;&#123;name&#125;_kernerl_&#123;i+1&#125;&quot;, initializer=tf.random_normal_initializer(stddev=1.0 / math.sqrt(float(input_size))), trainable=True)      b      = tf.get_variable(shape=[input_size], name=f&quot;&#123;name&#125;_bias_&#123;i+1&#125;&quot;, initializer=tf.zeros_initializer, trainable=True)      dot_   = tf.multiply(x_0, tf.add(tf.matmul(x_l, w), b))      x_l    = tf.add(dot_, x_l)      # x_l    = act_fun(x_l)    output_w = tf.get_variable(shape=[input_size, output_size], name=f&quot;&#123;name&#125;_output_w&quot;)    output_layer = tf.matmul(x_l, output_w)    return output_layerdef gdcn_stack_net(self, input_layer, cross_layer_num, output_size):  &quot;&quot;&quot;  formulaiton: c_&#123;l+1&#125; = c_0 \odot (w_l + b_l) \odot sigmoid(w_g \cdot x_l) + x_l  code: x_l = x_0 \odot (w_l + b_l) \odot sigmoid(w_g \cdot x_l) + x_l  &quot;&quot;&quot;  name = &#39;gdcn&#39;  with tf.variable_scope(&quot;gdcn_stack_net&quot;):    x_0 = input_layer    x_l = x_0    input_size = input_layer.shape1.value    for i in range(cross_layer_num):      wl = tf.get_variable(shape=[input_size, input_size], name=f&quot;&#123;name&#125;_wl_&#123;i+1&#125;&quot;, initializer=tf.random_normal_initializer(stddev=1.0 / math.sqrt(float(input_size))), trainable=True)      wg = tf.get_variable(shape=[input_size, input_size], name=f&quot;&#123;name&#125;_wg_&#123;i+1&#125;&quot;, initializer=tf.random_normal_initializer(stddev=1.0 / math.sqrt(float(input_size))), trainable=True)      b  = tf.get_variable(shape=[input_size], name=f&quot;&#123;name&#125;_b_&#123;i+1&#125;&quot;, initializer=tf.zeros_initializer, trainable=True)      dot1 = tf.multiply(x_0, tf.add(tf.matmul(x_l, wl), b))      dot2 = tf.nn.sigmoid(tf.matmul(x_l, wg))      x_l  = tf.add(tf.multiply(dot1, dot2), x_l)    output_w = tf.get_variable(shape=[input_size, output_size], name=f&quot;&#123;name&#125;_output_w&quot;)    output_layer = tf.matmul(x_l, output_w)    return output_layer</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. Towards Deeper, Lighter and Interpretable Cross Network for CTR Prediction.  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h2&gt;&lt;p&gt;1.提出一个 &lt;a href=&quot;https://goldandrabbit.</summary>
      
    
    
    
    <category term="Ads_RecSys" scheme="http://example.com/categories/Ads-RecSys/"/>
    
    
  </entry>
  
  <entry>
    <title>Self-Attention with Relative Position Representations</title>
    <link href="http://example.com/2023/10/20/Self-Attention%20with%20Relative%20Position%20Representations/"/>
    <id>http://example.com/2023/10/20/Self-Attention%20with%20Relative%20Position%20Representations/</id>
    <published>2023-10-20T12:08:00.000Z</published>
    <updated>2025-04-28T11:04:15.181Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>1.针对 transformer 结构, 提出一种相对位置编码的方法, 能够在 self-attention 过程中将相对位置信息直接编码到单个 token 的表示学习中<br>2.提出高效实现融入相对位置编码方法下的 scaled dot self-attention 实现方式  </p><h2 id="Recap-Why-position-embedding-回顾为什么需要-position-embedding"><a href="#Recap-Why-position-embedding-回顾为什么需要-position-embedding" class="headerlink" title="Recap: Why position embedding ? 回顾为什么需要 position embedding ?"></a>Recap: Why position embedding ? 回顾为什么需要 position embedding ?</h2><p>为什么要引入 position embedding ? 有时候一句话中相同的 token 因为具有不同的位置, 含义完全不同, 找几个极端的体会一下:</p><blockquote><p>Can you can a can as a canner can can a can ? 你能像个罐头工人一样装罐头吗<br>一把把把把把住了<br>我也想过过过过过过的生活</p></blockquote><p>稍微正常一点, 比如我们要对一句话 “I think therefore I am” 这句话编码, 第 1 个 token I 和第 4 个 token I, 在不同的上下文位置上因此有着不同的含义, 它俩因为不同的语境的位置, 应该有这不同的信息成分  </p><h2 id="Relation-aware-Self-Attention-感知相对位置关系的自注意力机制"><a href="#Relation-aware-Self-Attention-感知相对位置关系的自注意力机制" class="headerlink" title="Relation-aware Self-Attention 感知相对位置关系的自注意力机制"></a>Relation-aware Self-Attention 感知相对位置关系的自注意力机制</h2><p>回顾 self-attention 操作, 先设定只有 1 个 head, 对于 长度为 $n$ 的 token 序列 $x=(x_1,\ldots,x_n)$ , 其中 $x_i\in \mathbb R^{d_x}$, 计算一个等长度的结果 $z=(z_1,\ldots,z_n)$, 其中 $z_i\in \mathbb R^{d_z}$, 核心 attention 操作为</p><script type="math/tex; mode=display">\begin{align}z_i=&\sum_{j=1}^n\alpha_{ij}(x_jW^V) \\\alpha_{ij}=&\frac{\exp e_{ij}}{\sum_{k=1}^n\exp e_{ik}} \\e_{ij}=&\frac{(x_iW^Q)(x_jW^K)^T}{\sqrt{d_z}}\end{align}</script><p>引入相对位置编码, 目的显式建模 pairwise 级别的 token 之间的相对位置关系, 假设在 $x<em>i$ 和 $x_j$ 之间的 edge (将 token 抽象成 graph 中的 nodes) 引入两套相对位置表达 $a</em>{ij}^V, a_{ij}^{K} \in \mathbb R^{d_a}$, 注意到相比传统的 self attention: 式子 (1) v.s. (4), 式子 (3) v.s. (6), 新增引入的相对位置编码分别影响到了 $V$ 和 $K$ 之上  </p><script type="math/tex; mode=display">\begin{align}z_i=&\sum_{j=1}^n\alpha_{ij}(x_jW^V+a_{ij}^V) \\\alpha_{ij}=&\frac{\exp e_{ij}}{\sum_{k=1}^n\exp e_{ik}} \\e_{ij}=&\frac{(x_iW^Q)(x_jW^K+a_{ij}^K)^T}{\sqrt{d_z}}\end{align}</script><h2 id="Relative-Position-Representations-相对位置表达"><a href="#Relative-Position-Representations-相对位置表达" class="headerlink" title="Relative Position Representations 相对位置表达"></a>Relative Position Representations 相对位置表达</h2><p>所以这两套表达 $a<em>{ij}^V, a</em>{ij}^{K} \in \mathbb R^{d_a}$, 是怎么学出来的呢 ?</p><p>假设 $i$ 为我们研究的某个序列中间的位置, 它的左侧相对位置可以表示成 $i-k,\ldots,i-4, i-3, i-2, i-1$, 它的右侧侧相对位置可以表示成 $i+1, i+2, i+3, i+4, \ldots, i+k$  </p><blockquote><p>We hypothesized that precise relative position information is not useful beyond a certain distance</p></blockquote><p>我们先确定一个最大的相对值 $k$, 我们假设相对位置如果超过 $k$ 的话就会距离太远没意义了, 所以我们根据相对值 $k$ 做个 clip, 在这种设定下, 我们可以保证至多有 $2k+1$ 范围个 edges  </p><script type="math/tex; mode=display">a_{ij}^K=w_{\text{clip}(j-i,k)}^{K} \\a_{ij}^V=w_{\text{clip}(j-i,k)}^{V} \\\text{clip}(x,k)=\max(-k,\min(k,x))</script><p>目标学出来 $w^K=(w<em>{-k}^K,\ldots,w</em>{k}^K)$ 和 $w^V=(w<em>{-k}^V,\ldots,w</em>{k}^V)$, 其中 $w_i^K,w_i^V\in \mathbb R^{d_a}$</p><p>在实现的时候, 式子 (6)  有个更高效的实现方式</p><script type="math/tex; mode=display">\begin{align}e_{ij}=&\frac{(x_iW^Q)(x_jW^K+a_{ij}^K)^T}{\sqrt{d_z}} \\\Rightarrow e_{ij}&=\frac{(x_iW^Q)(x_jW^K)^T+(x_iW^Q)(a_{ij}^K)^T}{\sqrt{d_z}}\end{align}</script><h2 id="Core-Code-核心代码分析"><a href="#Core-Code-核心代码分析" class="headerlink" title="Core Code 核心代码分析"></a>Core Code 核心代码分析</h2><p>更新 scaled dot self attention 的实现方式</p><pre><code class="lang-python">def _relative_attention_inner(x, y, z, transpose):  # from https://github.com/tensorflow/tensor2tensor/blob/9e0a894034d8090892c238df1bd9bd3180c2b9a3/tensor2tensor/layers/common_attention.py#L1556-L1587  &quot;&quot;&quot;Relative position-aware dot-product attention inner calculation.  This batches matrix multiply calculations to avoid unnecessary broadcasting.  Args:    x: Tensor with shape [batch_size, heads, len, length or depth].    y: Tensor with shape [batch_size, heads, len, depth].    z: Tensor with shape [len, length, depth].    transpose: Whether to transpose inner matrices of y and z.     Should be true if last dimension of x is depth, not length.  Returns:    A Tensor with shape [batch_size, heads, length, length or depth].  &quot;&quot;&quot;  batch_size = tf.shape(x)[0]  heads = x.get_shape().as_list()[1]  length = tf.shape(x)[2]  xy_matmul = tf.matmul(x, y, transpose_b=transpose)                        # [b, heads, len, length or depth]  x_t = tf.transpose(x, [2, 0, 1, 3])                                       # [len, b, heads, length or depth]  x_t_r = tf.reshape(x_t, [length, heads * batch_size, -1])                 # [len, b * heads, length or depth]  x_tz_matmul = tf.matmul(x_t_r, z, transpose_b=transpose)                  # [len, b * heads, length or depth]  x_tz_matmul_r = tf.reshape(x_tz_matmul, [length, batch_size, heads, -1])  # [len, b, heads, length or depth]  x_tz_matmul_r_t = tf.transpose(x_tz_matmul_r, [1, 2, 0, 3])               # [b, heads, len, length or depth]  return xy_matmul + x_tz_matmul_r_t</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. Self-Attention with Relative Position Representations.<br>[2]. <a href="https://zhuanlan.zhihu.com/p/397269153">https://zhuanlan.zhihu.com/p/397269153</a>.<br>[3]. <a href="https://medium.com/@_init_/how-self-attention-with-relative-position-representations-works-28173b8c245a">https://medium.com/@_init_/how-self-attention-with-relative-position-representations-works-28173b8c245a</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h2&gt;&lt;p&gt;1.针对 transformer 结构, 提出一种相对位置编码的方法, 能够</summary>
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>RankFlow Joint Optimization of Multi-Stage Cascade Ranking Systems as Flows</title>
    <link href="http://example.com/2023/10/15/RankFlow%20Joint%20Optimization%20of%20Multi-Stage%20Cascade%20Ranking%20Systems%20as%20Flows/"/>
    <id>http://example.com/2023/10/15/RankFlow%20Joint%20Optimization%20of%20Multi-Stage%20Cascade%20Ranking%20Systems%20as%20Flows/</id>
    <published>2023-10-15T05:08:00.000Z</published>
    <updated>2025-04-28T09:20:54.409Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>1.多阶段一致性训练方案, 级联排序模型优化, 缓解全投放链路的 sample selection bias (SSB) 问题  </p><h2 id="Formulation"><a href="#Formulation" class="headerlink" title="Formulation"></a>Formulation</h2><p>1.总共有多个排序阶段的模型, 其中第 $i$ 个排序阶段为 $R<em>i$, 比如有 3 个阶段 Recall -&gt; Pre-Ranking -&gt; Ranking, 分别对应的是 $R</em>{i-1}$, $R<em>{i}$ , $R</em>{i+1}$ , 每个阶段的样本的分布为 $\text{Pr}_i(X,Y)$</p><div align="center"><img src="/imgs/RankFlow Joint Optimization of Multi-Stage Cascade Ranking Systems as Flows/0.png" width="40%"/></div><p>2.这个文章有一个基础设定: 优化的分布一致性最终目标是对齐到 impression/display (是否曝光) 这个维度, 即用 impression (log) 作为一致性 ground truth, 类似业务中的下发率预估模型 ; 通常每个Rank阶段都有自己的优化目标, 在这种设定下导致每个阶段的优化目标和最终进入曝光这个目标的分布是存在差异的; 作者提出一种多阶段联合训练的方法去建模各个 Ranking 阶段具备”统一对齐性质地”进入到曝光的分布一致性  </p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><blockquote><p>The major paradigm of RankFlow is fitting each ranker towards its own stage-specific data distribution, with the help of other stages.</p><p>the training data of each stage is generated by its preceding stages while the guidance signals come from not only the logs but also the succeeding ranker. </p></blockquote><p>intuitively,<br>1.整个排序系统分为多个阶段 (召回=&gt;粗排=&gt;精排=&gt;曝光/展示),  RankFlow 的核心思想是对具体某个阶段来说, 一方面建模该阶段进入到曝光的数据分布, 另一方面要借助后续排序阶段的数据分布去进行分布统一性建模, 且两个建模过程采取同时优化的范式, 最终的收益是是能使得对全链路上下游各个阶段保持较好的分布一致性<br>2.具体来说, 每个阶段的训练样本都来自于前面的阶段, 训练的监督信号不仅仅用这个阶段的最终曝光 label 做优化, 还要加上后面那个阶段的 label 信息  </p><p>Data Generation 数据生成<br>1.每个阶段用上一个阶段你的结果截断出来的 TopK 个结果做样本, 其实就是如果是精排用的样本是进入精排的样本=就是全量粗排的 TopK 截断; 如果是粗排就是用的进入粗排的样本 (这里默认进入粗排的样本和全量召回的样本不是一个概念, 用的是召回的样本做了 TopK 截断的样本, 其实就是默认召回阶段也有一个统一的相关性分)<br>2.因为每个阶段用了上一个阶段的样本截断生成, 所以第一个阶段比如召回阶段是没办法生成样本的, 因为它不存在上一个阶段搞相关性截断, 所以只能用随机采样的方式做生成  </p><p>Self-Learning Flow 自学习流<br>1.每个阶段的训练 label 的生成, 其实采用的是以终为始的思想, 我们最终的目标是预测一个 $\langle q,d \rangle$ 最后会不会进入曝光, 所以如果最终进入曝光, 那么设置为正样本; 如果最终没有曝光, 那么是负样本<br>2.构造 loss 的时候用的是一个交叉熵损失的形式, 正负 label 已经有了, 预估分采用的是属于该阶段的相关性分数, 也就是基于模型 $R_i$ 预估出来的分  </p><p>Tutor-Learning Flow 监督下的学习流<br>1.除了 self-learning 的生成之外  </p><div align="center"><img src="/imgs/RankFlow Joint Optimization of Multi-Stage Cascade Ranking Systems as Flows/1.png" width="100%"/></div><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. RankFlow: Joint Optimization of Multi-Stage Cascade Ranking Systems as Flows.  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h2&gt;&lt;p&gt;1.多阶段一致性训练方案, 级联排序模型优化, 缓解全投放链路的 sample select</summary>
      
    
    
    
    <category term="Ads_RecSys" scheme="http://example.com/categories/Ads-RecSys/"/>
    
    
  </entry>
  
</feed>
